{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Probing - Dynamic Performance Profiler for Distributed AI","text":""},{"location":"#probing","title":"Probing","text":"<p>Probing is a dynamic performance profiler for distributed AI applications.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Zero Intrusion - Attach to running processes without code changes</li> <li>SQL Analytics - Query performance data with standard SQL</li> <li>Live Execution - Run Python code in target processes</li> <li>Stack Analysis - Capture call stacks with variable values</li> <li>Distributed Ready - Monitor processes across multiple nodes</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Install\npip install probing\n\n# Inject into running process\nprobing -t &lt;pid&gt; inject\n\n# Query performance data\nprobing -t &lt;pid&gt; query \"SELECT * FROM python.torch_trace LIMIT 10\"\n</code></pre>"},{"location":"#use-cases","title":"Use Cases","text":"<ul> <li>Training Debugging - Debug training instabilities and hangs</li> <li>Memory Analysis - Track GPU/CPU memory usage</li> <li>Performance Profiling - Identify bottlenecks in model execution</li> <li>Production Monitoring - Monitor AI services without restarts</li> </ul>"},{"location":"#community","title":"Community","text":"<ul> <li>GitHub Repository</li> <li>Issue Tracker</li> <li>PyPI Package</li> </ul>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete reference for Probing's CLI commands and Python API.</p>"},{"location":"api-reference/#cli-commands","title":"CLI Commands","text":""},{"location":"api-reference/#probing-inject","title":"probing inject","text":"<p>Inject probes into a running process.</p> <pre><code>probing -t &lt;pid&gt; inject\n</code></pre> <p>Options:</p> <ul> <li><code>-t, --target &lt;pid&gt;</code> - Target process ID (required)</li> </ul> <p>Platform: Linux only</p>"},{"location":"api-reference/#probing-query","title":"probing query","text":"<p>Execute SQL queries against collected data.</p> <pre><code>probing -t &lt;endpoint&gt; query \"&lt;sql&gt;\"\n</code></pre> <p>Examples:</p> <pre><code># Query torch traces\nprobing -t 12345 query \"SELECT * FROM python.torch_trace LIMIT 10\"\n\n# Aggregate query\nprobing -t host:8080 query \"SELECT module, AVG(duration) FROM python.torch_trace GROUP BY module\"\n</code></pre>"},{"location":"api-reference/#probing-eval","title":"probing eval","text":"<p>Execute Python code in target process.</p> <pre><code>probing -t &lt;endpoint&gt; eval \"&lt;python_code&gt;\"\n</code></pre> <p>Examples:</p> <pre><code># Simple evaluation\nprobing -t 12345 eval \"print('hello')\"\n\n# Multi-statement\nprobing -t 12345 eval \"import torch; print(torch.cuda.is_available())\"\n</code></pre>"},{"location":"api-reference/#probing-backtrace","title":"probing backtrace","text":"<p>Capture current stack trace.</p> <pre><code>probing -t &lt;endpoint&gt; backtrace\n</code></pre> <p>Output: Stack frames with function names, files, and line numbers.</p>"},{"location":"api-reference/#probing-repl","title":"probing repl","text":"<p>Start interactive Python REPL.</p> <pre><code>probing -t &lt;endpoint&gt; repl\n</code></pre> <p>Features:</p> <ul> <li>Tab completion</li> <li>Multi-line input</li> <li>Command history</li> </ul>"},{"location":"api-reference/#probing-list","title":"probing list","text":"<p>List processes with probing enabled.</p> <pre><code>probing list\n</code></pre> <p>Output: Process IDs and their probing status.</p>"},{"location":"api-reference/#probing-config","title":"probing config","text":"<p>View or modify configuration.</p> <pre><code># View all config\nprobing -t &lt;endpoint&gt; config\n\n# View specific key\nprobing -t &lt;endpoint&gt; config probing.sample_rate\n\n# Set value\nprobing -t &lt;endpoint&gt; config probing.sample_rate=0.1\n</code></pre>"},{"location":"api-reference/#probing-memory","title":"probing memory","text":"<p>Quick memory overview.</p> <pre><code>probing -t &lt;endpoint&gt; memory\n</code></pre>"},{"location":"api-reference/#probing-rdma","title":"probing rdma","text":"<p>RDMA flow analysis.</p> <pre><code>probing -t &lt;endpoint&gt; rdma\n</code></pre>"},{"location":"api-reference/#python-api","title":"Python API","text":""},{"location":"api-reference/#probingconnect","title":"probing.connect","text":"<p>Connect to a probing endpoint.</p> <pre><code>from probing import connect\n\n# Connect by PID\nprobe = connect(pid=12345)\n\n# Connect by address\nprobe = connect(address=\"host:8080\")\n</code></pre>"},{"location":"api-reference/#probeeval","title":"probe.eval","text":"<p>Execute code in target process.</p> <pre><code>result = probe.eval(\"print('hello')\")\n</code></pre>"},{"location":"api-reference/#probequery","title":"probe.query","text":"<p>Execute SQL query.</p> <pre><code>df = probe.query(\"SELECT * FROM python.torch_trace\")\n</code></pre>"},{"location":"api-reference/#probingtable","title":"@probing.table","text":"<p>Register custom data table.</p> <pre><code>from probing import table\n\n@table(\"my_data\")\ndef get_my_data():\n    return [{\"key\": \"value\"}]\n</code></pre>"},{"location":"api-reference/#probingmetric","title":"@probing.metric","text":"<p>Register custom metric.</p> <pre><code>from probing import metric\n\n@metric(\"custom_metric\")\ndef get_metric():\n    return 42.0\n</code></pre>"},{"location":"api-reference/#sql-tables","title":"SQL Tables","text":""},{"location":"api-reference/#pythonbacktrace","title":"python.backtrace","text":"<p>Stack trace information.</p> Column Type Description func string Function name file string Source file lineno int Line number depth int Stack depth frame_type string Python/Native"},{"location":"api-reference/#pythontorch_trace","title":"python.torch_trace","text":"<p>PyTorch execution traces.</p> Column Type Description step int Training step seq int Sequence number module string Module name stage string forward/backward/step allocated float GPU memory (MB) max_allocated float Peak GPU memory (MB) cached float Cached memory (MB) duration float Execution time (sec)"},{"location":"api-reference/#pythonvariables","title":"python.variables","text":"<p>Variable tracking.</p> Column Type Description step int Training step func string Function name name string Variable name value string String representation"},{"location":"api-reference/#information_schemadf_settings","title":"information_schema.df_settings","text":"<p>Configuration settings.</p> Column Type Description name string Setting name value string Setting value"},{"location":"api-reference/#configuration-options","title":"Configuration Options","text":"Key Default Description <code>probing.sample_rate</code> 1.0 Sampling rate (0.0-1.0) <code>probing.buffer_size</code> 10000 Ring buffer size <code>probing.server.port</code> 0 TCP port (0=Unix socket only) <code>probing.torch.enabled</code> true Enable PyTorch tracing"},{"location":"api-reference/#environment-variables","title":"Environment Variables","text":"Variable Description <code>PROBING</code> Enable probing (1=on) <code>PROBING_PORT</code> TCP server port <code>PROBING_TORCH_PROFILING</code> PyTorch profiling (on/off) <code>PROBING_SAMPLE_RATE</code> Default sample rate <code>PROBING_AUTH_TOKEN</code> Authentication token"},{"location":"contributing/","title":"Contributing Guide","text":"<p>Thank you for your interest in contributing to Probing! This guide will help you get started.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li>Rust (latest stable)</li> <li>maturin (for building Python extensions)</li> </ul>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/DeepLink-org/probing.git\ncd probing\n</code></pre> <ol> <li>Create a virtual environment:</li> </ol> <pre><code>python -m venv .venv\nsource .venv/bin/activate  # Linux/macOS\n# or\n.venv\\Scripts\\activate  # Windows\n</code></pre> <ol> <li>Install development dependencies:</li> </ol> <pre><code>pip install -e \".[dev]\"\n</code></pre> <ol> <li>Build the Rust extension:</li> </ol> <pre><code>maturin develop\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nmake test\n\n# Run specific test\npytest tests/test_specific.py -v\n\n# Run with coverage\nmake coverage\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We use the following tools for code quality:</p> <p>Python:</p> <ul> <li><code>ruff</code> for linting and formatting</li> <li><code>mypy</code> for type checking</li> </ul> <pre><code># Format code\nruff format .\n\n# Check linting\nruff check .\n\n# Type check\nmypy python/probing\n</code></pre> <p>Rust:</p> <ul> <li><code>rustfmt</code> for formatting</li> <li><code>clippy</code> for linting</li> </ul> <pre><code># Format\ncargo fmt\n\n# Lint\ncargo clippy --all --tests --benches -- -D warnings\n</code></pre>"},{"location":"contributing/#building-documentation","title":"Building Documentation","text":"<pre><code>cd docs\nmake install  # Install dependencies\nmake serve    # Preview at http://127.0.0.1:8000\n</code></pre>"},{"location":"contributing/#submitting-changes","title":"Submitting Changes","text":""},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch:</li> </ol> <pre><code>git checkout -b feature/your-feature-name\n</code></pre> <ol> <li>Make your changes</li> <li>Run tests and linting:</li> </ol> <pre><code>make test\nmake lint\n</code></pre> <ol> <li>Commit with a descriptive message:</li> </ol> <pre><code>git commit -m \"feat: add new feature description\"\n</code></pre> <ol> <li>Push and create a pull request</li> </ol>"},{"location":"contributing/#commit-message-format","title":"Commit Message Format","text":"<p>We follow Conventional Commits:</p> <ul> <li><code>feat:</code> - New feature</li> <li><code>fix:</code> - Bug fix</li> <li><code>docs:</code> - Documentation changes</li> <li><code>style:</code> - Code style changes (formatting)</li> <li><code>refactor:</code> - Code refactoring</li> <li><code>test:</code> - Test changes</li> <li><code>chore:</code> - Build/tooling changes</li> </ul>"},{"location":"contributing/#code-review","title":"Code Review","text":"<p>All submissions require code review. Please:</p> <ul> <li>Keep PRs focused on a single change</li> <li>Add tests for new functionality</li> <li>Update documentation as needed</li> <li>Respond to review feedback promptly</li> </ul>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>probing/\n\u251c\u2500\u2500 python/             # Python source code\n\u2502   \u2514\u2500\u2500 probing/        # Main Python package\n\u251c\u2500\u2500 probing/            # Rust crates\n\u2502   \u251c\u2500\u2500 core/           # Core functionality\n\u2502   \u251c\u2500\u2500 server/         # HTTP server\n\u2502   \u251c\u2500\u2500 extensions/     # Python/PyTorch extensions\n\u2502   \u2514\u2500\u2500 cli/            # Command-line interface\n\u251c\u2500\u2500 tests/              # Python tests\n\u251c\u2500\u2500 docs/               # Documentation\n\u2514\u2500\u2500 examples/           # Usage examples\n</code></pre>"},{"location":"contributing/#areas-for-contribution","title":"Areas for Contribution","text":""},{"location":"contributing/#good-first-issues","title":"Good First Issues","text":"<p>Look for issues labeled <code>good-first-issue</code> on GitHub. These are suitable for newcomers.</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Improve existing documentation</li> <li>Add more examples</li> <li>Translate documentation</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Add test coverage</li> <li>Write integration tests</li> <li>Performance benchmarks</li> </ul>"},{"location":"contributing/#features","title":"Features","text":"<ul> <li>Check the roadmap and open issues</li> <li>Discuss large changes before implementing</li> </ul>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues: For bugs and feature requests</li> <li>Discussions: For questions and ideas</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please be respectful and constructive in all interactions. We are committed to providing a welcoming environment for everyone.</p>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the project's Apache 2.0 license.</p>"},{"location":"installation/","title":"Installation","text":"<p>This guide provides instructions on how to install Probing on your system.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>Python (version 3.7 or higher)</li> <li>Pip (Python package installer)</li> <li>For building from source:<ul> <li>Rust (latest stable version recommended)</li> <li>Cargo (Rust's package manager and build system)</li> </ul> </li> </ul>"},{"location":"installation/#installation-methods","title":"Installation Methods","text":""},{"location":"installation/#1-using-pip-recommended","title":"1. Using Pip (Recommended)","text":"<p>This is the easiest way to install Probing:</p> <pre><code>pip install probing\n</code></pre> <p>This command will download and install the latest stable release of Probing from the Python Package Index (PyPI).</p>"},{"location":"installation/#2-building-from-source","title":"2. Building from Source","text":"<p>If you want the latest development version or want to contribute to Probing, you can build it from source:</p> <pre><code># 1. Clone the repository\ngit clone https://github.com/DeepLink-org/probing.git\ncd probing\n\n# 2. Build and install the Python package\nmake wheel\npip install dist/probing-*.whl\n</code></pre> <p>This will compile the Rust components and build the Python wheel for installation.</p> <p>For detailed instructions on building from source, including prerequisites and troubleshooting, see the Building from Source guide.</p>"},{"location":"installation/#verifying-the-installation","title":"Verifying the Installation","text":"<p>After installation, you can verify that Probing is correctly installed by running:</p> <pre><code>probing --version\n</code></pre> <p>This should print the installed version of Probing, for example:</p> <pre><code>probing 0.2.3\n</code></pre> <p>You can also check if the <code>probing</code> command is available:</p> <pre><code>probing list\n</code></pre> <p>This command should list available probing commands or indicate that no processes are currently being probed.</p>"},{"location":"installation/#platform-support","title":"Platform Support","text":"Platform Injection Query/Eval Linux \u2705 Full support \u2705 Full support macOS \u274c Not supported \u2705 Supported Windows \u274c Not supported \u2705 Supported <p>Linux Required for Injection</p> <p>The dynamic probe injection feature (<code>probing inject</code>) requires Linux. On other platforms, you can still use query and eval features if the target process has probing enabled at startup.</p>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>With Probing installed, you are ready to start using it:</p> <ul> <li>Quick Start - Get started with your first analysis</li> <li>SQL Analytics - Learn the SQL query interface</li> <li>Memory Analysis - Debug memory issues</li> </ul>"},{"location":"quickstart/","title":"Quick Start","text":"<p>Get immediate value from Probing with this streamlined workflow.</p>"},{"location":"quickstart/#your-first-5-minutes","title":"Your First 5 Minutes","text":""},{"location":"quickstart/#step-1-set-your-target-process","title":"Step 1: Set Your Target Process","text":"<p>All Probing commands need a target endpoint. Set <code>$ENDPOINT</code> to either a local process ID or remote address:</p> <pre><code># Local process - find and set your Python process ID\nexport ENDPOINT=$(pgrep -f \"python.*your_script\")\n\n# Or for remote processes\nexport ENDPOINT=remote-host:8080\n</code></pre> <p>Finding Processes</p> <p>Use <code>ps aux | grep python</code> or <code>pgrep -f \"python.*train\"</code> to locate your target.</p>"},{"location":"quickstart/#step-2-connect-and-explore","title":"Step 2: Connect and Explore","text":"<pre><code># Connect to your process (Linux only)\nprobing $ENDPOINT inject\n\n# Get basic process info\nprobing $ENDPOINT eval \"import os, psutil; proc = psutil.Process(); print(f'PID: {os.getpid()}, Memory: {proc.memory_info().rss/1024**2:.1f}MB')\"\n</code></pre>"},{"location":"quickstart/#step-3-try-all-three-core-capabilities","title":"Step 3: Try All Three Core Capabilities","text":""},{"location":"quickstart/#query-structured-data","title":"\ud83d\udcca Query structured data","text":"<pre><code>probing $ENDPOINT query \"SELECT name, value FROM information_schema.df_settings LIMIT 5\"\n</code></pre>"},{"location":"quickstart/#execute-live-code","title":"\ud83c\udfaf Execute live code","text":"<pre><code>probing $ENDPOINT eval \"import torch; print(f'CUDA: {torch.cuda.is_available()}')\"\n</code></pre>"},{"location":"quickstart/#capture-execution-context","title":"\ud83d\udd0d Capture execution context","text":"<pre><code>probing $ENDPOINT backtrace\n\nprobing $ENDPOINT query \"SELECT func, file, lineno FROM python.backtrace ORDER BY depth LIMIT 5\"\n</code></pre>"},{"location":"quickstart/#three-core-capabilities","title":"Three Core Capabilities","text":"<p>Probing provides three powerful capabilities that work together:</p>"},{"location":"quickstart/#eval-execute-code-in-live-processes","title":"\ud83c\udfaf eval: Execute Code in Live Processes","text":"<p>Run arbitrary Python code directly inside your target process:</p> <pre><code># Check training threads\nprobing $ENDPOINT eval \"import threading; [print(f'{t.name}: {t.is_alive()}') for t in threading.enumerate()]\"\n\n# Check GPU memory usage\nprobing $ENDPOINT eval \"import torch; print(f'GPU: {torch.cuda.memory_allocated()/1024**3:.1f}GB allocated')\"\n</code></pre>"},{"location":"quickstart/#query-analyze-data-with-sql","title":"\ud83d\udcca query: Analyze Data with SQL","text":"<p>Query structured performance data using familiar SQL syntax:</p> <pre><code>probing $ENDPOINT query \"\nSELECT\n    step,\n    module,\n    SUM(allocated) as total_memory_mb,\n    COUNT(*) as operation_count\nFROM python.torch_trace\nWHERE step &gt; 100\nGROUP BY step, module\nORDER BY total_memory_mb DESC\nLIMIT 10\"\n</code></pre>"},{"location":"quickstart/#backtrace-debug-with-stack-context","title":"\ud83d\udd0d backtrace: Debug with Stack Context","text":"<p>Capture detailed call stacks with Python variable values:</p> <pre><code># Capture current call stack\nprobing $ENDPOINT backtrace\n\n# Query the stack trace\nprobing $ENDPOINT query \"SELECT func, file, lineno FROM python.backtrace ORDER BY depth LIMIT 3\"\n</code></pre>"},{"location":"quickstart/#real-world-debugging-scenarios","title":"Real-World Debugging Scenarios","text":""},{"location":"quickstart/#scenario-1-training-process-hanging","title":"Scenario 1: Training Process Hanging","text":"<p>Problem: PyTorch training suddenly stops progressing.</p> <pre><code># 1. See what main thread is doing\nprobing $ENDPOINT backtrace\n\n# 2. Check thread states\nprobing $ENDPOINT eval \"import threading; [(t.name, t.is_alive()) for t in threading.enumerate()]\"\n\n# 3. Analyze stack context\nprobing $ENDPOINT query \"SELECT func, file, lineno FROM python.backtrace ORDER BY depth LIMIT 10\"\n</code></pre>"},{"location":"quickstart/#scenario-2-memory-leak-investigation","title":"Scenario 2: Memory Leak Investigation","text":"<p>Problem: Memory usage keeps growing during training.</p> <pre><code># Force cleanup and get current state\nprobing $ENDPOINT eval \"import gc, torch; gc.collect(); torch.cuda.empty_cache()\"\n\n# Analyze allocation trends\nprobing $ENDPOINT query \"SELECT step, AVG(allocated) as avg_memory FROM python.torch_trace GROUP BY step ORDER BY step\"\n</code></pre>"},{"location":"quickstart/#scenario-3-performance-bottleneck-analysis","title":"Scenario 3: Performance Bottleneck Analysis","text":"<p>Problem: Need to identify which model components are slowest.</p> <pre><code># Find most expensive operations\nprobing $ENDPOINT query \"\nSELECT module, stage, AVG(duration) as avg_duration\nFROM python.torch_trace\nGROUP BY module, stage\nORDER BY avg_duration DESC\nLIMIT 10\"\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>SQL Analytics - Advanced query techniques</li> <li>Memory Analysis - Deep dive into memory debugging</li> <li>Debugging Guide - Expert debugging patterns</li> </ul>"},{"location":"versions/","title":"Version Compatibility","text":"<p>This page documents Probing version compatibility and changelog.</p>"},{"location":"versions/#current-version","title":"Current Version","text":"<p>Probing v0.6.x (Latest)</p>"},{"location":"versions/#system-requirements","title":"System Requirements","text":""},{"location":"versions/#python-version","title":"Python Version","text":"Probing Version Python Support 0.6.x Python 3.9 - 3.12 0.5.x Python 3.8 - 3.11"},{"location":"versions/#pytorch-version","title":"PyTorch Version","text":"Probing Version PyTorch Support 0.6.x PyTorch 2.0+ 0.5.x PyTorch 1.13+"},{"location":"versions/#operating-systems","title":"Operating Systems","text":"<ul> <li>Linux: Full support (recommended for production)</li> <li>macOS: Full support (Intel and Apple Silicon)</li> <li>Windows: Experimental (WSL2 recommended)</li> </ul>"},{"location":"versions/#changelog","title":"Changelog","text":""},{"location":"versions/#v060","title":"v0.6.0","text":"<p>New Features</p> <ul> <li>SQL query engine based on DataFusion</li> <li>Mermaid diagram support in documentation</li> <li>Improved distributed debugging support</li> <li>New <code>torch_trace</code> table for PyTorch profiling</li> </ul> <p>Breaking Changes</p> <ul> <li>Deprecated <code>probing.trace()</code> API, use <code>probing.enable_torch_profiling()</code> instead</li> <li>Configuration format changed from JSON to TOML</li> </ul> <p>Bug Fixes</p> <ul> <li>Fixed memory leak in long-running sessions</li> <li>Improved error messages for invalid SQL queries</li> </ul>"},{"location":"versions/#v050","title":"v0.5.0","text":"<p>New Features</p> <ul> <li>Initial PyTorch profiling support</li> <li>Memory analysis capabilities</li> <li>Basic SQL query support</li> </ul> <p>Bug Fixes</p> <ul> <li>Various stability improvements</li> </ul>"},{"location":"versions/#upgrade-guide","title":"Upgrade Guide","text":""},{"location":"versions/#from-v05x-to-v06x","title":"From v0.5.x to v0.6.x","text":"<ol> <li>Update Python to 3.9+ if needed</li> <li>Update PyTorch to 2.0+ if needed</li> <li>Update Probing:</li> </ol> <pre><code>pip install --upgrade probing\n</code></pre> <ol> <li>Update configuration files (if using custom config):</li> </ol> <pre><code># Old format (v0.5.x)\nprobing.trace(enabled=True)\n\n# New format (v0.6.x)\nprobing.enable_torch_profiling()\n</code></pre>"},{"location":"versions/#deprecation-policy","title":"Deprecation Policy","text":"<ul> <li>Major version changes may include breaking changes</li> <li>Minor version changes maintain backward compatibility</li> <li>Deprecated features show warnings for at least one minor version before removal</li> </ul>"},{"location":"versions/#feature-support-matrix","title":"Feature Support Matrix","text":"Feature v0.5.x v0.6.x Basic Profiling \u2705 \u2705 SQL Queries Partial \u2705 PyTorch Tracing Basic Full Memory Analysis Basic Full Distributed Support \u274c \u2705 Custom Tables \u274c \u2705 Web UI \u274c Beta"},{"location":"versions/#reporting-issues","title":"Reporting Issues","text":"<p>For bugs and feature requests, please use the GitHub Issue Tracker.</p> <p>When reporting issues, please include:</p> <ul> <li>Probing version (<code>pip show probing</code>)</li> <li>Python version (<code>python --version</code>)</li> <li>PyTorch version (if applicable)</li> <li>Operating system</li> <li>Minimal reproduction example</li> </ul>"},{"location":"design/","title":"Design Overview","text":""},{"location":"design/#why-probing","title":"Why Probing?","text":""},{"location":"design/#the-pythonic-advantage","title":"The Pythonic Advantage","text":"<p>Python's dominance in AI stems from one core principle: everything feels like Python. Whether you're using pandas, PyTorch, or NumPy, you can talk to them pythonically\u2014the same <code>print()</code>, iteration, and attribute access patterns work everywhere.</p>"},{"location":"design/#how-distributed-systems-break-this","title":"How Distributed Systems Break This","text":"<p>As AI models scale to distributed clusters, something fundamental breaks: distributed systems aren't Pythonic. Single-machine debugging feels natural\u2014<code>print(model.parameters())</code>, <code>loss.item()</code>, <code>torch.cuda.memory_allocated()</code>\u2014but distributed debugging forces you into system administration tools: <code>kubectl get nodes</code>, SSH sessions, log file parsing, monitoring dashboards.</p>"},{"location":"design/#probings-mission","title":"Probing's Mission","text":"<p>Probing's core mission is simple: make distributed systems feel Pythonic again. Your cluster, nodes, and distributed processes become accessible through familiar interfaces. Instead of context-switching between tools, you stay in Python and talk to your distributed system pythonically.</p>"},{"location":"design/#design-principles","title":"Design Principles","text":""},{"location":"design/#zero-intrusion","title":"\ud83d\udd0d Zero Intrusion","text":"<ul> <li>No code modifications required</li> <li>No environment setup changes needed</li> <li>No workflow disruptions</li> <li>Dynamic probe injection into running processes</li> </ul>"},{"location":"design/#zero-learning-curve","title":"\ud83c\udfaf Zero Learning Curve","text":"<ul> <li>Standard SQL interface for data analysis</li> <li>Familiar database query patterns</li> <li>Intuitive command-line tools</li> <li>Web-based dashboard for visualization</li> </ul>"},{"location":"design/#zero-deployment-burden","title":"\ud83d\udce6 Zero Deployment Burden","text":"<ul> <li>Single binary deployment (Rust-based)</li> <li>Static compilation with minimal dependencies</li> <li>Linux-first design with query/eval support on other platforms</li> <li>Elastic scaling capabilities</li> </ul>"},{"location":"design/#design-documents","title":"Design Documents","text":"Document Description Architecture System structure and components Profiling Performance data collection Debugging Debugging capabilities Distributed Multi-node support Extensibility Custom tables and metrics"},{"location":"design/architecture/","title":"System Architecture","text":"<p>Probing is designed as a simple two-layer structure to minimize complexity and deployment difficulty.</p>"},{"location":"design/architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TB\n    subgraph \"User Interface\"\n        CLI[CLI Client]\n        HTTP[HTTP API]\n        WEB[Web UI]\n    end\n\n    subgraph \"Target Process\"\n        PROBE[Probe]\n        subgraph \"Probe Components\"\n            ENGINE[Engine]\n            SERVER[Server]\n            EXT[Extensions]\n        end\n    end\n\n    CLI --&gt; |Unix Socket / TCP| PROBE\n    HTTP --&gt; |HTTP/REST| PROBE\n    WEB --&gt; |WebSocket| PROBE\n\n    PROBE --&gt; ENGINE\n    PROBE --&gt; SERVER\n    PROBE --&gt; EXT</code></pre>"},{"location":"design/architecture/#components","title":"Components","text":""},{"location":"design/architecture/#1-probe","title":"1. Probe","text":"<p>Injected into target processes to gain full access to all resources:</p> <ul> <li>Python interpreter access</li> <li>File system access</li> <li>Memory inspection</li> <li>Network capabilities</li> </ul> <p>The probe runs an embedded HTTP server that listens on:</p> <ul> <li>Unix domain socket - For local connections (default)</li> <li>TCP port - For remote connections</li> </ul>"},{"location":"design/architecture/#2-cli","title":"2. CLI","text":"<p>Command-line interface for controlling probes:</p> <ul> <li>Process discovery and listing</li> <li>Probe injection and management</li> <li>Query execution</li> <li>Code evaluation</li> </ul> <p>Communication happens via HTTP protocol over Unix domain sockets (local) or TCP (remote).</p>"},{"location":"design/architecture/#3-http-api","title":"3. HTTP API","text":"<p>RESTful API for programmatic access:</p> <ul> <li>All CLI commands available as endpoints</li> <li>WebSocket support for real-time data</li> <li>Integration with monitoring tools</li> </ul>"},{"location":"design/architecture/#probe-internal-architecture","title":"Probe Internal Architecture","text":"<pre><code>graph LR\n    subgraph \"Probe\"\n        SERVER[HTTP Server]\n        ENGINE[Query Engine]\n        CONFIG[Config Manager]\n\n        subgraph \"Extensions\"\n            PYTHON[Python Extension]\n            TORCH[PyTorch Extension]\n            CUSTOM[Custom Tables]\n        end\n    end\n\n    SERVER --&gt; ENGINE\n    SERVER --&gt; CONFIG\n    ENGINE --&gt; PYTHON\n    ENGINE --&gt; TORCH\n    ENGINE --&gt; CUSTOM</code></pre>"},{"location":"design/architecture/#engine","title":"Engine","text":"<p>Core data storage and processing:</p> <ul> <li>DataFusion - SQL query engine</li> <li>Arrow - Columnar data format</li> <li>Time-series data storage</li> <li>Configuration management</li> </ul>"},{"location":"design/architecture/#server","title":"Server","text":"<p>HTTP server handling:</p> <ul> <li>Request routing</li> <li>Authentication (optional)</li> <li>WebSocket connections</li> <li>Response formatting</li> </ul>"},{"location":"design/architecture/#extensions","title":"Extensions","text":"<p>Pluggable data providers:</p> <ul> <li>Python Extension - Backtrace, variables</li> <li>PyTorch Extension - Torch traces, memory</li> <li>Custom Tables - User-defined data sources</li> </ul>"},{"location":"design/architecture/#data-flow","title":"Data Flow","text":"<pre><code>sequenceDiagram\n    participant CLI\n    participant Probe\n    participant Engine\n    participant Extension\n\n    CLI-&gt;&gt;Probe: query \"SELECT * FROM python.torch_trace\"\n    Probe-&gt;&gt;Engine: Parse &amp; Plan Query\n    Engine-&gt;&gt;Extension: Request Data\n    Extension--&gt;&gt;Engine: Arrow RecordBatch\n    Engine--&gt;&gt;Probe: Query Results\n    Probe--&gt;&gt;CLI: JSON Response</code></pre>"},{"location":"design/architecture/#communication-protocol","title":"Communication Protocol","text":""},{"location":"design/architecture/#local-connection","title":"Local Connection","text":"<pre><code>probing -t &lt;pid&gt; query \"...\"\n         |\n         v\n    Unix Socket: /tmp/probing-&lt;pid&gt;.sock\n         |\n         v\n    HTTP Request: POST /query\n</code></pre>"},{"location":"design/architecture/#remote-connection","title":"Remote Connection","text":"<pre><code>probing -t host:port query \"...\"\n         |\n         v\n    TCP Connection: host:port\n         |\n         v\n    HTTP Request: POST /query\n</code></pre>"},{"location":"design/architecture/#security-considerations","title":"Security Considerations","text":"<ul> <li>Local mode: Unix socket permissions (process owner only)</li> <li>Remote mode: Optional authentication</li> <li>Network: Support for TLS encryption</li> </ul>"},{"location":"design/architecture/#performance-characteristics","title":"Performance Characteristics","text":"Aspect Target Overhead &lt; 5% in typical workloads Memory &lt; 50MB additional Latency &lt; 10ms for queries Throughput 1000+ queries/sec"},{"location":"design/debugging/","title":"Debugging Architecture","text":"<p>Probing provides powerful debugging capabilities through code injection and stack analysis.</p>"},{"location":"design/debugging/#overview","title":"Overview","text":"<p>The debugging subsystem enables:</p> <ul> <li>Live code execution in target processes</li> <li>Stack trace capture with variable inspection</li> <li>Interactive REPL sessions</li> <li>Remote debugging support</li> </ul>"},{"location":"design/debugging/#code-execution","title":"Code Execution","text":""},{"location":"design/debugging/#eval-command","title":"Eval Command","text":"<p>Execute arbitrary Python code in the target process context:</p> <pre><code>probing $ENDPOINT eval \"print(model.state_dict().keys())\"\n</code></pre>"},{"location":"design/debugging/#execution-flow","title":"Execution Flow","text":"<pre><code>sequenceDiagram\n    participant CLI\n    participant Server\n    participant Python\n\n    CLI-&gt;&gt;Server: POST /eval {\"code\": \"...\"}\n    Server-&gt;&gt;Python: PyRun_String(code)\n    Python-&gt;&gt;Python: Execute in __main__\n    Python--&gt;&gt;Server: Result/Exception\n    Server--&gt;&gt;CLI: JSON Response</code></pre>"},{"location":"design/debugging/#execution-context","title":"Execution Context","text":"<p>Code executes in the main module's global namespace:</p> <ul> <li>Access to all imported modules</li> <li>Access to global variables</li> <li>Can modify state directly</li> </ul>"},{"location":"design/debugging/#safety-considerations","title":"Safety Considerations","text":"<ul> <li>Code runs with full privileges</li> <li>No sandboxing (by design)</li> <li>Use with appropriate access controls</li> </ul>"},{"location":"design/debugging/#stack-analysis","title":"Stack Analysis","text":""},{"location":"design/debugging/#backtrace-capture","title":"Backtrace Capture","text":"<p>Captures current execution stack:</p> <pre><code>probing $ENDPOINT backtrace\n</code></pre>"},{"location":"design/debugging/#frame-information","title":"Frame Information","text":"<p>Each stack frame includes:</p> Field Description func Function name file Source file path lineno Line number depth Stack depth (0 = innermost) frame_type Python or Native locals Local variables (optional)"},{"location":"design/debugging/#query-stack","title":"Query Stack","text":"<pre><code>SELECT func, file, lineno, depth\nFROM python.backtrace\nORDER BY depth;\n</code></pre>"},{"location":"design/debugging/#interactive-repl","title":"Interactive REPL","text":""},{"location":"design/debugging/#starting-repl","title":"Starting REPL","text":"<pre><code>probing -t &lt;pid&gt; repl\n</code></pre>"},{"location":"design/debugging/#repl-features","title":"REPL Features","text":"<ul> <li>Tab completion</li> <li>Multi-line input</li> <li>History support</li> <li>Exception display</li> </ul>"},{"location":"design/debugging/#example-session","title":"Example Session","text":"<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; model = get_model()\n&gt;&gt;&gt; model.training\nTrue\n&gt;&gt;&gt; torch.cuda.memory_allocated() / 1024**3\n2.5\n</code></pre>"},{"location":"design/debugging/#remote-debugging","title":"Remote Debugging","text":""},{"location":"design/debugging/#setup","title":"Setup","text":"<pre><code># On target machine\nPROBING_PORT=8080 python train.py\n\n# From remote machine\nprobing -t host:8080 eval \"...\"\n</code></pre>"},{"location":"design/debugging/#security","title":"Security","text":"<ul> <li>Authentication via tokens</li> <li>TLS encryption support</li> <li>IP-based access control</li> </ul>"},{"location":"design/debugging/#debugging-patterns","title":"Debugging Patterns","text":""},{"location":"design/debugging/#finding-hangs","title":"Finding Hangs","text":"<pre><code># Capture stack\nprobing $ENDPOINT backtrace\n\n# Check where execution is blocked\nprobing $ENDPOINT query \"\nSELECT func, file, lineno\nFROM python.backtrace\nWHERE depth &lt; 5\"\n</code></pre>"},{"location":"design/debugging/#inspecting-state","title":"Inspecting State","text":"<pre><code># Check model state\nprobing $ENDPOINT eval \"\nfor name, param in model.named_parameters():\n    print(f'{name}: {param.shape}')\"\n</code></pre>"},{"location":"design/debugging/#modifying-behavior","title":"Modifying Behavior","text":"<pre><code># Change learning rate\nprobing $ENDPOINT eval \"\noptimizer.param_groups[0]['lr'] = 0.0001\"\n\n# Force checkpoint\nprobing $ENDPOINT eval \"\ntrainer.save_checkpoint('debug_checkpoint.pt')\"\n</code></pre>"},{"location":"design/debugging/#thread-debugging","title":"Thread Debugging","text":""},{"location":"design/debugging/#list-threads","title":"List Threads","text":"<pre><code>probing $ENDPOINT eval \"\nimport threading\nfor t in threading.enumerate():\n    print(f'{t.name}: alive={t.is_alive()}')\"\n</code></pre>"},{"location":"design/debugging/#main-thread-focus","title":"Main Thread Focus","text":"<p>Backtrace captures the main thread by default. For other threads:</p> <pre><code>probing $ENDPOINT eval \"\nimport sys, traceback\nfor thread_id, frame in sys._current_frames().items():\n    print(f'Thread {thread_id}:')\n    traceback.print_stack(frame)\"\n</code></pre>"},{"location":"design/debugging/#integration-with-ides","title":"Integration with IDEs","text":""},{"location":"design/debugging/#vs-code","title":"VS Code","text":"<p>The HTTP API can be used for IDE integration:</p> <pre><code>{\n  \"type\": \"probing\",\n  \"request\": \"attach\",\n  \"endpoint\": \"localhost:8080\"\n}\n</code></pre>"},{"location":"design/debugging/#jupyter","title":"Jupyter","text":"<pre><code>from probing import connect\n\nprobe = connect(pid=12345)\nprobe.eval(\"print(globals().keys())\")\n</code></pre>"},{"location":"design/distributed/","title":"Distributed Architecture","text":"<p>Probing supports monitoring and debugging distributed AI workloads across multiple nodes.</p>"},{"location":"design/distributed/#overview","title":"Overview","text":"<p>Distributed training introduces challenges:</p> <ul> <li>Multiple processes across nodes</li> <li>Communication between ranks</li> <li>Synchronized debugging needs</li> <li>Cross-node data correlation</li> </ul> <p>Probing addresses these through a distributed architecture.</p>"},{"location":"design/distributed/#architecture","title":"Architecture","text":"<pre><code>graph TB\n    subgraph \"Node 1\"\n        P1[Process Rank 0]\n        PROBE1[Probe]\n    end\n\n    subgraph \"Node 2\"\n        P2[Process Rank 1]\n        PROBE2[Probe]\n    end\n\n    subgraph \"Control Plane\"\n        CLI[CLI Client]\n        AGG[Aggregator]\n    end\n\n    PROBE1 --&gt; AGG\n    PROBE2 --&gt; AGG\n    CLI --&gt; AGG\n    CLI --&gt; PROBE1\n    CLI --&gt; PROBE2</code></pre>"},{"location":"design/distributed/#process-discovery","title":"Process Discovery","text":""},{"location":"design/distributed/#local-discovery","title":"Local Discovery","text":"<pre><code># List all probing-enabled processes on local machine\nprobing list\n</code></pre>"},{"location":"design/distributed/#remote-discovery","title":"Remote Discovery","text":"<pre><code># Connect to remote node\nprobing -t node1:8080 list\nprobing -t node2:8080 list\n</code></pre>"},{"location":"design/distributed/#cluster-view","title":"Cluster View","text":"<pre><code># Aggregate view across cluster\nprobing cluster list\n</code></pre>"},{"location":"design/distributed/#cross-node-queries","title":"Cross-Node Queries","text":""},{"location":"design/distributed/#query-single-node","title":"Query Single Node","text":"<pre><code>probing -t node1:8080 query \"\nSELECT * FROM python.torch_trace\nWHERE step = (SELECT MAX(step) FROM python.torch_trace)\"\n</code></pre>"},{"location":"design/distributed/#query-all-nodes","title":"Query All Nodes","text":"<pre><code># Federated query (future feature)\nprobing cluster query \"\nSELECT node, rank, step, loss\nFROM python.training_metrics\nORDER BY step\"\n</code></pre>"},{"location":"design/distributed/#synchronized-debugging","title":"Synchronized Debugging","text":""},{"location":"design/distributed/#capture-all-stacks","title":"Capture All Stacks","text":"<pre><code># Capture stack traces from all ranks\nfor node in node1 node2 node3; do\n    echo \"=== $node ===\"\n    probing -t $node:8080 backtrace\ndone\n</code></pre>"},{"location":"design/distributed/#check-distributed-state","title":"Check Distributed State","text":"<pre><code>probing -t $ENDPOINT eval \"\nimport torch.distributed as dist\n\nif dist.is_initialized():\n    print(f'Rank: {dist.get_rank()}')\n    print(f'World Size: {dist.get_world_size()}')\n    print(f'Backend: {dist.get_backend()}')\"\n</code></pre>"},{"location":"design/distributed/#communication-analysis","title":"Communication Analysis","text":""},{"location":"design/distributed/#nccl-operations","title":"NCCL Operations","text":"<pre><code>-- Analyze communication patterns\nSELECT\n    operation,\n    src_rank,\n    dst_rank,\n    bytes_transferred,\n    duration_ms\nFROM python.nccl_trace\nWHERE step = (SELECT MAX(step) FROM python.nccl_trace)\nORDER BY duration_ms DESC;\n</code></pre>"},{"location":"design/distributed/#rdma-flow-analysis","title":"RDMA Flow Analysis","text":"<pre><code># RDMA-specific analysis\nprobing -t $ENDPOINT rdma\n</code></pre>"},{"location":"design/distributed/#troubleshooting-distributed-issues","title":"Troubleshooting Distributed Issues","text":""},{"location":"design/distributed/#rank-synchronization","title":"Rank Synchronization","text":"<pre><code># Check if all ranks are at same step\nfor node in node1 node2 node3; do\n    probing -t $node:8080 eval \"print(f'Step: {trainer.current_step}')\"\ndone\n</code></pre>"},{"location":"design/distributed/#deadlock-detection","title":"Deadlock Detection","text":"<pre><code># Check for hanging collective operations\nprobing -t $ENDPOINT query \"\nSELECT func, file, lineno\nFROM python.backtrace\nWHERE func LIKE '%collective%' OR func LIKE '%allreduce%'\"\n</code></pre>"},{"location":"design/distributed/#memory-imbalance","title":"Memory Imbalance","text":"<pre><code>-- Compare memory across ranks\nSELECT\n    rank,\n    AVG(allocated) as avg_memory,\n    MAX(allocated) as peak_memory\nFROM python.torch_trace\nGROUP BY rank;\n</code></pre>"},{"location":"design/distributed/#configuration","title":"Configuration","text":""},{"location":"design/distributed/#enable-remote-access","title":"Enable Remote Access","text":"<pre><code># Start with TCP server\nPROBING_PORT=8080 python train.py\n\n# Or configure dynamically\nprobing $ENDPOINT config probing.server.port=8080\n</code></pre>"},{"location":"design/distributed/#security","title":"Security","text":"<pre><code># Enable authentication\nPROBING_AUTH_TOKEN=secret python train.py\n\n# Connect with token\nprobing -t host:8080 --token secret query \"...\"\n</code></pre>"},{"location":"design/distributed/#best-practices","title":"Best Practices","text":""},{"location":"design/distributed/#1-consistent-configuration","title":"1. Consistent Configuration","text":"<p>Use same configuration across all nodes:</p> <pre><code>export PROBING_PORT=8080\nexport PROBING_TORCH_PROFILING=on\n</code></pre>"},{"location":"design/distributed/#2-centralized-collection","title":"2. Centralized Collection","text":"<p>For large clusters, consider aggregation:</p> <pre><code># Export data to central location\nprobing -t $node query \"SELECT * FROM python.torch_trace\" &gt;&gt; /shared/traces.json\n</code></pre>"},{"location":"design/distributed/#3-timestamp-synchronization","title":"3. Timestamp Synchronization","text":"<p>Ensure NTP is configured for accurate cross-node timing.</p>"},{"location":"design/distributed/#4-network-considerations","title":"4. Network Considerations","text":"<ul> <li>Use dedicated network for probing traffic if possible</li> <li>Consider firewall rules for probing ports</li> <li>Monitor probing overhead on training network</li> </ul>"},{"location":"design/extensibility/","title":"Extensibility","text":"<p>Probing provides mechanisms for extending its capabilities with custom data sources and metrics.</p>"},{"location":"design/extensibility/#overview","title":"Overview","text":"<p>The extension system allows:</p> <ul> <li>Custom data tables</li> <li>User-defined metrics</li> <li>Integration with external tools</li> <li>Plugin architecture</li> </ul>"},{"location":"design/extensibility/#custom-tables","title":"Custom Tables","text":""},{"location":"design/extensibility/#python-api","title":"Python API","text":"<p>Create custom tables using the <code>@table</code> decorator:</p> <pre><code>from probing import table\n\n@table(\"my_metrics\")\ndef get_metrics():\n    \"\"\"Return data as list of dicts or pandas DataFrame.\"\"\"\n    return [\n        {\"name\": \"loss\", \"value\": current_loss},\n        {\"name\": \"accuracy\", \"value\": current_acc},\n    ]\n</code></pre>"},{"location":"design/extensibility/#query-custom-tables","title":"Query Custom Tables","text":"<pre><code>SELECT * FROM python.my_metrics;\n</code></pre>"},{"location":"design/extensibility/#table-schema","title":"Table Schema","text":"<p>Tables are dynamically typed based on returned data:</p> <pre><code>@table(\"training_state\")\ndef get_training_state():\n    return {\n        \"step\": trainer.current_step,      # int\n        \"loss\": trainer.last_loss,         # float\n        \"lr\": optimizer.param_groups[0][\"lr\"],  # float\n        \"epoch\": trainer.current_epoch,    # int\n    }\n</code></pre>"},{"location":"design/extensibility/#external-table-integration","title":"External Table Integration","text":""},{"location":"design/extensibility/#pandas-dataframes","title":"Pandas DataFrames","text":"<pre><code>import pandas as pd\nfrom probing import register_table\n\ndf = pd.DataFrame({\n    \"timestamp\": timestamps,\n    \"metric\": values\n})\n\nregister_table(\"external_metrics\", df)\n</code></pre>"},{"location":"design/extensibility/#arrow-tables","title":"Arrow Tables","text":"<pre><code>import pyarrow as pa\nfrom probing import register_table\n\ntable = pa.table({\n    \"id\": [1, 2, 3],\n    \"value\": [10.0, 20.0, 30.0]\n})\n\nregister_table(\"arrow_data\", table)\n</code></pre>"},{"location":"design/extensibility/#custom-metrics","title":"Custom Metrics","text":""},{"location":"design/extensibility/#defining-metrics","title":"Defining Metrics","text":"<pre><code>from probing import metric\n\n@metric(\"gpu_utilization\")\ndef gpu_util():\n    \"\"\"Return current GPU utilization.\"\"\"\n    import pynvml\n    pynvml.nvmlInit()\n    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n    util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n    return util.gpu\n</code></pre>"},{"location":"design/extensibility/#querying-metrics","title":"Querying Metrics","text":"<pre><code>SELECT * FROM python.metrics WHERE name = 'gpu_utilization';\n</code></pre>"},{"location":"design/extensibility/#hook-system","title":"Hook System","text":""},{"location":"design/extensibility/#module-hooks","title":"Module Hooks","text":"<pre><code>from probing import register_hook\n\n@register_hook(\"torch.nn.Linear\", \"forward\")\ndef linear_hook(module, input, output):\n    \"\"\"Called on every Linear forward pass.\"\"\"\n    record_custom_data({\n        \"module\": str(module),\n        \"input_shape\": list(input[0].shape),\n        \"output_shape\": list(output.shape),\n    })\n</code></pre>"},{"location":"design/extensibility/#function-hooks","title":"Function Hooks","text":"<pre><code>from probing import hook_function\n\n@hook_function(\"torch.optim.Adam.step\")\ndef optimizer_hook(optimizer):\n    \"\"\"Called on every optimizer step.\"\"\"\n    record_custom_data({\n        \"lr\": optimizer.param_groups[0][\"lr\"],\n        \"step_count\": optimizer.state_dict()[\"state\"][0][\"step\"],\n    })\n</code></pre>"},{"location":"design/extensibility/#plugin-architecture","title":"Plugin Architecture","text":""},{"location":"design/extensibility/#creating-a-plugin","title":"Creating a Plugin","text":"<pre><code># my_plugin.py\nfrom probing import Plugin\n\nclass MyPlugin(Plugin):\n    name = \"my_plugin\"\n\n    def on_load(self):\n        \"\"\"Called when plugin is loaded.\"\"\"\n        self.register_table(\"plugin_data\", self.get_data)\n\n    def on_unload(self):\n        \"\"\"Called when plugin is unloaded.\"\"\"\n        pass\n\n    def get_data(self):\n        return [{\"key\": \"value\"}]\n</code></pre>"},{"location":"design/extensibility/#loading-plugins","title":"Loading Plugins","text":"<pre><code># Environment variable\nPROBING_PLUGINS=my_plugin python train.py\n\n# Or programmatically\nimport probing\nprobing.load_plugin(\"my_plugin\")\n</code></pre>"},{"location":"design/extensibility/#configuration-extension","title":"Configuration Extension","text":""},{"location":"design/extensibility/#custom-config-options","title":"Custom Config Options","text":"<pre><code>from probing import config\n\n# Register custom config\nconfig.register(\"my_plugin.sample_rate\", default=0.1, type=float)\n\n# Use in plugin\nrate = config.get(\"my_plugin.sample_rate\")\n</code></pre>"},{"location":"design/extensibility/#query-configuration","title":"Query Configuration","text":"<pre><code>SELECT * FROM information_schema.df_settings\nWHERE name LIKE 'my_plugin.%';\n</code></pre>"},{"location":"design/extensibility/#integration-examples","title":"Integration Examples","text":""},{"location":"design/extensibility/#weights-biases","title":"Weights &amp; Biases","text":"<pre><code>from probing import table\nimport wandb\n\n@table(\"wandb_metrics\")\ndef get_wandb_metrics():\n    run = wandb.run\n    if run:\n        return {\n            \"run_id\": run.id,\n            \"step\": run.step,\n            \"summary\": dict(run.summary),\n        }\n    return {}\n</code></pre>"},{"location":"design/extensibility/#tensorboard","title":"TensorBoard","text":"<pre><code>from probing import table\nfrom torch.utils.tensorboard import SummaryWriter\n\nwriter = SummaryWriter()\n\n@table(\"tensorboard_scalars\")\ndef get_tb_scalars():\n    # Access TensorBoard data\n    return logged_scalars\n</code></pre>"},{"location":"design/extensibility/#prometheus","title":"Prometheus","text":"<pre><code>from probing import metric\nfrom prometheus_client import Gauge\n\ngpu_memory = Gauge(\"gpu_memory_bytes\", \"GPU memory usage\")\n\n@metric(\"prometheus_gpu_memory\")\ndef update_prometheus():\n    mem = torch.cuda.memory_allocated()\n    gpu_memory.set(mem)\n    return mem\n</code></pre>"},{"location":"design/extensibility/#best-practices","title":"Best Practices","text":""},{"location":"design/extensibility/#1-lightweight-data-collection","title":"1. Lightweight Data Collection","text":"<pre><code># Good: Return only necessary data\n@table(\"efficient\")\ndef get_efficient():\n    return {\"step\": step, \"loss\": loss}\n\n# Avoid: Expensive operations\n@table(\"expensive\")\ndef get_expensive():\n    return serialize_entire_model()  # Too heavy\n</code></pre>"},{"location":"design/extensibility/#2-error-handling","title":"2. Error Handling","text":"<pre><code>@table(\"safe_data\")\ndef get_safe_data():\n    try:\n        return {\"value\": compute_value()}\n    except Exception as e:\n        return {\"error\": str(e)}\n</code></pre>"},{"location":"design/extensibility/#3-caching","title":"3. Caching","text":"<pre><code>from functools import lru_cache\n\n@table(\"cached_data\")\n@lru_cache(maxsize=1)\ndef get_cached_data():\n    return expensive_computation()\n</code></pre>"},{"location":"design/profiling/","title":"Profiling Implementation","text":"<p>Probing provides comprehensive profiling capabilities for AI workloads.</p>"},{"location":"design/profiling/#overview","title":"Overview","text":"<p>The profiling system collects performance data with minimal overhead through:</p> <ul> <li>Event-based collection</li> <li>Efficient sampling strategies</li> <li>Columnar data storage</li> <li>SQL query interface</li> </ul>"},{"location":"design/profiling/#data-collection-architecture","title":"Data Collection Architecture","text":"<pre><code>graph TB\n    subgraph \"Data Sources\"\n        TORCH[PyTorch Hooks]\n        PYTHON[Python Frames]\n        SYSTEM[System Metrics]\n    end\n\n    subgraph \"Collection Layer\"\n        BUFFER[Ring Buffer]\n        SAMPLER[Sampler]\n    end\n\n    subgraph \"Storage Layer\"\n        ARROW[Arrow Tables]\n        QUERY[Query Engine]\n    end\n\n    TORCH --&gt; SAMPLER\n    PYTHON --&gt; SAMPLER\n    SYSTEM --&gt; SAMPLER\n\n    SAMPLER --&gt; BUFFER\n    BUFFER --&gt; ARROW\n    ARROW --&gt; QUERY</code></pre>"},{"location":"design/profiling/#pytorch-profiling","title":"PyTorch Profiling","text":""},{"location":"design/profiling/#hook-integration","title":"Hook Integration","text":"<p>Probing integrates with PyTorch's module hooks:</p> <pre><code># Forward hook\ndef forward_hook(module, input, output):\n    record_trace(module, \"forward\", memory_stats())\n\n# Backward hook\ndef backward_hook(module, grad_input, grad_output):\n    record_trace(module, \"backward\", memory_stats())\n</code></pre>"},{"location":"design/profiling/#collected-data","title":"Collected Data","text":"Field Type Description step int Training step number seq int Sequence within step module string Module name stage string forward/backward/step allocated float GPU memory allocated (MB) max_allocated float Peak GPU memory (MB) cached float GPU memory cached (MB) duration float Execution time (seconds)"},{"location":"design/profiling/#enable-pytorch-profiling","title":"Enable PyTorch Profiling","text":"<pre><code># Environment variable\nPROBING_TORCH_PROFILING=on python train.py\n\n# Or programmatically\nimport probing\nprobing.enable_torch_profiling()\n</code></pre>"},{"location":"design/profiling/#python-stack-profiling","title":"Python Stack Profiling","text":""},{"location":"design/profiling/#backtrace-collection","title":"Backtrace Collection","text":"<p>Captures Python call stack with:</p> <ul> <li>Function names</li> <li>File paths</li> <li>Line numbers</li> <li>Local variables (optional)</li> </ul> <pre><code># Stack frame data\n{\n    \"func\": \"forward\",\n    \"file\": \"/app/model.py\",\n    \"lineno\": 123,\n    \"depth\": 5,\n    \"frame_type\": \"Python\"\n}\n</code></pre>"},{"location":"design/profiling/#sampling-strategy","title":"Sampling Strategy","text":"<ul> <li>Periodic sampling: Configurable interval (default: 100ms)</li> <li>Event-triggered: On specific operations</li> <li>On-demand: Via backtrace command</li> </ul>"},{"location":"design/profiling/#system-metrics","title":"System Metrics","text":""},{"location":"design/profiling/#collected-metrics","title":"Collected Metrics","text":"<ul> <li>CPU utilization</li> <li>Memory usage (RSS, VMS)</li> <li>GPU utilization</li> <li>GPU memory</li> <li>I/O statistics</li> <li>Network statistics</li> </ul>"},{"location":"design/profiling/#collection-interval","title":"Collection Interval","text":"<pre><code># Configure sampling interval\nprobing $ENDPOINT config probing.sample_rate=0.1  # 100ms\n</code></pre>"},{"location":"design/profiling/#data-storage","title":"Data Storage","text":""},{"location":"design/profiling/#ring-buffer","title":"Ring Buffer","text":"<p>Efficient fixed-size buffer for recent data:</p> <ul> <li>Configurable size (default: 10000 records)</li> <li>Automatic eviction of old data</li> <li>Lock-free concurrent access</li> </ul>"},{"location":"design/profiling/#arrow-tables","title":"Arrow Tables","text":"<p>Columnar format for efficient queries:</p> <ul> <li>Vectorized operations</li> <li>Memory-mapped I/O</li> <li>Compression support</li> </ul>"},{"location":"design/profiling/#query-interface","title":"Query Interface","text":""},{"location":"design/profiling/#available-tables","title":"Available Tables","text":"<pre><code>-- List available tables\nSELECT table_name FROM information_schema.tables;\n\n-- Query torch traces\nSELECT * FROM python.torch_trace WHERE step &gt; 100;\n\n-- Query backtraces\nSELECT * FROM python.backtrace;\n</code></pre>"},{"location":"design/profiling/#custom-aggregations","title":"Custom Aggregations","text":"<pre><code>-- Per-module statistics\nSELECT\n    module,\n    COUNT(*) as count,\n    AVG(duration) as avg_duration,\n    MAX(allocated) as peak_memory\nFROM python.torch_trace\nGROUP BY module;\n</code></pre>"},{"location":"design/profiling/#performance-overhead","title":"Performance Overhead","text":""},{"location":"design/profiling/#measurement-results","title":"Measurement Results","text":"Scenario Overhead Idle (no profiling) &lt; 0.1% Basic profiling &lt; 1% Full PyTorch profiling &lt; 5% With variable capture &lt; 10%"},{"location":"design/profiling/#optimization-techniques","title":"Optimization Techniques","text":"<ol> <li>Lazy evaluation - Only compute metrics when queried</li> <li>Batched writes - Buffer multiple records before storage</li> <li>Sampling - Configurable sample rates</li> <li>Selective hooks - Enable only needed data sources</li> </ol>"},{"location":"examples/","title":"Examples","text":"<p>Real-world examples demonstrating Probing's capabilities.</p>"},{"location":"examples/#overview","title":"Overview","text":"<p>These examples show common debugging and profiling scenarios in AI/ML workflows.</p> Example Description Training Debugging Debug training issues Memory Leak Find and fix memory leaks Performance Analysis Identify bottlenecks"},{"location":"examples/#quick-examples","title":"Quick Examples","text":""},{"location":"examples/#check-training-progress","title":"Check Training Progress","text":"<pre><code>probing $ENDPOINT eval \"\nprint(f'Step: {trainer.current_step}')\nprint(f'Loss: {trainer.last_loss:.4f}')\nprint(f'LR: {optimizer.param_groups[0][\\\"lr\\\"]:.6f}')\"\n</code></pre>"},{"location":"examples/#monitor-gpu-memory","title":"Monitor GPU Memory","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\nallocated = torch.cuda.memory_allocated() / 1024**3\nreserved = torch.cuda.memory_reserved() / 1024**3\nprint(f'Allocated: {allocated:.2f} GB')\nprint(f'Reserved: {reserved:.2f} GB')\"\n</code></pre>"},{"location":"examples/#find-slow-operations","title":"Find Slow Operations","text":"<pre><code>probing $ENDPOINT query \"\nSELECT module, AVG(duration) as avg_time\nFROM python.torch_trace\nWHERE step &gt; (SELECT MAX(step) - 5 FROM python.torch_trace)\nGROUP BY module\nORDER BY avg_time DESC\nLIMIT 5\"\n</code></pre>"},{"location":"examples/#check-thread-states","title":"Check Thread States","text":"<pre><code>probing $ENDPOINT eval \"\nimport threading\nfor t in threading.enumerate():\n    print(f'{t.name}: alive={t.is_alive()}, daemon={t.daemon}')\"\n</code></pre>"},{"location":"examples/#running-examples","title":"Running Examples","text":"<p>All examples assume you have:</p> <ol> <li>A running Python process with Probing enabled</li> <li>The <code>$ENDPOINT</code> environment variable set</li> </ol> <pre><code># Set endpoint\nexport ENDPOINT=12345  # Process ID\n# or\nexport ENDPOINT=host:8080  # Remote address\n\n# Run example commands\nprobing $ENDPOINT eval \"...\"\n</code></pre>"},{"location":"examples/#contributing-examples","title":"Contributing Examples","text":"<p>Have a useful debugging pattern? Contributions welcome!</p> <ol> <li>Fork the repository</li> <li>Add your example to <code>docs/src/examples/</code></li> <li>Submit a pull request</li> </ol>"},{"location":"examples/memory-leak/","title":"Memory Leak Examples","text":"<p>Detecting and fixing memory leaks in AI applications.</p>"},{"location":"examples/memory-leak/#identifying-memory-leaks","title":"Identifying Memory Leaks","text":""},{"location":"examples/memory-leak/#memory-growth-pattern","title":"Memory Growth Pattern","text":"<pre><code># Track memory over steps\nprobing $ENDPOINT query \"\nSELECT\n    step,\n    MAX(allocated) as peak_memory_mb\nFROM python.torch_trace\nGROUP BY step\nORDER BY step\"\n</code></pre>"},{"location":"examples/memory-leak/#monotonic-growth-detection","title":"Monotonic Growth Detection","text":"<pre><code>WITH memory_trend AS (\n  SELECT\n    step,\n    MAX(allocated) as peak,\n    LAG(MAX(allocated)) OVER (ORDER BY step) as prev_peak\n  FROM python.torch_trace\n  GROUP BY step\n)\nSELECT\n    step,\n    peak,\n    peak - prev_peak as growth\nFROM memory_trend\nWHERE peak &gt; prev_peak\nORDER BY step;\n</code></pre>"},{"location":"examples/memory-leak/#common-leak-patterns","title":"Common Leak Patterns","text":""},{"location":"examples/memory-leak/#pattern-1-accumulating-tensors-in-lists","title":"Pattern 1: Accumulating Tensors in Lists","text":"<p>Problem:</p> <pre><code># BAD: Tensors accumulate in list\nall_losses = []\nfor batch in dataloader:\n    loss = model(batch)\n    all_losses.append(loss)  # Holds computation graph!\n</code></pre> <p>Detection:</p> <pre><code>probing $ENDPOINT eval \"\nimport gc\nimport torch\ntensors = [obj for obj in gc.get_objects() if isinstance(obj, torch.Tensor)]\nprint(f'Total tensors: {len(tensors)}')\"\n</code></pre> <p>Fix:</p> <pre><code>probing $ENDPOINT eval \"\n# Use .item() to extract scalar\nall_losses.clear()\nprint('Cleared loss list')\"\n</code></pre>"},{"location":"examples/memory-leak/#pattern-2-forgotten-gradient-graphs","title":"Pattern 2: Forgotten Gradient Graphs","text":"<p>Problem:</p> <pre><code># BAD: Intermediate tensors hold grad_fn\nintermediate = model.encoder(x)\n# ... lots of operations ...\n# intermediate still holds computation graph\n</code></pre> <p>Detection:</p> <pre><code>probing $ENDPOINT eval \"\nimport torch\nfor name, param in model.named_parameters():\n    if param.grad is not None:\n        print(f'{name}: grad_fn={param.grad.grad_fn}')\"\n</code></pre> <p>Fix:</p> <pre><code>probing $ENDPOINT eval \"\nmodel.zero_grad(set_to_none=True)\nimport torch\ntorch.cuda.empty_cache()\nprint('Cleared gradients')\"\n</code></pre>"},{"location":"examples/memory-leak/#pattern-3-circular-references","title":"Pattern 3: Circular References","text":"<p>Detection:</p> <pre><code>probing $ENDPOINT eval \"\nimport gc\ngc.set_debug(gc.DEBUG_SAVEALL)\ngc.collect()\nprint(f'Uncollectable: {len(gc.garbage)}')\"\n</code></pre>"},{"location":"examples/memory-leak/#gpu-memory-leaks","title":"GPU Memory Leaks","text":""},{"location":"examples/memory-leak/#check-cuda-memory-state","title":"Check CUDA Memory State","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\nprint(f'Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB')\nprint(f'Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB')\nprint(f'Max allocated: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB')\"\n</code></pre>"},{"location":"examples/memory-leak/#memory-snapshot","title":"Memory Snapshot","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\nif torch.cuda.is_available():\n    snapshot = torch.cuda.memory_snapshot()\n    print(f'Number of blocks: {len(snapshot)}')\"\n</code></pre>"},{"location":"examples/memory-leak/#force-cuda-cleanup","title":"Force CUDA Cleanup","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\nimport gc\n\n# Clear all references\ngc.collect()\n\n# Empty CUDA cache\ntorch.cuda.empty_cache()\n\n# Reset peak stats\ntorch.cuda.reset_peak_memory_stats()\n\nprint('CUDA memory cleaned')\"\n</code></pre>"},{"location":"examples/memory-leak/#cpu-memory-leaks","title":"CPU Memory Leaks","text":""},{"location":"examples/memory-leak/#track-process-memory","title":"Track Process Memory","text":"<pre><code>probing $ENDPOINT eval \"\nimport psutil\nimport os\n\nproc = psutil.Process(os.getpid())\nmem = proc.memory_info()\nprint(f'RSS: {mem.rss / 1024**3:.2f} GB')\nprint(f'VMS: {mem.vms / 1024**3:.2f} GB')\"\n</code></pre>"},{"location":"examples/memory-leak/#find-large-objects","title":"Find Large Objects","text":"<pre><code>probing $ENDPOINT eval \"\nimport sys\nimport gc\n\n# Find largest objects\nobjects = gc.get_objects()\nsizes = [(sys.getsizeof(obj), type(obj).__name__) for obj in objects[:1000]]\nsizes.sort(reverse=True)\nfor size, name in sizes[:10]:\n    print(f'{name}: {size / 1024:.1f} KB')\"\n</code></pre>"},{"location":"examples/memory-leak/#data-loader-leaks","title":"Data Loader Leaks","text":""},{"location":"examples/memory-leak/#check-worker-state","title":"Check Worker State","text":"<pre><code>probing $ENDPOINT eval \"\nprint(f'Num workers: {train_loader.num_workers}')\nprint(f'Batch size: {train_loader.batch_size}')\"\n</code></pre>"},{"location":"examples/memory-leak/#inspect-prefetched-data","title":"Inspect Prefetched Data","text":"<pre><code>probing $ENDPOINT eval \"\n# Check if data is being held\nif hasattr(train_loader, '_iterator'):\n    print('Iterator exists')\nelse:\n    print('No active iterator')\"\n</code></pre>"},{"location":"examples/memory-leak/#monitoring-over-time","title":"Monitoring Over Time","text":""},{"location":"examples/memory-leak/#periodic-memory-check","title":"Periodic Memory Check","text":"<pre><code># Run every minute\nwhile true; do\n    probing $ENDPOINT eval \"\nimport torch\nimport psutil\nimport os\nproc = psutil.Process(os.getpid())\nprint(f'CPU: {proc.memory_info().rss / 1024**3:.2f} GB, GPU: {torch.cuda.memory_allocated() / 1024**3:.2f} GB')\"\n    sleep 60\ndone\n</code></pre>"},{"location":"examples/memory-leak/#sql-based-monitoring","title":"SQL-Based Monitoring","text":"<pre><code>-- Memory trend over last 100 steps\nSELECT\n    step,\n    AVG(allocated) as avg_memory,\n    MAX(allocated) as peak_memory\nFROM python.torch_trace\nWHERE step &gt; (SELECT MAX(step) - 100 FROM python.torch_trace)\nGROUP BY step\nORDER BY step;\n</code></pre>"},{"location":"examples/memory-leak/#prevention-best-practices","title":"Prevention Best Practices","text":""},{"location":"examples/memory-leak/#1-use-context-managers","title":"1. Use Context Managers","text":"<pre><code># Good practice\nwith torch.no_grad():\n    output = model(input)\n</code></pre>"},{"location":"examples/memory-leak/#2-detach-tensors","title":"2. Detach Tensors","text":"<pre><code># When storing for later\nstored_output = output.detach().cpu()\n</code></pre>"},{"location":"examples/memory-leak/#3-clear-caches-periodically","title":"3. Clear Caches Periodically","text":"<pre><code>if step % 100 == 0:\n    gc.collect()\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"examples/memory-leak/#4-use-set_to_nonetrue","title":"4. Use <code>set_to_none=True</code>","text":"<pre><code>optimizer.zero_grad(set_to_none=True)  # More efficient\n</code></pre>"},{"location":"examples/performance-analysis/","title":"Performance Analysis Examples","text":"<p>Identify and fix performance bottlenecks in AI workloads.</p>"},{"location":"examples/performance-analysis/#finding-bottlenecks","title":"Finding Bottlenecks","text":""},{"location":"examples/performance-analysis/#overall-performance-profile","title":"Overall Performance Profile","text":"<pre><code>probing $ENDPOINT query \"\nSELECT\n    module,\n    stage,\n    COUNT(*) as executions,\n    AVG(duration) as avg_time_sec,\n    SUM(duration) as total_time_sec,\n    SUM(duration) * 100.0 / SUM(SUM(duration)) OVER () as pct_time\nFROM python.torch_trace\nWHERE step &gt; (SELECT MAX(step) - 10 FROM python.torch_trace)\nGROUP BY module, stage\nORDER BY total_time_sec DESC\nLIMIT 15\"\n</code></pre>"},{"location":"examples/performance-analysis/#per-step-breakdown","title":"Per-Step Breakdown","text":"<pre><code>probing $ENDPOINT query \"\nSELECT\n    step,\n    SUM(CASE WHEN stage = 'forward' THEN duration ELSE 0 END) as forward_time,\n    SUM(CASE WHEN stage = 'backward' THEN duration ELSE 0 END) as backward_time,\n    SUM(CASE WHEN stage = 'step' THEN duration ELSE 0 END) as optimizer_time\nFROM python.torch_trace\nWHERE step &gt; (SELECT MAX(step) - 5 FROM python.torch_trace)\nGROUP BY step\nORDER BY step\"\n</code></pre>"},{"location":"examples/performance-analysis/#gpu-utilization","title":"GPU Utilization","text":""},{"location":"examples/performance-analysis/#check-current-utilization","title":"Check Current Utilization","text":"<pre><code>probing $ENDPOINT eval \"\nimport subprocess\nresult = subprocess.run(\n    ['nvidia-smi', '--query-gpu=utilization.gpu,utilization.memory,temperature.gpu',\n     '--format=csv,noheader,nounits'],\n    capture_output=True, text=True\n)\nfor i, line in enumerate(result.stdout.strip().split('\\\\n')):\n    gpu_util, mem_util, temp = line.split(', ')\n    print(f'GPU {i}: Util={gpu_util}%, Mem={mem_util}%, Temp={temp}\u00b0C')\"\n</code></pre>"},{"location":"examples/performance-analysis/#cuda-synchronization-overhead","title":"CUDA Synchronization Overhead","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\nimport time\n\n# Measure sync overhead\nstart = time.perf_counter()\ntorch.cuda.synchronize()\nsync_time = time.perf_counter() - start\nprint(f'CUDA sync time: {sync_time*1000:.2f} ms')\"\n</code></pre>"},{"location":"examples/performance-analysis/#memory-bandwidth","title":"Memory Bandwidth","text":""},{"location":"examples/performance-analysis/#memory-bound-operations","title":"Memory-Bound Operations","text":"<pre><code>probing $ENDPOINT query \"\nSELECT\n    module,\n    AVG(allocated) as avg_memory_mb,\n    AVG(duration) as avg_time_sec,\n    AVG(allocated) / AVG(duration) as memory_bandwidth_mb_per_sec\nFROM python.torch_trace\nWHERE duration &gt; 0.001\nGROUP BY module\nORDER BY memory_bandwidth_mb_per_sec DESC\nLIMIT 10\"\n</code></pre>"},{"location":"examples/performance-analysis/#data-loading-performance","title":"Data Loading Performance","text":""},{"location":"examples/performance-analysis/#data-loader-timing","title":"Data Loader Timing","text":"<pre><code>probing $ENDPOINT eval \"\nimport time\n\n# Time one batch load\nstart = time.perf_counter()\nbatch = next(iter(train_loader))\nload_time = time.perf_counter() - start\nprint(f'Batch load time: {load_time*1000:.2f} ms')\"\n</code></pre>"},{"location":"examples/performance-analysis/#worker-analysis","title":"Worker Analysis","text":"<pre><code>probing $ENDPOINT eval \"\nprint(f'Num workers: {train_loader.num_workers}')\nprint(f'Pin memory: {train_loader.pin_memory}')\nprint(f'Prefetch factor: {getattr(train_loader, \\\"prefetch_factor\\\", 2)}')\"\n</code></pre>"},{"location":"examples/performance-analysis/#communication-overhead-distributed","title":"Communication Overhead (Distributed)","text":""},{"location":"examples/performance-analysis/#nccl-operation-times","title":"NCCL Operation Times","text":"<pre><code>probing $ENDPOINT query \"\nSELECT\n    operation_type,\n    COUNT(*) as count,\n    AVG(duration_ms) as avg_time_ms,\n    MAX(duration_ms) as max_time_ms\nFROM python.nccl_trace\nGROUP BY operation_type\nORDER BY avg_time_ms DESC\"\n</code></pre>"},{"location":"examples/performance-analysis/#all-reduce-scaling","title":"All-Reduce Scaling","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch.distributed as dist\nimport time\n\nif dist.is_initialized():\n    tensor = torch.randn(1000000, device='cuda')\n\n    start = time.perf_counter()\n    dist.all_reduce(tensor)\n    torch.cuda.synchronize()\n    allreduce_time = time.perf_counter() - start\n\n    print(f'All-reduce time for 4MB: {allreduce_time*1000:.2f} ms')\"\n</code></pre>"},{"location":"examples/performance-analysis/#attention-bottlenecks","title":"Attention Bottlenecks","text":""},{"location":"examples/performance-analysis/#self-attention-analysis","title":"Self-Attention Analysis","text":"<pre><code>probing $ENDPOINT query \"\nSELECT\n    module,\n    AVG(duration) as avg_time,\n    AVG(allocated) as avg_memory\nFROM python.torch_trace\nWHERE module LIKE '%attention%' OR module LIKE '%attn%'\nGROUP BY module\nORDER BY avg_time DESC\"\n</code></pre>"},{"location":"examples/performance-analysis/#memory-per-sequence-length","title":"Memory per Sequence Length","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\n\n# Check attention memory scaling\nseq_len = model.config.max_position_embeddings\nhidden = model.config.hidden_size\nnum_heads = model.config.num_attention_heads\n\n# Attention score memory: O(seq_len^2)\nattention_memory = seq_len * seq_len * num_heads * 4 / 1024**3  # GB\nprint(f'Estimated attention memory: {attention_memory:.2f} GB')\"\n</code></pre>"},{"location":"examples/performance-analysis/#optimization-recommendations","title":"Optimization Recommendations","text":""},{"location":"examples/performance-analysis/#profile-guided-optimization","title":"Profile-Guided Optimization","text":"<pre><code># 1. Identify slowest modules\nprobing $ENDPOINT query \"\nSELECT module, AVG(duration) as avg_time\nFROM python.torch_trace\nGROUP BY module\nORDER BY avg_time DESC\nLIMIT 5\"\n\n# 2. Check if compute-bound or memory-bound\nprobing $ENDPOINT eval \"\nimport torch\n# High compute utilization + low memory bandwidth = compute-bound\n# Low compute utilization + high memory utilization = memory-bound\"\n</code></pre>"},{"location":"examples/performance-analysis/#common-optimizations","title":"Common Optimizations","text":""},{"location":"examples/performance-analysis/#enable-torch-compile","title":"Enable Torch Compile","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\nif hasattr(torch, 'compile'):\n    model = torch.compile(model)\n    print('Model compiled with torch.compile')\"\n</code></pre>"},{"location":"examples/performance-analysis/#enable-mixed-precision","title":"Enable Mixed Precision","text":"<pre><code>probing $ENDPOINT eval \"\nfrom torch.cuda.amp import autocast\nprint(f'AMP enabled: {torch.cuda.amp.autocast_mode.is_autocast_enabled()}')\"\n</code></pre>"},{"location":"examples/performance-analysis/#check-gradient-checkpointing","title":"Check Gradient Checkpointing","text":"<pre><code>probing $ENDPOINT eval \"\n# Check if gradient checkpointing is enabled\nfor name, module in model.named_modules():\n    if hasattr(module, 'gradient_checkpointing'):\n        print(f'{name}: checkpoint={module.gradient_checkpointing}')\"\n</code></pre>"},{"location":"examples/performance-analysis/#benchmarking","title":"Benchmarking","text":""},{"location":"examples/performance-analysis/#throughput-measurement","title":"Throughput Measurement","text":"<pre><code>probing $ENDPOINT eval \"\nimport time\n\n# Measure throughput over 10 steps\nsteps = 10\nstart = time.perf_counter()\nfor _ in range(steps):\n    trainer.train_step()\nelapsed = time.perf_counter() - start\n\nsamples_per_sec = (steps * batch_size) / elapsed\nprint(f'Throughput: {samples_per_sec:.1f} samples/sec')\"\n</code></pre>"},{"location":"examples/performance-analysis/#compare-beforeafter","title":"Compare Before/After","text":"<pre><code># Before optimization\nprobing $ENDPOINT query \"\nSELECT AVG(duration) as before_avg\nFROM python.torch_trace\nWHERE step BETWEEN 100 AND 110\"\n\n# After optimization\nprobing $ENDPOINT query \"\nSELECT AVG(duration) as after_avg\nFROM python.torch_trace\nWHERE step BETWEEN 200 AND 210\"\n</code></pre>"},{"location":"examples/training-debugging/","title":"Training Debugging Examples","text":"<p>Common training debugging scenarios and solutions.</p>"},{"location":"examples/training-debugging/#training-hangs","title":"Training Hangs","text":""},{"location":"examples/training-debugging/#symptoms","title":"Symptoms","text":"<ul> <li>Loss stops updating</li> <li>No error messages</li> <li>Process still running</li> </ul>"},{"location":"examples/training-debugging/#diagnosis","title":"Diagnosis","text":"<pre><code># 1. Capture stack trace\nprobing $ENDPOINT backtrace\n\n# 2. Check where execution is stuck\nprobing $ENDPOINT query \"\nSELECT func, file, lineno, depth\nFROM python.backtrace\nORDER BY depth\nLIMIT 10\"\n\n# 3. Check thread states\nprobing $ENDPOINT eval \"\nimport threading\nfor t in threading.enumerate():\n    print(f'{t.name}: alive={t.is_alive()}')\"\n</code></pre>"},{"location":"examples/training-debugging/#common-causes","title":"Common Causes","text":""},{"location":"examples/training-debugging/#data-loader-stall","title":"Data Loader Stall","text":"<pre><code># Check data loader\nprobing $ENDPOINT eval \"\nimport torch.utils.data\nprint(f'DataLoader workers: {train_loader.num_workers}')\nprint(f'Prefetch factor: {train_loader.prefetch_factor}')\"\n</code></pre>"},{"location":"examples/training-debugging/#distributed-deadlock","title":"Distributed Deadlock","text":"<pre><code># Check distributed state\nprobing $ENDPOINT eval \"\nimport torch.distributed as dist\nif dist.is_initialized():\n    print(f'Rank: {dist.get_rank()}')\n    print(f'Waiting for barrier...')\n    # Don't actually call barrier here!\"\n</code></pre>"},{"location":"examples/training-debugging/#loss-explosion","title":"Loss Explosion","text":""},{"location":"examples/training-debugging/#symptoms_1","title":"Symptoms","text":"<ul> <li>Loss becomes NaN or Inf</li> <li>Training diverges</li> </ul>"},{"location":"examples/training-debugging/#diagnosis_1","title":"Diagnosis","text":"<pre><code># Check for NaN in gradients\nprobing $ENDPOINT eval \"\nimport torch\nfor name, param in model.named_parameters():\n    if param.grad is not None:\n        has_nan = torch.isnan(param.grad).any().item()\n        has_inf = torch.isinf(param.grad).any().item()\n        if has_nan or has_inf:\n            print(f'{name}: NaN={has_nan}, Inf={has_inf}')\"\n</code></pre>"},{"location":"examples/training-debugging/#fix-gradient-clipping","title":"Fix: Gradient Clipping","text":"<pre><code># Check current clipping\nprobing $ENDPOINT eval \"\nprint(f'Grad clip value: {getattr(trainer, \\\"grad_clip\\\", None)}')\"\n\n# Apply emergency clipping\nprobing $ENDPOINT eval \"\ntorch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\"\n</code></pre>"},{"location":"examples/training-debugging/#slow-training","title":"Slow Training","text":""},{"location":"examples/training-debugging/#diagnosis_2","title":"Diagnosis","text":"<pre><code># Find slowest modules\nprobing $ENDPOINT query \"\nSELECT\n    module,\n    stage,\n    COUNT(*) as count,\n    AVG(duration) as avg_time,\n    SUM(duration) as total_time\nFROM python.torch_trace\nWHERE step &gt; (SELECT MAX(step) - 5 FROM python.torch_trace)\nGROUP BY module, stage\nORDER BY total_time DESC\nLIMIT 10\"\n</code></pre>"},{"location":"examples/training-debugging/#check-gpu-utilization","title":"Check GPU Utilization","text":"<pre><code>probing $ENDPOINT eval \"\nimport subprocess\nresult = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader'],\n                        capture_output=True, text=True)\nprint(f'GPU Utilization: {result.stdout.strip()}')\"\n</code></pre>"},{"location":"examples/training-debugging/#memory-issues-during-training","title":"Memory Issues During Training","text":""},{"location":"examples/training-debugging/#track-memory-growth","title":"Track Memory Growth","text":"<pre><code>probing $ENDPOINT query \"\nSELECT\n    step,\n    MAX(allocated) as peak_memory,\n    MAX(allocated) - MIN(allocated) as memory_range\nFROM python.torch_trace\nWHERE step &gt; (SELECT MAX(step) - 20 FROM python.torch_trace)\nGROUP BY step\nORDER BY step\"\n</code></pre>"},{"location":"examples/training-debugging/#force-cleanup","title":"Force Cleanup","text":"<pre><code>probing $ENDPOINT eval \"\nimport gc\nimport torch\n\n# Clear gradients\nmodel.zero_grad(set_to_none=True)\n\n# Garbage collection\ngc.collect()\n\n# Clear CUDA cache\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint('Cleanup complete')\"\n</code></pre>"},{"location":"examples/training-debugging/#checkpoint-recovery","title":"Checkpoint Recovery","text":""},{"location":"examples/training-debugging/#save-emergency-checkpoint","title":"Save Emergency Checkpoint","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\ncheckpoint = {\n    'step': trainer.current_step,\n    'model': model.state_dict(),\n    'optimizer': optimizer.state_dict(),\n}\ntorch.save(checkpoint, 'emergency_checkpoint.pt')\nprint('Emergency checkpoint saved')\"\n</code></pre>"},{"location":"examples/training-debugging/#inspect-checkpoint","title":"Inspect Checkpoint","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\nckpt = torch.load('checkpoint.pt', map_location='cpu')\nprint(f'Keys: {ckpt.keys()}')\nprint(f'Step: {ckpt.get(\\\"step\\\", \\\"N/A\\\")}')\"\n</code></pre>"},{"location":"examples/training-debugging/#learning-rate-issues","title":"Learning Rate Issues","text":""},{"location":"examples/training-debugging/#check-current-lr","title":"Check Current LR","text":"<pre><code>probing $ENDPOINT eval \"\nfor i, pg in enumerate(optimizer.param_groups):\n    print(f'Group {i}: lr={pg[\\\"lr\\\"]}, weight_decay={pg.get(\\\"weight_decay\\\", 0)}')\"\n</code></pre>"},{"location":"examples/training-debugging/#adjust-lr","title":"Adjust LR","text":"<pre><code># Reduce learning rate\nprobing $ENDPOINT eval \"\nfor pg in optimizer.param_groups:\n    pg['lr'] *= 0.1\nprint(f'New LR: {optimizer.param_groups[0][\\\"lr\\\"]}')\"\n</code></pre>"},{"location":"examples/training-debugging/#distributed-training-issues","title":"Distributed Training Issues","text":""},{"location":"examples/training-debugging/#check-all-ranks","title":"Check All Ranks","text":"<pre><code># Run on each node\nprobing -t node1:8080 eval \"print(f'Rank 0 step: {trainer.current_step}')\"\nprobing -t node2:8080 eval \"print(f'Rank 1 step: {trainer.current_step}')\"\n</code></pre>"},{"location":"examples/training-debugging/#verify-synchronization","title":"Verify Synchronization","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch.distributed as dist\nif dist.is_initialized():\n    tensor = torch.tensor([trainer.current_step], device='cuda')\n    dist.all_reduce(tensor)\n    print(f'Sum of steps across ranks: {tensor.item()}')\"\n</code></pre>"},{"location":"guide/","title":"User Guide","text":"<p>Welcome to the Probing User Guide. This section covers the core features and usage patterns.</p>"},{"location":"guide/#overview","title":"Overview","text":"<p>Probing provides three core capabilities for analyzing and debugging your AI applications:</p> Capability Command Description eval <code>probing $ENDPOINT eval \"...\"</code> Execute Python code in target process query <code>probing $ENDPOINT query \"...\"</code> Query performance data with SQL backtrace <code>probing $ENDPOINT backtrace</code> Capture execution stack with variables"},{"location":"guide/#getting-started","title":"Getting Started","text":"<p>If you're new to Probing, we recommend starting with these guides in order:</p> <ol> <li>SQL Analytics - Learn the powerful SQL query interface</li> <li>Memory Analysis - Debug memory leaks and usage patterns</li> <li>Debugging - Master stack analysis and live debugging</li> <li>Troubleshooting - Common issues and solutions</li> </ol>"},{"location":"guide/#core-concepts","title":"Core Concepts","text":""},{"location":"guide/#target-endpoint","title":"Target Endpoint","text":"<p>All Probing commands require a target endpoint, which can be:</p> <ul> <li>Process ID: Local process (e.g., <code>12345</code>)</li> <li>Remote Address: Network endpoint (e.g., <code>host:8080</code>)</li> </ul> <pre><code># Set target endpoint\nexport ENDPOINT=12345  # or host:8080\n</code></pre>"},{"location":"guide/#data-tables","title":"Data Tables","text":"<p>Probing exposes performance data through SQL tables:</p> Table Description <code>python.backtrace</code> Stack trace information <code>python.torch_trace</code> PyTorch execution traces <code>python.variables</code> Variable tracking <code>information_schema.df_settings</code> Configuration settings"},{"location":"guide/#workflow-patterns","title":"Workflow Patterns","text":"<p>Debugging Workflow: <pre><code># 1. Capture current state\nprobing $ENDPOINT backtrace\n\n# 2. Inspect specific values\nprobing $ENDPOINT eval \"print(my_variable)\"\n\n# 3. Query historical data\nprobing $ENDPOINT query \"SELECT * FROM python.torch_trace\"\n</code></pre></p>"},{"location":"guide/#advanced-topics","title":"Advanced Topics","text":"<ul> <li>Architecture - System design and internals</li> <li>Distributed - Multi-node monitoring</li> <li>Extensibility - Custom tables and metrics</li> </ul>"},{"location":"guide/debugging/","title":"Debugging Guide","text":"<p>Master the art of debugging AI applications with Probing's powerful introspection capabilities.</p>"},{"location":"guide/debugging/#overview","title":"Overview","text":"<p>Probing provides three complementary debugging approaches:</p> <ul> <li>backtrace: Capture execution context with stack frames</li> <li>eval: Execute arbitrary Python code in the target process</li> <li>query: Analyze collected data with SQL</li> </ul>"},{"location":"guide/debugging/#stack-analysis","title":"Stack Analysis","text":""},{"location":"guide/debugging/#capture-current-stack","title":"Capture Current Stack","text":"<pre><code># Get current execution stack\nprobing $ENDPOINT backtrace\n\n# Query stack frames\nprobing $ENDPOINT query \"\nSELECT func, file, lineno, depth\nFROM python.backtrace\nORDER BY depth\nLIMIT 10\"\n</code></pre>"},{"location":"guide/debugging/#understanding-stack-frames","title":"Understanding Stack Frames","text":"<p>The <code>python.backtrace</code> table provides:</p> Column Description <code>func</code> Function name <code>file</code> Source file path <code>lineno</code> Line number <code>depth</code> Stack depth (0 = deepest) <code>frame_type</code> 'Python' or 'Native'"},{"location":"guide/debugging/#find-where-code-is-stuck","title":"Find Where Code Is Stuck","text":"<pre><code># Capture stack\nprobing $ENDPOINT backtrace\n\n# Check top of stack\nprobing $ENDPOINT query \"\nSELECT func, file, lineno\nFROM python.backtrace\nWHERE depth = 0\"\n</code></pre>"},{"location":"guide/debugging/#live-inspection","title":"Live Inspection","text":""},{"location":"guide/debugging/#inspect-variables","title":"Inspect Variables","text":"<pre><code># Check global variables\nprobing $ENDPOINT eval \"print(globals().keys())\"\n\n# Inspect specific object\nprobing $ENDPOINT eval \"print(type(model), model)\"\n\n# Check model parameters\nprobing $ENDPOINT eval \"\nfor name, param in model.named_parameters():\n    print(f'{name}: {param.shape}, grad={param.grad is not None}')\n\"\n</code></pre>"},{"location":"guide/debugging/#check-thread-state","title":"Check Thread State","text":"<pre><code>probing $ENDPOINT eval \"\nimport threading\nfor t in threading.enumerate():\n    print(f'{t.name}: alive={t.is_alive()}, daemon={t.daemon}')\n\"\n</code></pre>"},{"location":"guide/debugging/#monitor-training-progress","title":"Monitor Training Progress","text":"<pre><code>probing $ENDPOINT eval \"\n# Check current step\nprint(f'Current step: {trainer.current_step}')\nprint(f'Current loss: {trainer.last_loss}')\nprint(f'Learning rate: {optimizer.param_groups[0][\\\"lr\\\"]}')\n\"\n</code></pre>"},{"location":"guide/debugging/#debugging-scenarios","title":"Debugging Scenarios","text":""},{"location":"guide/debugging/#scenario-1-training-hangs","title":"Scenario 1: Training Hangs","text":"<p>Symptoms: Training progress stops, no errors.</p> <p>Diagnosis:</p> <pre><code># 1. Capture stack\nprobing $ENDPOINT backtrace\n\n# 2. Check what's blocking\nprobing $ENDPOINT query \"\nSELECT func, file, lineno\nFROM python.backtrace\nWHERE depth &lt; 5\"\n\n# 3. Check thread states\nprobing $ENDPOINT eval \"\nimport threading\nfor t in threading.enumerate():\n    print(f'{t.name}: {t.is_alive()}')\"\n\n# 4. Check for deadlocks\nprobing $ENDPOINT eval \"\nimport torch.distributed as dist\nif dist.is_initialized():\n    print(f'Rank: {dist.get_rank()}, World: {dist.get_world_size()}')\"\n</code></pre>"},{"location":"guide/debugging/#scenario-2-naninf-in-gradients","title":"Scenario 2: NaN/Inf in Gradients","text":"<p>Symptoms: Loss becomes NaN or Inf.</p> <p>Diagnosis:</p> <pre><code># Check for NaN in model parameters\nprobing $ENDPOINT eval \"\nimport torch\nfor name, param in model.named_parameters():\n    if param.grad is not None:\n        if torch.isnan(param.grad).any():\n            print(f'NaN gradient in {name}')\n        if torch.isinf(param.grad).any():\n            print(f'Inf gradient in {name}')\"\n\n# Check loss value\nprobing $ENDPOINT eval \"\nimport torch\nprint(f'Loss: {loss.item()}')\nprint(f'IsNaN: {torch.isnan(loss).item()}')\nprint(f'IsInf: {torch.isinf(loss).item()}')\"\n</code></pre>"},{"location":"guide/debugging/#scenario-3-slow-training","title":"Scenario 3: Slow Training","text":"<p>Symptoms: Training slower than expected.</p> <p>Diagnosis:</p> <pre><code>-- Find slowest operations\nSELECT module, stage, avg(duration) as avg_time\nFROM python.torch_trace\nWHERE step &gt; (SELECT max(step) - 5 FROM python.torch_trace)\nGROUP BY module, stage\nORDER BY avg_time DESC\nLIMIT 10;\n</code></pre> <pre><code># Check GPU utilization\nprobing $ENDPOINT eval \"\nimport torch\nif torch.cuda.is_available():\n    print(f'CUDA synchronize time test...')\n    import time\n    start = time.time()\n    torch.cuda.synchronize()\n    print(f'Sync took: {time.time() - start:.3f}s')\"\n</code></pre>"},{"location":"guide/debugging/#advanced-debugging","title":"Advanced Debugging","text":""},{"location":"guide/debugging/#conditional-breakpoints-conceptual","title":"Conditional Breakpoints (Conceptual)","text":"<pre><code># Monitor until condition is met\nprobing $ENDPOINT eval \"\nimport torch\nstep = trainer.current_step\nloss = trainer.last_loss\nif loss &gt; 10 or torch.isnan(torch.tensor(loss)):\n    print(f'ALERT: Step {step}, Loss {loss}')\n    # Trigger investigation\n\"\n</code></pre>"},{"location":"guide/debugging/#data-pipeline-debugging","title":"Data Pipeline Debugging","text":"<pre><code># Check data loader\nprobing $ENDPOINT eval \"\nbatch = next(iter(train_loader))\nprint(f'Batch shape: {batch[0].shape}')\nprint(f'Batch dtype: {batch[0].dtype}')\nprint(f'Has NaN: {torch.isnan(batch[0]).any()}')\"\n</code></pre>"},{"location":"guide/debugging/#distributed-debugging","title":"Distributed Debugging","text":"<pre><code># Check distributed state\nprobing $ENDPOINT eval \"\nimport torch.distributed as dist\nif dist.is_initialized():\n    print(f'Backend: {dist.get_backend()}')\n    print(f'Rank: {dist.get_rank()}')\n    print(f'World size: {dist.get_world_size()}')\n    print(f'Is NCCL available: {dist.is_nccl_available()}')\"\n</code></pre>"},{"location":"guide/debugging/#best-practices","title":"Best Practices","text":"<ol> <li>Start with backtrace - Understand where execution is before diving deeper</li> <li>Use query for trends - SQL is great for analyzing patterns over time</li> <li>Use eval for real-time - Get current state with Python code</li> <li>Combine approaches - backtrace \u2192 eval \u2192 query workflow</li> <li>Log important states - Use eval to print diagnostic information</li> </ol>"},{"location":"guide/debugging/#next-steps","title":"Next Steps","text":"<ul> <li>Memory Analysis - Debug memory issues</li> <li>Troubleshooting - Common problems and solutions</li> <li>Design Architecture - Understand internals</li> </ul>"},{"location":"guide/memory-analysis/","title":"Memory Analysis","text":"<p>Probing provides comprehensive tools for analyzing memory usage in AI applications.</p>"},{"location":"guide/memory-analysis/#overview","title":"Overview","text":"<p>Memory issues are common in AI workloads, especially during training. Probing helps you:</p> <ul> <li>Track GPU and CPU memory allocation</li> <li>Detect memory leaks</li> <li>Analyze memory usage patterns</li> <li>Optimize memory efficiency</li> </ul>"},{"location":"guide/memory-analysis/#quick-memory-check","title":"Quick Memory Check","text":"<pre><code># Get current memory state\nprobing $ENDPOINT eval \"\nimport torch\nimport psutil\n\nproc = psutil.Process()\nprint(f'CPU Memory: {proc.memory_info().rss / 1024**3:.2f} GB')\n\nif torch.cuda.is_available():\n    print(f'GPU Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB')\n    print(f'GPU Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB')\n\"\n</code></pre>"},{"location":"guide/memory-analysis/#memory-usage-trends","title":"Memory Usage Trends","text":""},{"location":"guide/memory-analysis/#track-memory-over-training-steps","title":"Track Memory Over Training Steps","text":"<pre><code>SELECT\n  step,\n  avg(allocated) as avg_memory_mb,\n  max(allocated) as peak_memory_mb,\n  min(allocated) as min_memory_mb\nFROM python.torch_trace\nWHERE step IS NOT NULL\nGROUP BY step\nORDER BY step;\n</code></pre>"},{"location":"guide/memory-analysis/#detect-memory-growth","title":"Detect Memory Growth","text":"<pre><code>SELECT\n  step,\n  max(allocated) - min(allocated) as memory_growth_mb\nFROM python.torch_trace\nWHERE step &gt; (SELECT max(step) - 10 FROM python.torch_trace)\nGROUP BY step\nHAVING max(allocated) - min(allocated) &gt; 50\nORDER BY step;\n</code></pre>"},{"location":"guide/memory-analysis/#memory-by-module","title":"Memory by Module","text":"<p>Identify which model components use the most memory:</p> <pre><code>SELECT\n  module,\n  stage,\n  avg(allocated) as avg_memory,\n  count(*) as execution_count\nFROM python.torch_trace\nWHERE module IS NOT NULL\nGROUP BY module, stage\nORDER BY avg_memory DESC\nLIMIT 10;\n</code></pre>"},{"location":"guide/memory-analysis/#memory-leak-detection","title":"Memory Leak Detection","text":""},{"location":"guide/memory-analysis/#pattern-monotonic-growth","title":"Pattern: Monotonic Growth","text":"<pre><code>WITH memory_trend AS (\n  SELECT\n    step,\n    max(allocated) as peak_memory,\n    LAG(max(allocated)) OVER (ORDER BY step) as prev_peak\n  FROM python.torch_trace\n  GROUP BY step\n)\nSELECT step, peak_memory, prev_peak,\n       peak_memory - prev_peak as growth\nFROM memory_trend\nWHERE peak_memory &gt; prev_peak\nORDER BY step;\n</code></pre>"},{"location":"guide/memory-analysis/#force-garbage-collection","title":"Force Garbage Collection","text":"<pre><code>probing $ENDPOINT eval \"\nimport gc\nimport torch\n\n# Force garbage collection\ngc.collect()\n\n# Clear CUDA cache\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    print('CUDA cache cleared')\n\nprint(f'Garbage collected: {gc.get_count()}')\n\"\n</code></pre>"},{"location":"guide/memory-analysis/#gpu-memory-analysis","title":"GPU Memory Analysis","text":""},{"location":"guide/memory-analysis/#current-gpu-state","title":"Current GPU State","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\n\nif torch.cuda.is_available():\n    for i in range(torch.cuda.device_count()):\n        props = torch.cuda.get_device_properties(i)\n        allocated = torch.cuda.memory_allocated(i) / 1024**3\n        reserved = torch.cuda.memory_reserved(i) / 1024**3\n        total = props.total_memory / 1024**3\n\n        print(f'GPU {i}: {props.name}')\n        print(f'  Total: {total:.2f} GB')\n        print(f'  Allocated: {allocated:.2f} GB')\n        print(f'  Reserved: {reserved:.2f} GB')\n        print(f'  Free: {total - reserved:.2f} GB')\n\"\n</code></pre>"},{"location":"guide/memory-analysis/#memory-by-operation-stage","title":"Memory by Operation Stage","text":"<pre><code>SELECT\n  stage,\n  avg(allocated) as avg_allocated,\n  avg(max_allocated) as avg_peak,\n  avg(cached) as avg_cached\nFROM python.torch_trace\nWHERE step = (SELECT max(step) FROM python.torch_trace)\nGROUP BY stage\nORDER BY avg_peak DESC;\n</code></pre>"},{"location":"guide/memory-analysis/#best-practices","title":"Best Practices","text":""},{"location":"guide/memory-analysis/#1-regular-memory-snapshots","title":"1. Regular Memory Snapshots","text":"<p>Take periodic snapshots during training:</p> <pre><code># Add to training loop\nprobing $ENDPOINT eval \"\nimport torch\nstep = ...  # current step\nif step % 100 == 0:\n    allocated = torch.cuda.memory_allocated() / 1024**3\n    print(f'Step {step}: {allocated:.2f} GB')\n\"\n</code></pre>"},{"location":"guide/memory-analysis/#2-profile-memory-intensive-operations","title":"2. Profile Memory-Intensive Operations","text":"<pre><code>-- Find operations with highest memory variance\nSELECT\n  module,\n  stage,\n  stddev(allocated) as memory_variance,\n  max(allocated) - min(allocated) as memory_range\nFROM python.torch_trace\nGROUP BY module, stage\nHAVING stddev(allocated) &gt; 10\nORDER BY memory_variance DESC;\n</code></pre>"},{"location":"guide/memory-analysis/#3-monitor-memory-fragmentation","title":"3. Monitor Memory Fragmentation","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\nif torch.cuda.is_available():\n    stats = torch.cuda.memory_stats()\n    print(f'Allocated blocks: {stats.get(\\\"allocated_bytes.all.current\\\", 0) / 1024**3:.2f} GB')\n    print(f'Reserved blocks: {stats.get(\\\"reserved_bytes.all.current\\\", 0) / 1024**3:.2f} GB')\n\"\n</code></pre>"},{"location":"guide/memory-analysis/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/memory-analysis/#out-of-memory-oom","title":"Out of Memory (OOM)","text":"<ol> <li>Check current memory state</li> <li>Identify memory-heavy modules</li> <li>Force garbage collection</li> <li>Reduce batch size or model size</li> </ol>"},{"location":"guide/memory-analysis/#memory-not-released","title":"Memory Not Released","text":"<ol> <li>Check for circular references</li> <li>Verify tensors are not held in lists/dicts</li> <li>Use <code>del</code> explicitly for large tensors</li> <li>Call <code>torch.cuda.empty_cache()</code></li> </ol>"},{"location":"guide/memory-analysis/#next-steps","title":"Next Steps","text":"<ul> <li>SQL Analytics - More query patterns</li> <li>Debugging - Stack analysis techniques</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"guide/sql-analytics/","title":"SQL Analytics Interface","text":"<p>Probing provides a powerful SQL interface for analyzing performance and monitoring data.</p>"},{"location":"guide/sql-analytics/#overview","title":"Overview","text":"<p>The SQL analytics interface transforms complex performance analysis into intuitive database queries. All monitoring data is accessible through standard SQL operations including <code>SELECT</code>, <code>WHERE</code>, <code>GROUP BY</code>, <code>ORDER BY</code>, and advanced analytical functions.</p>"},{"location":"guide/sql-analytics/#basic-query-structure","title":"Basic Query Structure","text":"<pre><code>probing $ENDPOINT query \"SELECT columns FROM table WHERE conditions\"\n</code></pre>"},{"location":"guide/sql-analytics/#core-tables","title":"Core Tables","text":""},{"location":"guide/sql-analytics/#configuration-and-metadata","title":"Configuration and Metadata","text":"<p><code>information_schema.df_settings</code> - System configuration and settings</p> <pre><code>SELECT * FROM information_schema.df_settings\nWHERE name LIKE 'probing.%';\n</code></pre>"},{"location":"guide/sql-analytics/#python-namespace-tables","title":"Python Namespace Tables","text":"<p><code>python.backtrace</code> - Stack trace information</p> <pre><code>SELECT * FROM python.backtrace LIMIT 10;\n</code></pre> <p>Common columns:</p> <ul> <li><code>ip</code> - Instruction pointer (for native frames)</li> <li><code>file</code> - Source file name</li> <li><code>func</code> - Function name</li> <li><code>lineno</code> - Line number</li> <li><code>depth</code> - Stack depth</li> <li><code>frame_type</code> - Frame type ('Python' or 'Native')</li> </ul>"},{"location":"guide/sql-analytics/#pytorch-integration","title":"PyTorch Integration","text":"<p>When monitoring PyTorch applications, additional tables become available:</p> <p><code>python.torch_trace</code> - PyTorch execution traces</p> <pre><code>SELECT step, module, stage, duration, allocated\nFROM python.torch_trace\nWHERE step &gt;= 5\nORDER BY step DESC, seq;\n</code></pre> <p>Common columns:</p> <ul> <li><code>step</code> - Training step number</li> <li><code>seq</code> - Sequence number within step</li> <li><code>module</code> - Module name</li> <li><code>stage</code> - Execution stage (forward, backward, step)</li> <li><code>allocated</code> - GPU memory allocated (MB)</li> <li><code>duration</code> - Execution duration (seconds)</li> </ul>"},{"location":"guide/sql-analytics/#advanced-analytics","title":"Advanced Analytics","text":""},{"location":"guide/sql-analytics/#time-series-analysis","title":"Time-Series Analysis","text":"<p>Memory growth over time:</p> <pre><code>SELECT\n  step,\n  stage,\n  avg(allocated) as avg_memory_mb,\n  max(allocated) as peak_memory_mb\nFROM python.torch_trace\nWHERE step &gt; (SELECT max(step) - 10 FROM python.torch_trace)\nGROUP BY step, stage\nORDER BY step, stage;\n</code></pre> <p>Rolling averages:</p> <pre><code>SELECT\n  step,\n  module,\n  duration,\n  AVG(duration) OVER (\n    PARTITION BY module\n    ORDER BY step, seq\n    ROWS BETWEEN 4 PRECEDING AND CURRENT ROW\n  ) as avg_duration_5_samples\nFROM python.torch_trace\nWHERE step &gt; (SELECT max(step) - 5 FROM python.torch_trace);\n</code></pre>"},{"location":"guide/sql-analytics/#performance-analysis","title":"Performance Analysis","text":"<p>Top slowest operations:</p> <pre><code>SELECT\n  module,\n  stage,\n  count(*) as execution_count,\n  avg(duration) as avg_duration,\n  max(duration) as max_duration\nFROM python.torch_trace\nWHERE step &gt; (SELECT max(step) - 10 FROM python.torch_trace)\n  AND duration &gt; 0\nGROUP BY module, stage\nORDER BY avg_duration DESC\nLIMIT 10;\n</code></pre>"},{"location":"guide/sql-analytics/#aggregation-functions","title":"Aggregation Functions","text":""},{"location":"guide/sql-analytics/#statistical-functions","title":"Statistical Functions","text":"<pre><code>SELECT\n  module,\n  stage,\n  count(*) as total_executions,\n  avg(duration) as mean_duration,\n  percentile_cont(0.5) WITHIN GROUP (ORDER BY duration) as median_duration,\n  percentile_cont(0.95) WITHIN GROUP (ORDER BY duration) as p95_duration,\n  min(duration) as min_duration,\n  max(duration) as max_duration\nFROM python.torch_trace\nWHERE duration &gt; 0\nGROUP BY module, stage;\n</code></pre>"},{"location":"guide/sql-analytics/#window-functions","title":"Window Functions","text":"<pre><code>SELECT\n  step,\n  allocated,\n  LAG(allocated) OVER (ORDER BY step, seq) as prev_memory,\n  LEAD(allocated) OVER (ORDER BY step, seq) as next_memory,\n  ROW_NUMBER() OVER (ORDER BY allocated DESC) as memory_rank\nFROM python.torch_trace\nWHERE step &gt; (SELECT max(step) - 5 FROM python.torch_trace);\n</code></pre>"},{"location":"guide/sql-analytics/#data-export","title":"Data Export","text":"<p>Results can be exported for further analysis:</p> <pre><code># Export to JSON\nprobing $ENDPOINT query \"SELECT * FROM python.torch_trace\" &gt; torch_traces.json\n\n# Time-series data for plotting\nprobing $ENDPOINT query \"\n  SELECT step, stage, avg(duration), avg(allocated)\n  FROM python.torch_trace\n  GROUP BY step, stage\n\" &gt; training_metrics.json\n</code></pre>"},{"location":"guide/sql-analytics/#best-practices","title":"Best Practices","text":"<ol> <li>Use step-based filtering - Always include step constraints for better performance</li> <li>Limit result sets - Use <code>LIMIT</code> clauses for large datasets</li> <li>Aggregate appropriately - Use <code>GROUP BY</code> for summary statistics</li> <li>Test queries incrementally - Start simple and add complexity gradually</li> </ol>"},{"location":"guide/troubleshooting/","title":"Troubleshooting","text":"<p>Common issues and their solutions when using Probing.</p>"},{"location":"guide/troubleshooting/#connection-issues","title":"Connection Issues","text":""},{"location":"guide/troubleshooting/#cannot-connect-to-process","title":"Cannot Connect to Process","text":"<p>Symptom: <code>probing $ENDPOINT inject</code> fails or times out.</p> <p>Solutions:</p> <ol> <li> <p>Verify process exists:    <pre><code>ps aux | grep $ENDPOINT\n</code></pre></p> </li> <li> <p>Check Linux requirement:    Injection only works on Linux. On other platforms, start your process with:    <pre><code>PROBING=1 python your_script.py\n</code></pre></p> </li> <li> <p>Check permissions:    <pre><code># May need sudo for injection\nsudo probing $ENDPOINT inject\n</code></pre></p> </li> </ol>"},{"location":"guide/troubleshooting/#connection-refused-remote","title":"Connection Refused (Remote)","text":"<p>Symptom: Cannot connect to remote process.</p> <p>Solutions:</p> <ol> <li> <p>Verify server is running:    <pre><code># On remote machine\nnetstat -tlnp | grep $PORT\n</code></pre></p> </li> <li> <p>Check firewall:    <pre><code># Allow port\nsudo ufw allow $PORT\n</code></pre></p> </li> <li> <p>Verify endpoint format:    <pre><code>export ENDPOINT=hostname:port  # Not just hostname\n</code></pre></p> </li> </ol>"},{"location":"guide/troubleshooting/#query-issues","title":"Query Issues","text":""},{"location":"guide/troubleshooting/#table-not-found","title":"Table Not Found","text":"<p>Symptom: <code>Table 'python.torch_trace' not found</code></p> <p>Solutions:</p> <ol> <li> <p>Check if PyTorch profiling is enabled:    <pre><code>probing $ENDPOINT eval \"\nimport probing\nprint(probing.get_config())\"\n</code></pre></p> </li> <li> <p>Enable PyTorch tracing:    <pre><code>PROBING_TORCH_PROFILING=on python your_script.py\n</code></pre></p> </li> <li> <p>Wait for data collection:    Tables are populated as operations occur. Run some training steps first.</p> </li> </ol>"},{"location":"guide/troubleshooting/#empty-results","title":"Empty Results","text":"<p>Symptom: Query returns no rows.</p> <p>Solutions:</p> <ol> <li> <p>Check table contents:    <pre><code>SELECT COUNT(*) FROM python.torch_trace;\n</code></pre></p> </li> <li> <p>Verify filter conditions:    <pre><code>-- Remove filters to debug\nSELECT * FROM python.torch_trace LIMIT 5;\n</code></pre></p> </li> <li> <p>Check step range:    <pre><code>SELECT MIN(step), MAX(step) FROM python.torch_trace;\n</code></pre></p> </li> </ol>"},{"location":"guide/troubleshooting/#eval-issues","title":"Eval Issues","text":""},{"location":"guide/troubleshooting/#code-execution-fails","title":"Code Execution Fails","text":"<p>Symptom: <code>probing eval</code> returns error or unexpected result.</p> <p>Solutions:</p> <ol> <li> <p>Check syntax:    <pre><code># Use proper quoting\nprobing $ENDPOINT eval \"print('hello')\"\n</code></pre></p> </li> <li> <p>Handle imports:    <pre><code># Import modules first\nprobing $ENDPOINT eval \"import torch; print(torch.__version__)\"\n</code></pre></p> </li> <li> <p>Check variable scope:    <pre><code># Use globals() to see available variables\nprobing $ENDPOINT eval \"print(list(globals().keys())[:10])\"\n</code></pre></p> </li> </ol>"},{"location":"guide/troubleshooting/#import-errors","title":"Import Errors","text":"<p>Symptom: <code>ModuleNotFoundError</code> in eval.</p> <p>Solutions:</p> <ol> <li> <p>Check if module is loaded:    <pre><code>probing $ENDPOINT eval \"import sys; print('torch' in sys.modules)\"\n</code></pre></p> </li> <li> <p>Use try-except:    <pre><code>probing $ENDPOINT eval \"\ntry:\n    import torch\n    print(torch.__version__)\nexcept ImportError:\n    print('torch not available')\"\n</code></pre></p> </li> </ol>"},{"location":"guide/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"guide/troubleshooting/#high-overhead","title":"High Overhead","text":"<p>Symptom: Application runs slower with Probing.</p> <p>Solutions:</p> <ol> <li> <p>Reduce sampling rate:    <pre><code>probing $ENDPOINT config probing.sample_rate=0.01\n</code></pre></p> </li> <li> <p>Disable unused features:    <pre><code>PROBING_TORCH_PROFILING=off python your_script.py\n</code></pre></p> </li> <li> <p>Use targeted profiling:    Only enable profiling for specific modules or operations.</p> </li> </ol>"},{"location":"guide/troubleshooting/#query-timeout","title":"Query Timeout","text":"<p>Symptom: SQL queries take too long.</p> <p>Solutions:</p> <ol> <li> <p>Add LIMIT clause:    <pre><code>SELECT * FROM python.torch_trace LIMIT 100;\n</code></pre></p> </li> <li> <p>Use step filtering:    <pre><code>WHERE step &gt; (SELECT MAX(step) - 10 FROM python.torch_trace)\n</code></pre></p> </li> <li> <p>Aggregate data:    <pre><code>SELECT step, AVG(duration) FROM python.torch_trace GROUP BY step;\n</code></pre></p> </li> </ol>"},{"location":"guide/troubleshooting/#data-issues","title":"Data Issues","text":""},{"location":"guide/troubleshooting/#missing-data","title":"Missing Data","text":"<p>Symptom: Expected data not appearing in tables.</p> <p>Solutions:</p> <ol> <li> <p>Verify profiling is active:    <pre><code>probing $ENDPOINT eval \"\nimport probing\nprint(probing.is_profiling_active())\"\n</code></pre></p> </li> <li> <p>Check data retention:    <pre><code>probing $ENDPOINT config | grep retention\n</code></pre></p> </li> <li> <p>Force data flush:    <pre><code>probing $ENDPOINT eval \"\nimport probing\nprobing.flush()\"\n</code></pre></p> </li> </ol>"},{"location":"guide/troubleshooting/#incorrect-values","title":"Incorrect Values","text":"<p>Symptom: Data values seem wrong.</p> <p>Solutions:</p> <ol> <li>Verify units:</li> <li>Memory is typically in MB</li> <li> <p>Duration is in seconds</p> </li> <li> <p>Check for aggregation:    <pre><code>-- Sum vs individual values\nSELECT SUM(allocated) vs SELECT allocated\n</code></pre></p> </li> <li> <p>Validate manually:    <pre><code>probing $ENDPOINT eval \"\nimport torch\nprint(torch.cuda.memory_allocated() / 1024**2)\"  # MB\n</code></pre></p> </li> </ol>"},{"location":"guide/troubleshooting/#platform-specific-issues","title":"Platform-Specific Issues","text":""},{"location":"guide/troubleshooting/#linux","title":"Linux","text":"<ul> <li>ptrace errors: May need <code>CAP_SYS_PTRACE</code> capability</li> <li>SELinux: May need to adjust policies</li> </ul>"},{"location":"guide/troubleshooting/#macos","title":"macOS","text":"<ul> <li>Injection not supported: Use <code>PROBING=1</code> at startup</li> <li>SIP restrictions: May affect some features</li> </ul>"},{"location":"guide/troubleshooting/#windows","title":"Windows","text":"<ul> <li>Limited support: Only query/eval with pre-enabled processes</li> </ul>"},{"location":"guide/troubleshooting/#getting-help","title":"Getting Help","text":"<p>If you're still stuck:</p> <ol> <li> <p>Check logs:    <pre><code>probing $ENDPOINT eval \"\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\"\n</code></pre></p> </li> <li> <p>Report issue:    GitHub Issues</p> </li> <li> <p>Include diagnostics:    <pre><code>probing --version\npython --version\nuname -a\n</code></pre></p> </li> </ol>"},{"location":"zh/","title":"Probing - \u200b\u5206\u5e03\u5f0f\u200b AI \u200b\u52a8\u6001\u200b\u6027\u80fd\u200b\u5206\u6790\u5668","text":""},{"location":"zh/#probing","title":"Probing","text":"<p>Probing \u200b\u662f\u200b\u4e00\u4e2a\u200b\u9762\u5411\u200b\u5206\u5e03\u5f0f\u200b AI \u200b\u5e94\u7528\u200b\u7684\u200b\u52a8\u6001\u200b\u6027\u80fd\u200b\u5206\u6790\u5668\u200b\u3002</p>"},{"location":"zh/#_1","title":"\u6838\u5fc3\u200b\u7279\u6027","text":"<ul> <li>\u200b\u96f6\u200b\u4fb5\u5165\u200b - \u200b\u65e0\u9700\u200b\u4fee\u6539\u200b\u4ee3\u7801\u200b\u5373\u53ef\u200b\u9644\u52a0\u200b\u5230\u200b\u8fd0\u884c\u200b\u4e2d\u200b\u7684\u200b\u8fdb\u7a0b\u200b</li> <li>SQL \u200b\u5206\u6790\u200b - \u200b\u4f7f\u7528\u200b\u6807\u51c6\u200b SQL \u200b\u67e5\u8be2\u200b\u6027\u80fd\u200b\u6570\u636e\u200b</li> <li>\u200b\u5b9e\u65f6\u200b\u6267\u884c\u200b - \u200b\u5728\u200b\u76ee\u6807\u200b\u8fdb\u7a0b\u200b\u4e2d\u200b\u8fd0\u884c\u200b Python \u200b\u4ee3\u7801\u200b</li> <li>\u200b\u5806\u6808\u200b\u5206\u6790\u200b - \u200b\u6355\u83b7\u200b\u5e26\u6709\u200b\u53d8\u91cf\u503c\u200b\u7684\u200b\u8c03\u7528\u200b\u6808\u200b</li> <li>\u200b\u5206\u5e03\u5f0f\u200b\u652f\u6301\u200b - \u200b\u76d1\u63a7\u200b\u8de8\u591a\u200b\u8282\u70b9\u200b\u7684\u200b\u8fdb\u7a0b\u200b</li> </ul>"},{"location":"zh/#_2","title":"\u5feb\u901f\u200b\u5f00\u59cb","text":"<pre><code># \u200b\u5b89\u88c5\u200b\npip install probing\n\n# \u200b\u6ce8\u5165\u200b\u5230\u200b\u8fd0\u884c\u200b\u4e2d\u200b\u7684\u200b\u8fdb\u7a0b\u200b\nprobing -t &lt;pid&gt; inject\n\n# \u200b\u67e5\u8be2\u200b\u6027\u80fd\u200b\u6570\u636e\u200b\nprobing -t &lt;pid&gt; query \"SELECT * FROM python.torch_trace LIMIT 10\"\n</code></pre>"},{"location":"zh/#_3","title":"\u4f7f\u7528\u200b\u573a\u666f","text":"<ul> <li>\u200b\u8bad\u7ec3\u200b\u8c03\u8bd5\u200b - \u200b\u8c03\u8bd5\u200b\u8bad\u7ec3\u200b\u4e0d\u200b\u7a33\u5b9a\u200b\u548c\u200b\u5361\u4f4f\u200b\u95ee\u9898\u200b</li> <li>\u200b\u5185\u5b58\u200b\u5206\u6790\u200b - \u200b\u8ffd\u8e2a\u200b GPU/CPU \u200b\u5185\u5b58\u200b\u4f7f\u7528\u200b</li> <li>\u200b\u6027\u80fd\u200b\u5206\u6790\u200b - \u200b\u8bc6\u522b\u200b\u6a21\u578b\u200b\u6267\u884c\u200b\u4e2d\u200b\u7684\u200b\u74f6\u9888\u200b</li> <li>\u200b\u751f\u4ea7\u200b\u76d1\u63a7\u200b - \u200b\u65e0\u9700\u200b\u91cd\u542f\u200b\u5373\u53ef\u200b\u76d1\u63a7\u200b AI \u200b\u670d\u52a1\u200b</li> </ul>"},{"location":"zh/#_4","title":"\u793e\u533a","text":"<ul> <li>GitHub \u200b\u4ed3\u5e93\u200b</li> <li>\u200b\u95ee\u9898\u200b\u8ffd\u8e2a\u200b</li> <li>PyPI \u200b\u5305\u200b</li> </ul>"},{"location":"zh/api-reference/","title":"API \u200b\u53c2\u8003","text":"<p>Probing CLI \u200b\u547d\u4ee4\u200b\u548c\u200b Python API \u200b\u7684\u200b\u5b8c\u6574\u200b\u53c2\u8003\u200b\u3002</p>"},{"location":"zh/api-reference/#cli","title":"CLI \u200b\u547d\u4ee4","text":""},{"location":"zh/api-reference/#probing-inject","title":"probing inject","text":"<p>\u200b\u5c06\u200b\u63a2\u9488\u200b\u6ce8\u5165\u200b\u5230\u200b\u8fd0\u884c\u200b\u4e2d\u200b\u7684\u200b\u8fdb\u7a0b\u200b\u3002</p> <pre><code>probing -t &lt;pid&gt; inject\n</code></pre> <p>\u200b\u9009\u9879\u200b\uff1a</p> <ul> <li><code>-t, --target &lt;pid&gt;</code> - \u200b\u76ee\u6807\u200b\u8fdb\u7a0b\u200b ID\uff08\u200b\u5fc5\u9700\u200b\uff09</li> </ul> <p>\u200b\u5e73\u53f0\u200b\uff1a \u200b\u4ec5\u200b Linux</p>"},{"location":"zh/api-reference/#probing-query","title":"probing query","text":"<p>\u200b\u5bf9\u200b\u6536\u96c6\u200b\u7684\u200b\u6570\u636e\u200b\u6267\u884c\u200b SQL \u200b\u67e5\u8be2\u200b\u3002</p> <pre><code>probing -t &lt;endpoint&gt; query \"&lt;sql&gt;\"\n</code></pre> <p>\u200b\u793a\u4f8b\u200b\uff1a</p> <pre><code># \u200b\u67e5\u8be2\u200b torch \u200b\u8ffd\u8e2a\u200b\nprobing -t 12345 query \"SELECT * FROM python.torch_trace LIMIT 10\"\n\n# \u200b\u805a\u5408\u200b\u67e5\u8be2\u200b\nprobing -t host:8080 query \"SELECT module, AVG(duration) FROM python.torch_trace GROUP BY module\"\n</code></pre>"},{"location":"zh/api-reference/#probing-eval","title":"probing eval","text":"<p>\u200b\u5728\u200b\u76ee\u6807\u200b\u8fdb\u7a0b\u200b\u4e2d\u200b\u6267\u884c\u200b Python \u200b\u4ee3\u7801\u200b\u3002</p> <pre><code>probing -t &lt;endpoint&gt; eval \"&lt;python_code&gt;\"\n</code></pre> <p>\u200b\u793a\u4f8b\u200b\uff1a</p> <pre><code># \u200b\u7b80\u5355\u200b\u6267\u884c\u200b\nprobing -t 12345 eval \"print('hello')\"\n\n# \u200b\u591a\u200b\u8bed\u53e5\u200b\nprobing -t 12345 eval \"import torch; print(torch.cuda.is_available())\"\n</code></pre>"},{"location":"zh/api-reference/#probing-backtrace","title":"probing backtrace","text":"<p>\u200b\u6355\u83b7\u200b\u5f53\u524d\u200b\u5806\u6808\u200b\u8ddf\u8e2a\u200b\u3002</p> <pre><code>probing -t &lt;endpoint&gt; backtrace\n</code></pre> <p>\u200b\u8f93\u51fa\u200b\uff1a \u200b\u5305\u542b\u200b\u51fd\u6570\u200b\u540d\u200b\u3001\u200b\u6587\u4ef6\u200b\u548c\u200b\u884c\u53f7\u200b\u7684\u200b\u5806\u6808\u200b\u5e27\u200b\u3002</p>"},{"location":"zh/api-reference/#probing-repl","title":"probing repl","text":"<p>\u200b\u542f\u52a8\u200b\u4ea4\u4e92\u5f0f\u200b Python REPL\u3002</p> <pre><code>probing -t &lt;endpoint&gt; repl\n</code></pre> <p>\u200b\u529f\u80fd\u200b\uff1a</p> <ul> <li>Tab \u200b\u8865\u5168\u200b</li> <li>\u200b\u591a\u884c\u200b\u8f93\u5165\u200b</li> <li>\u200b\u547d\u4ee4\u200b\u5386\u53f2\u200b</li> </ul>"},{"location":"zh/api-reference/#probing-list","title":"probing list","text":"<p>\u200b\u5217\u51fa\u200b\u542f\u7528\u200b\u4e86\u200b probing \u200b\u7684\u200b\u8fdb\u7a0b\u200b\u3002</p> <pre><code>probing list\n</code></pre> <p>\u200b\u8f93\u51fa\u200b\uff1a \u200b\u8fdb\u7a0b\u200b ID \u200b\u53ca\u5176\u200b probing \u200b\u72b6\u6001\u200b\u3002</p>"},{"location":"zh/api-reference/#probing-config","title":"probing config","text":"<p>\u200b\u67e5\u770b\u200b\u6216\u200b\u4fee\u6539\u200b\u914d\u7f6e\u200b\u3002</p> <pre><code># \u200b\u67e5\u770b\u200b\u6240\u6709\u200b\u914d\u7f6e\u200b\nprobing -t &lt;endpoint&gt; config\n\n# \u200b\u67e5\u770b\u200b\u7279\u5b9a\u200b\u952e\u200b\nprobing -t &lt;endpoint&gt; config probing.sample_rate\n\n# \u200b\u8bbe\u7f6e\u200b\u503c\u200b\nprobing -t &lt;endpoint&gt; config probing.sample_rate=0.1\n</code></pre>"},{"location":"zh/api-reference/#probingconnect","title":"probing.connect","text":"<p>\u200b\u8fde\u63a5\u200b\u5230\u200b probing \u200b\u7aef\u70b9\u200b\u3002</p> <pre><code>from probing import connect\n\n# \u200b\u901a\u8fc7\u200b PID \u200b\u8fde\u63a5\u200b\nprobe = connect(pid=12345)\n\n# \u200b\u901a\u8fc7\u200b\u5730\u5740\u200b\u8fde\u63a5\u200b\nprobe = connect(address=\"host:8080\")\n</code></pre>"},{"location":"zh/api-reference/#probingtable","title":"@probing.table","text":"<p>\u200b\u6ce8\u518c\u200b\u81ea\u5b9a\u4e49\u200b\u6570\u636e\u8868\u200b\u3002</p> <pre><code>from probing import table\n\n@table(\"my_data\")\ndef get_my_data():\n    return [{\"key\": \"value\"}]\n</code></pre>"},{"location":"zh/api-reference/#sql","title":"SQL \u200b\u8868","text":""},{"location":"zh/api-reference/#pythonbacktrace","title":"python.backtrace","text":"<p>\u200b\u5806\u6808\u200b\u8ddf\u8e2a\u200b\u4fe1\u606f\u200b\u3002</p> \u200b\u5217\u200b \u200b\u7c7b\u578b\u200b \u200b\u63cf\u8ff0\u200b func string \u200b\u51fd\u6570\u200b\u540d\u200b file string \u200b\u6e90\u6587\u4ef6\u200b lineno int \u200b\u884c\u53f7\u200b depth int \u200b\u5806\u6808\u200b\u6df1\u5ea6\u200b frame_type string Python/Native"},{"location":"zh/api-reference/#pythontorch_trace","title":"python.torch_trace","text":"<p>PyTorch \u200b\u6267\u884c\u200b\u8ddf\u8e2a\u200b\u3002</p> \u200b\u5217\u200b \u200b\u7c7b\u578b\u200b \u200b\u63cf\u8ff0\u200b step int \u200b\u8bad\u7ec3\u200b\u6b65\u9aa4\u200b seq int \u200b\u5e8f\u5217\u53f7\u200b module string \u200b\u6a21\u5757\u200b\u540d\u200b stage string forward/backward/step allocated float GPU \u200b\u5185\u5b58\u200b (MB) duration float \u200b\u6267\u884c\u200b\u65f6\u95f4\u200b (\u200b\u79d2\u200b)"},{"location":"zh/api-reference/#_1","title":"\u914d\u7f6e\u200b\u9009\u9879","text":"\u952e\u200b \u200b\u9ed8\u8ba4\u503c\u200b \u200b\u63cf\u8ff0\u200b <code>probing.sample_rate</code> 1.0 \u200b\u91c7\u6837\u7387\u200b (0.0-1.0) <code>probing.buffer_size</code> 10000 \u200b\u73af\u5f62\u200b\u7f13\u51b2\u533a\u200b\u5927\u5c0f\u200b <code>probing.server.port</code> 0 TCP \u200b\u7aef\u53e3\u200b (0=\u200b\u4ec5\u200b Unix socket) <code>probing.torch.enabled</code> true \u200b\u542f\u7528\u200b PyTorch \u200b\u8ffd\u8e2a"},{"location":"zh/api-reference/#_2","title":"\u73af\u5883\u53d8\u91cf","text":"\u53d8\u91cf\u200b \u200b\u63cf\u8ff0\u200b <code>PROBING</code> \u200b\u542f\u7528\u200b probing (1=\u200b\u5f00\u542f\u200b) <code>PROBING_PORT</code> TCP \u200b\u670d\u52a1\u5668\u7aef\u200b\u53e3\u200b <code>PROBING_TORCH_PROFILING</code> PyTorch \u200b\u5206\u6790\u200b (on/off) <code>PROBING_SAMPLE_RATE</code> \u200b\u9ed8\u8ba4\u200b\u91c7\u6837\u7387\u200b <code>PROBING_AUTH_TOKEN</code> \u200b\u8ba4\u8bc1\u200b\u4ee4\u724c"},{"location":"zh/contributing/","title":"\u8d21\u732e\u200b\u6307\u5357","text":"<p>\u200b\u611f\u8c22\u60a8\u200b\u5bf9\u200b\u8d21\u732e\u200b Probing \u200b\u7684\u200b\u5174\u8da3\u200b\uff01\u200b\u672c\u200b\u6307\u5357\u200b\u5c06\u200b\u5e2e\u52a9\u200b\u60a8\u200b\u5f00\u59cb\u200b\u3002</p>"},{"location":"zh/contributing/#_2","title":"\u5165\u95e8","text":""},{"location":"zh/contributing/#_3","title":"\u524d\u63d0\u6761\u4ef6","text":"<ul> <li>Python 3.9+</li> <li>Rust\uff08\u200b\u6700\u65b0\u200b\u7a33\u5b9a\u7248\u200b\uff09</li> <li>maturin\uff08\u200b\u7528\u4e8e\u200b\u6784\u5efa\u200b Python \u200b\u6269\u5c55\u200b\uff09</li> </ul>"},{"location":"zh/contributing/#_4","title":"\u5f00\u53d1\u200b\u73af\u5883\u200b\u8bbe\u7f6e","text":"<ol> <li>\u200b\u514b\u9686\u200b\u4ed3\u5e93\u200b\uff1a</li> </ol> <pre><code>git clone https://github.com/DeepLink-org/probing.git\ncd probing\n</code></pre> <ol> <li>\u200b\u521b\u5efa\u200b\u865a\u62df\u73af\u5883\u200b\uff1a</li> </ol> <pre><code>python -m venv .venv\nsource .venv/bin/activate  # Linux/macOS\n# \u200b\u6216\u200b\n.venv\\Scripts\\activate  # Windows\n</code></pre> <ol> <li>\u200b\u5b89\u88c5\u200b\u5f00\u53d1\u200b\u4f9d\u8d56\u200b\uff1a</li> </ol> <pre><code>pip install -e \".[dev]\"\n</code></pre> <ol> <li>\u200b\u6784\u5efa\u200b Rust \u200b\u6269\u5c55\u200b\uff1a</li> </ol> <pre><code>maturin develop\n</code></pre>"},{"location":"zh/contributing/#_5","title":"\u5f00\u53d1\u200b\u6d41\u7a0b","text":""},{"location":"zh/contributing/#_6","title":"\u8fd0\u884c\u200b\u6d4b\u8bd5","text":"<pre><code># \u200b\u8fd0\u884c\u200b\u6240\u6709\u200b\u6d4b\u8bd5\u200b\nmake test\n\n# \u200b\u8fd0\u884c\u200b\u7279\u5b9a\u200b\u6d4b\u8bd5\u200b\npytest tests/test_specific.py -v\n\n# \u200b\u8fd0\u884c\u200b\u5e26\u200b\u8986\u76d6\u7387\u200b\u7684\u200b\u6d4b\u8bd5\u200b\nmake coverage\n</code></pre>"},{"location":"zh/contributing/#_7","title":"\u4ee3\u7801\u200b\u98ce\u683c","text":"<p>\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u4ee5\u4e0b\u200b\u5de5\u5177\u200b\u4fdd\u8bc1\u200b\u4ee3\u7801\u200b\u8d28\u91cf\u200b\uff1a</p> <p>Python\uff1a</p> <ul> <li><code>ruff</code> \u200b\u7528\u4e8e\u200b\u4ee3\u7801\u200b\u68c0\u67e5\u548c\u200b\u683c\u5f0f\u5316\u200b</li> <li><code>mypy</code> \u200b\u7528\u4e8e\u200b\u7c7b\u578b\u200b\u68c0\u67e5\u200b</li> </ul> <pre><code># \u200b\u683c\u5f0f\u5316\u200b\u4ee3\u7801\u200b\nruff format .\n\n# \u200b\u68c0\u67e5\u200b\u4ee3\u7801\u200b\u89c4\u8303\u200b\nruff check .\n\n# \u200b\u7c7b\u578b\u200b\u68c0\u67e5\u200b\nmypy python/probing\n</code></pre> <p>Rust\uff1a</p> <ul> <li><code>rustfmt</code> \u200b\u7528\u4e8e\u200b\u683c\u5f0f\u5316\u200b</li> <li><code>clippy</code> \u200b\u7528\u4e8e\u200b\u4ee3\u7801\u200b\u68c0\u67e5\u200b</li> </ul> <pre><code># \u200b\u683c\u5f0f\u5316\u200b\ncargo fmt\n\n# \u200b\u68c0\u67e5\u200b\ncargo clippy --all --tests --benches -- -D warnings\n</code></pre>"},{"location":"zh/contributing/#_8","title":"\u6784\u5efa\u200b\u6587\u6863","text":"<pre><code>cd docs\nmake install  # \u200b\u5b89\u88c5\u200b\u4f9d\u8d56\u200b\nmake serve    # \u200b\u5728\u200b http://127.0.0.1:8000 \u200b\u9884\u89c8\u200b\n</code></pre>"},{"location":"zh/contributing/#_9","title":"\u63d0\u4ea4\u200b\u53d8\u66f4","text":""},{"location":"zh/contributing/#pull-request","title":"Pull Request \u200b\u6d41\u7a0b","text":"<ol> <li>Fork \u200b\u4ed3\u5e93\u200b</li> <li>\u200b\u521b\u5efa\u200b\u529f\u80fd\u200b\u5206\u652f\u200b\uff1a</li> </ol> <pre><code>git checkout -b feature/your-feature-name\n</code></pre> <ol> <li>\u200b\u8fdb\u884c\u200b\u4fee\u6539\u200b</li> <li>\u200b\u8fd0\u884c\u200b\u6d4b\u8bd5\u200b\u548c\u200b\u68c0\u67e5\u200b\uff1a</li> </ol> <pre><code>make test\nmake lint\n</code></pre> <ol> <li>\u200b\u4f7f\u7528\u200b\u63cf\u8ff0\u6027\u200b\u6d88\u606f\u200b\u63d0\u4ea4\u200b\uff1a</li> </ol> <pre><code>git commit -m \"feat: \u200b\u6dfb\u52a0\u200b\u65b0\u200b\u529f\u80fd\u200b\u63cf\u8ff0\u200b\"\n</code></pre> <ol> <li>\u200b\u63a8\u9001\u200b\u5e76\u200b\u521b\u5efa\u200b Pull Request</li> </ol>"},{"location":"zh/contributing/#_10","title":"\u63d0\u4ea4\u200b\u6d88\u606f\u200b\u683c\u5f0f","text":"<p>\u200b\u6211\u4eec\u200b\u9075\u5faa\u200b Conventional Commits\uff1a</p> <ul> <li><code>feat:</code> - \u200b\u65b0\u200b\u529f\u80fd\u200b</li> <li><code>fix:</code> - Bug \u200b\u4fee\u590d\u200b</li> <li><code>docs:</code> - \u200b\u6587\u6863\u200b\u53d8\u66f4\u200b</li> <li><code>style:</code> - \u200b\u4ee3\u7801\u200b\u98ce\u683c\u200b\u53d8\u66f4\u200b\uff08\u200b\u683c\u5f0f\u5316\u200b\uff09</li> <li><code>refactor:</code> - \u200b\u4ee3\u7801\u200b\u91cd\u6784\u200b</li> <li><code>test:</code> - \u200b\u6d4b\u8bd5\u200b\u53d8\u66f4\u200b</li> <li><code>chore:</code> - \u200b\u6784\u5efa\u200b/\u200b\u5de5\u5177\u200b\u53d8\u66f4\u200b</li> </ul>"},{"location":"zh/contributing/#_11","title":"\u4ee3\u7801\u200b\u5ba1\u67e5","text":"<p>\u200b\u6240\u6709\u200b\u63d0\u4ea4\u200b\u90fd\u200b\u9700\u8981\u200b\u4ee3\u7801\u200b\u5ba1\u67e5\u200b\u3002\u200b\u8bf7\u200b\uff1a</p> <ul> <li>\u200b\u4fdd\u6301\u200b PR \u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u5355\u4e00\u200b\u53d8\u66f4\u200b</li> <li>\u200b\u4e3a\u200b\u65b0\u200b\u529f\u80fd\u200b\u6dfb\u52a0\u200b\u6d4b\u8bd5\u200b</li> <li>\u200b\u6839\u636e\u200b\u9700\u8981\u200b\u66f4\u65b0\u200b\u6587\u6863\u200b</li> <li>\u200b\u53ca\u65f6\u200b\u54cd\u5e94\u200b\u5ba1\u67e5\u200b\u53cd\u9988\u200b</li> </ul>"},{"location":"zh/contributing/#_12","title":"\u9879\u76ee\u200b\u7ed3\u6784","text":"<pre><code>probing/\n\u251c\u2500\u2500 python/             # Python \u200b\u6e90\u4ee3\u7801\u200b\n\u2502   \u2514\u2500\u2500 probing/        # \u200b\u4e3b\u200b Python \u200b\u5305\u200b\n\u251c\u2500\u2500 probing/            # Rust crates\n\u2502   \u251c\u2500\u2500 core/           # \u200b\u6838\u5fc3\u200b\u529f\u80fd\u200b\n\u2502   \u251c\u2500\u2500 server/         # HTTP \u200b\u670d\u52a1\u5668\u200b\n\u2502   \u251c\u2500\u2500 extensions/     # Python/PyTorch \u200b\u6269\u5c55\u200b\n\u2502   \u2514\u2500\u2500 cli/            # \u200b\u547d\u4ee4\u884c\u200b\u754c\u9762\u200b\n\u251c\u2500\u2500 tests/              # Python \u200b\u6d4b\u8bd5\u200b\n\u251c\u2500\u2500 docs/               # \u200b\u6587\u6863\u200b\n\u2514\u2500\u2500 examples/           # \u200b\u4f7f\u7528\u200b\u793a\u4f8b\u200b\n</code></pre>"},{"location":"zh/contributing/#_13","title":"\u8d21\u732e\u200b\u9886\u57df","text":""},{"location":"zh/contributing/#issues","title":"\u9002\u5408\u200b\u65b0\u624b\u200b\u7684\u200b Issues","text":"<p>\u200b\u5728\u200b GitHub \u200b\u4e0a\u200b\u67e5\u627e\u200b\u6807\u8bb0\u200b\u4e3a\u200b <code>good-first-issue</code> \u200b\u7684\u200b\u95ee\u9898\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u9002\u5408\u200b\u65b0\u200b\u8d21\u732e\u8005\u200b\u3002</p>"},{"location":"zh/contributing/#_14","title":"\u6587\u6863","text":"<ul> <li>\u200b\u6539\u8fdb\u200b\u73b0\u6709\u200b\u6587\u6863\u200b</li> <li>\u200b\u6dfb\u52a0\u200b\u66f4\u200b\u591a\u200b\u793a\u4f8b\u200b</li> <li>\u200b\u7ffb\u8bd1\u200b\u6587\u6863\u200b</li> </ul>"},{"location":"zh/contributing/#_15","title":"\u6d4b\u8bd5","text":"<ul> <li>\u200b\u589e\u52a0\u200b\u6d4b\u8bd5\u200b\u8986\u76d6\u7387\u200b</li> <li>\u200b\u7f16\u5199\u200b\u96c6\u6210\u200b\u6d4b\u8bd5\u200b</li> <li>\u200b\u6027\u80fd\u200b\u57fa\u51c6\u200b\u6d4b\u8bd5\u200b</li> </ul>"},{"location":"zh/contributing/#_16","title":"\u529f\u80fd","text":"<ul> <li>\u200b\u67e5\u770b\u200b\u8def\u7ebf\u56fe\u200b\u548c\u200b\u5f00\u653e\u200b\u7684\u200b issues</li> <li>\u200b\u5927\u578b\u200b\u53d8\u66f4\u200b\u524d\u200b\u8bf7\u200b\u5148\u200b\u8ba8\u8bba\u200b</li> </ul>"},{"location":"zh/contributing/#_17","title":"\u83b7\u53d6\u200b\u5e2e\u52a9","text":"<ul> <li>GitHub Issues\uff1a\u200b\u7528\u4e8e\u200b Bug \u200b\u548c\u200b\u529f\u80fd\u200b\u8bf7\u6c42\u200b</li> <li>Discussions\uff1a\u200b\u7528\u4e8e\u200b\u95ee\u9898\u200b\u548c\u200b\u60f3\u6cd5\u200b\u8ba8\u8bba\u200b</li> </ul>"},{"location":"zh/contributing/#_18","title":"\u884c\u4e3a\u51c6\u5219","text":"<p>\u200b\u8bf7\u200b\u5728\u200b\u6240\u6709\u200b\u4e92\u52a8\u200b\u4e2d\u200b\u4fdd\u6301\u200b\u5c0a\u91cd\u200b\u548c\u200b\u5efa\u8bbe\u6027\u200b\u3002\u200b\u6211\u4eec\u200b\u81f4\u529b\u4e8e\u200b\u4e3a\u200b\u6bcf\u4e2a\u200b\u4eba\u200b\u63d0\u4f9b\u200b\u53cb\u597d\u200b\u7684\u200b\u73af\u5883\u200b\u3002</p>"},{"location":"zh/contributing/#_19","title":"\u8bb8\u53ef\u8bc1","text":"<p>\u200b\u901a\u8fc7\u200b\u8d21\u732e\u200b\uff0c\u200b\u60a8\u200b\u540c\u610f\u200b\u60a8\u200b\u7684\u200b\u8d21\u732e\u200b\u5c06\u200b\u6839\u636e\u200b\u9879\u76ee\u200b\u7684\u200b Apache 2.0 \u200b\u8bb8\u53ef\u8bc1\u200b\u8fdb\u884c\u200b\u8bb8\u53ef\u200b\u3002</p>"},{"location":"zh/installation/","title":"\u5b89\u88c5\u200b\u6307\u5357","text":"<p>\u200b\u672c\u200b\u6307\u5357\u200b\u4ecb\u7ecd\u200b\u5982\u4f55\u200b\u5728\u200b\u60a8\u200b\u7684\u200b\u7cfb\u7edf\u200b\u4e0a\u200b\u5b89\u88c5\u200b Probing\u3002</p>"},{"location":"zh/installation/#_2","title":"\u73af\u5883\u200b\u8981\u6c42","text":"<p>\u200b\u5728\u200b\u5f00\u59cb\u200b\u4e4b\u524d\u200b\uff0c\u200b\u8bf7\u200b\u786e\u4fdd\u60a8\u200b\u7684\u200b\u7cfb\u7edf\u200b\u6ee1\u8db3\u200b\u4ee5\u4e0b\u200b\u8981\u6c42\u200b\uff1a</p> <ul> <li>Python\uff083.7 \u200b\u6216\u200b\u66f4\u200b\u9ad8\u200b\u7248\u672c\u200b\uff09</li> <li>Pip\uff08Python \u200b\u5305\u200b\u5b89\u88c5\u200b\u5668\u200b\uff09</li> <li>\u200b\u5982\u9700\u200b\u4ece\u200b\u6e90\u7801\u200b\u6784\u5efa\u200b\uff1a<ul> <li>Rust\uff08\u200b\u63a8\u8350\u200b\u6700\u65b0\u200b\u7a33\u5b9a\u7248\u200b\uff09</li> <li>Cargo\uff08Rust \u200b\u7684\u200b\u5305\u200b\u7ba1\u7406\u5668\u200b\u548c\u200b\u6784\u5efa\u200b\u7cfb\u7edf\u200b\uff09</li> </ul> </li> </ul>"},{"location":"zh/installation/#_3","title":"\u5b89\u88c5\u200b\u65b9\u5f0f","text":""},{"location":"zh/installation/#1-pip","title":"1. \u200b\u4f7f\u7528\u200b Pip\uff08\u200b\u63a8\u8350\u200b\uff09","text":"<p>\u200b\u8fd9\u662f\u200b\u5b89\u88c5\u200b Probing \u200b\u6700\u200b\u7b80\u5355\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff1a</p> <pre><code>pip install probing\n</code></pre> <p>\u200b\u6b64\u200b\u547d\u4ee4\u200b\u5c06\u200b\u4ece\u200b Python Package Index (PyPI) \u200b\u4e0b\u8f7d\u200b\u5e76\u200b\u5b89\u88c5\u200b Probing \u200b\u7684\u200b\u6700\u65b0\u200b\u7a33\u5b9a\u200b\u7248\u672c\u200b\u3002</p>"},{"location":"zh/installation/#2","title":"2. \u200b\u4ece\u200b\u6e90\u7801\u200b\u6784\u5efa","text":"<p>\u200b\u5982\u679c\u200b\u60a8\u200b\u9700\u8981\u200b\u6700\u65b0\u200b\u7684\u200b\u5f00\u53d1\u200b\u7248\u672c\u200b\u6216\u200b\u60f3\u8981\u200b\u8d21\u732e\u200b\u4ee3\u7801\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4ece\u200b\u6e90\u7801\u200b\u6784\u5efa\u200b\uff1a</p> <pre><code># 1. \u200b\u514b\u9686\u200b\u4ed3\u5e93\u200b\ngit clone https://github.com/DeepLink-org/probing.git\ncd probing\n\n# 2. \u200b\u6784\u5efa\u200b\u5e76\u200b\u5b89\u88c5\u200b Python \u200b\u5305\u200b\nmake wheel\npip install dist/probing-*.whl\n</code></pre> <p>\u200b\u8fd9\u200b\u5c06\u200b\u7f16\u8bd1\u200b Rust \u200b\u7ec4\u4ef6\u200b\u5e76\u200b\u6784\u5efa\u200b\u7528\u4e8e\u200b\u5b89\u88c5\u200b\u7684\u200b Python wheel \u200b\u5305\u200b\u3002</p>"},{"location":"zh/installation/#_4","title":"\u9a8c\u8bc1\u200b\u5b89\u88c5","text":"<p>\u200b\u5b89\u88c5\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u547d\u4ee4\u200b\u9a8c\u8bc1\u200b Probing \u200b\u662f\u5426\u200b\u6b63\u786e\u200b\u5b89\u88c5\u200b\uff1a</p> <pre><code>probing --version\n</code></pre> <p>\u200b\u5e94\u8be5\u200b\u4f1a\u200b\u8f93\u51fa\u200b\u5df2\u200b\u5b89\u88c5\u200b\u7684\u200b Probing \u200b\u7248\u672c\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff1a</p> <pre><code>probing 0.2.3\n</code></pre> <p>\u200b\u60a8\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u68c0\u67e5\u200b <code>probing</code> \u200b\u547d\u4ee4\u200b\u662f\u5426\u200b\u53ef\u7528\u200b\uff1a</p> <pre><code>probing list\n</code></pre> <p>\u200b\u6b64\u200b\u547d\u4ee4\u200b\u5e94\u8be5\u200b\u4f1a\u200b\u5217\u51fa\u200b\u53ef\u7528\u200b\u7684\u200b probing \u200b\u547d\u4ee4\u200b\u6216\u200b\u663e\u793a\u200b\u5f53\u524d\u200b\u6ca1\u6709\u200b\u8fdb\u7a0b\u200b\u6b63\u5728\u200b\u88ab\u200b\u63a2\u6d4b\u200b\u3002</p>"},{"location":"zh/installation/#_5","title":"\u5e73\u53f0\u200b\u652f\u6301","text":"\u5e73\u53f0\u200b \u200b\u6ce8\u5165\u200b\u529f\u80fd\u200b \u200b\u67e5\u8be2\u200b/\u200b\u6267\u884c\u200b Linux \u2705 \u200b\u5b8c\u5168\u200b\u652f\u6301\u200b \u2705 \u200b\u5b8c\u5168\u200b\u652f\u6301\u200b macOS \u274c \u200b\u4e0d\u200b\u652f\u6301\u200b \u2705 \u200b\u652f\u6301\u200b Windows \u274c \u200b\u4e0d\u200b\u652f\u6301\u200b \u2705 \u200b\u652f\u6301\u200b <p>\u200b\u6ce8\u5165\u200b\u529f\u80fd\u200b\u9700\u8981\u200b Linux</p> <p>\u200b\u52a8\u6001\u200b\u63a2\u9488\u200b\u6ce8\u5165\u200b\u529f\u80fd\u200b\uff08<code>probing inject</code>\uff09\u200b\u9700\u8981\u200b Linux \u200b\u7cfb\u7edf\u200b\u3002\u200b\u5728\u200b\u5176\u4ed6\u200b\u5e73\u53f0\u200b\u4e0a\u200b\uff0c\u200b\u5982\u679c\u200b\u76ee\u6807\u200b\u8fdb\u7a0b\u200b\u5728\u200b\u542f\u52a8\u200b\u65f6\u200b\u542f\u7528\u200b\u4e86\u200b probing\uff0c\u200b\u60a8\u200b\u4ecd\u7136\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u67e5\u8be2\u200b\u548c\u200b\u6267\u884c\u200b\u529f\u80fd\u200b\u3002</p>"},{"location":"zh/installation/#_6","title":"\u4e0b\u200b\u4e00\u6b65","text":"<p>\u200b\u5b89\u88c5\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5f00\u59cb\u200b\u4f7f\u7528\u200b Probing\uff1a</p> <ul> <li>\u200b\u5feb\u901f\u200b\u5f00\u59cb\u200b - \u200b\u5f00\u59cb\u200b\u60a8\u200b\u7684\u200b\u7b2c\u4e00\u6b21\u200b\u5206\u6790\u200b</li> <li>SQL \u200b\u5206\u6790\u200b - \u200b\u5b66\u4e60\u200b SQL \u200b\u67e5\u8be2\u200b\u63a5\u53e3\u200b</li> <li>\u200b\u5185\u5b58\u200b\u5206\u6790\u200b - \u200b\u8c03\u8bd5\u200b\u5185\u5b58\u200b\u95ee\u9898\u200b</li> </ul>"},{"location":"zh/quickstart/","title":"\u5feb\u901f\u200b\u5f00\u59cb","text":"<p>\u200b\u901a\u8fc7\u200b\u8fd9\u4e2a\u200b\u7cbe\u7b80\u200b\u7684\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\uff0c\u200b\u5feb\u901f\u200b\u83b7\u5f97\u200b Probing \u200b\u7684\u200b\u4ef7\u503c\u200b\u3002</p>"},{"location":"zh/quickstart/#5","title":"5 \u200b\u5206\u949f\u200b\u4e0a\u200b\u624b","text":""},{"location":"zh/quickstart/#1","title":"\u6b65\u9aa4\u200b 1\uff1a\u200b\u8bbe\u7f6e\u200b\u76ee\u6807\u200b\u8fdb\u7a0b","text":"<p>\u200b\u6240\u6709\u200b Probing \u200b\u547d\u4ee4\u200b\u90fd\u200b\u9700\u8981\u200b\u4e00\u4e2a\u200b\u76ee\u6807\u200b\u7aef\u70b9\u200b\u3002\u200b\u5c06\u200b <code>$ENDPOINT</code> \u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u672c\u5730\u200b\u8fdb\u7a0b\u200b ID \u200b\u6216\u200b\u8fdc\u7a0b\u200b\u5730\u5740\u200b\uff1a</p> <pre><code># \u200b\u672c\u5730\u200b\u8fdb\u7a0b\u200b - \u200b\u67e5\u627e\u200b\u5e76\u200b\u8bbe\u7f6e\u200b Python \u200b\u8fdb\u7a0b\u200b ID\nexport ENDPOINT=$(pgrep -f \"python.*your_script\")\n\n# \u200b\u6216\u8005\u200b\u8fdc\u7a0b\u200b\u8fdb\u7a0b\u200b\nexport ENDPOINT=remote-host:8080\n</code></pre> <p>\u200b\u67e5\u627e\u200b\u8fdb\u7a0b\u200b</p> <p>\u200b\u4f7f\u7528\u200b <code>ps aux | grep python</code> \u200b\u6216\u200b <code>pgrep -f \"python.*train\"</code> \u200b\u6765\u200b\u5b9a\u4f4d\u200b\u76ee\u6807\u200b\u8fdb\u7a0b\u200b\u3002</p>"},{"location":"zh/quickstart/#2","title":"\u6b65\u9aa4\u200b 2\uff1a\u200b\u8fde\u63a5\u200b\u5e76\u200b\u63a2\u7d22","text":"<pre><code># \u200b\u8fde\u63a5\u200b\u5230\u200b\u8fdb\u7a0b\u200b\uff08\u200b\u4ec5\u200b Linux\uff09\nprobing $ENDPOINT inject\n\n# \u200b\u83b7\u53d6\u200b\u57fa\u672c\u200b\u8fdb\u7a0b\u200b\u4fe1\u606f\u200b\nprobing $ENDPOINT eval \"import os, psutil; proc = psutil.Process(); print(f'PID: {os.getpid()}, \u200b\u5185\u5b58\u200b: {proc.memory_info().rss/1024**2:.1f}MB')\"\n</code></pre>"},{"location":"zh/quickstart/#3","title":"\u6b65\u9aa4\u200b 3\uff1a\u200b\u5c1d\u8bd5\u200b\u4e09\u5927\u200b\u6838\u5fc3\u200b\u529f\u80fd","text":""},{"location":"zh/quickstart/#_2","title":"\ud83d\udcca \u200b\u67e5\u8be2\u200b\u7ed3\u6784\u5316\u200b\u6570\u636e","text":"<pre><code>probing $ENDPOINT query \"SELECT name, value FROM information_schema.df_settings LIMIT 5\"\n</code></pre>"},{"location":"zh/quickstart/#_3","title":"\ud83c\udfaf \u200b\u6267\u884c\u200b\u5b9e\u65f6\u200b\u4ee3\u7801","text":"<pre><code>probing $ENDPOINT eval \"import torch; print(f'CUDA: {torch.cuda.is_available()}')\"\n</code></pre>"},{"location":"zh/quickstart/#_4","title":"\ud83d\udd0d \u200b\u6355\u83b7\u200b\u6267\u884c\u200b\u4e0a\u4e0b\u6587","text":"<pre><code>probing $ENDPOINT backtrace\n\nprobing $ENDPOINT query \"SELECT func, file, lineno FROM python.backtrace ORDER BY depth LIMIT 5\"\n</code></pre>"},{"location":"zh/quickstart/#_5","title":"\u4e09\u5927\u200b\u6838\u5fc3\u200b\u80fd\u529b","text":"<p>Probing \u200b\u63d0\u4f9b\u200b\u4e09\u4e2a\u200b\u5f3a\u5927\u200b\u7684\u200b\u80fd\u529b\u200b\uff0c\u200b\u5b83\u4eec\u200b\u534f\u540c\u5de5\u4f5c\u200b\uff1a</p>"},{"location":"zh/quickstart/#eval","title":"\ud83c\udfaf eval\uff1a\u200b\u5728\u200b\u8fd0\u884c\u200b\u4e2d\u200b\u7684\u200b\u8fdb\u7a0b\u200b\u6267\u884c\u200b\u4ee3\u7801","text":"<p>\u200b\u76f4\u63a5\u200b\u5728\u200b\u76ee\u6807\u200b\u8fdb\u7a0b\u200b\u4e2d\u200b\u8fd0\u884c\u200b\u4efb\u610f\u200b Python \u200b\u4ee3\u7801\u200b\uff1a</p> <pre><code># \u200b\u68c0\u67e5\u200b\u8bad\u7ec3\u200b\u7ebf\u7a0b\u200b\nprobing $ENDPOINT eval \"import threading; [print(f'{t.name}: {t.is_alive()}') for t in threading.enumerate()]\"\n\n# \u200b\u68c0\u67e5\u200b GPU \u200b\u5185\u5b58\u200b\u4f7f\u7528\u200b\nprobing $ENDPOINT eval \"import torch; print(f'GPU: {torch.cuda.memory_allocated()/1024**3:.1f}GB \u200b\u5df2\u200b\u5206\u914d\u200b')\"\n</code></pre>"},{"location":"zh/quickstart/#query-sql","title":"\ud83d\udcca query\uff1a\u200b\u7528\u200b SQL \u200b\u5206\u6790\u200b\u6570\u636e","text":"<p>\u200b\u4f7f\u7528\u200b\u719f\u6089\u200b\u7684\u200b SQL \u200b\u8bed\u6cd5\u200b\u67e5\u8be2\u200b\u7ed3\u6784\u5316\u200b\u6027\u80fd\u200b\u6570\u636e\u200b\uff1a</p> <pre><code>probing $ENDPOINT query \"\nSELECT\n    step,\n    module,\n    SUM(allocated) as total_memory_mb,\n    COUNT(*) as operation_count\nFROM python.torch_trace\nWHERE step &gt; 100\nGROUP BY step, module\nORDER BY total_memory_mb DESC\nLIMIT 10\"\n</code></pre>"},{"location":"zh/quickstart/#backtrace","title":"\ud83d\udd0d backtrace\uff1a\u200b\u5e26\u200b\u5806\u6808\u200b\u4e0a\u4e0b\u6587\u200b\u7684\u200b\u8c03\u8bd5","text":"<p>\u200b\u6355\u83b7\u200b\u5e26\u6709\u200b Python \u200b\u53d8\u91cf\u503c\u200b\u7684\u200b\u8be6\u7ec6\u200b\u8c03\u7528\u200b\u6808\u200b\uff1a</p> <pre><code># \u200b\u6355\u83b7\u200b\u5f53\u524d\u200b\u8c03\u7528\u200b\u6808\u200b\nprobing $ENDPOINT backtrace\n\n# \u200b\u67e5\u8be2\u200b\u5806\u6808\u200b\u8ddf\u8e2a\u200b\nprobing $ENDPOINT query \"SELECT func, file, lineno FROM python.backtrace ORDER BY depth LIMIT 3\"\n</code></pre>"},{"location":"zh/quickstart/#_6","title":"\u771f\u5b9e\u200b\u8c03\u8bd5\u200b\u573a\u666f","text":""},{"location":"zh/quickstart/#1_1","title":"\u573a\u666f\u200b 1\uff1a\u200b\u8bad\u7ec3\u200b\u8fdb\u7a0b\u200b\u5361\u4f4f","text":"<p>\u200b\u95ee\u9898\u200b\uff1aPyTorch \u200b\u8bad\u7ec3\u200b\u7a81\u7136\u200b\u505c\u6b62\u200b\u8fdb\u5c55\u200b\u3002</p> <pre><code># 1. \u200b\u67e5\u770b\u200b\u4e3b\u7ebf\u200b\u7a0b\u5728\u200b\u505a\u200b\u4ec0\u4e48\u200b\nprobing $ENDPOINT backtrace\n\n# 2. \u200b\u68c0\u67e5\u200b\u7ebf\u7a0b\u200b\u72b6\u6001\u200b\nprobing $ENDPOINT eval \"import threading; [(t.name, t.is_alive()) for t in threading.enumerate()]\"\n\n# 3. \u200b\u5206\u6790\u200b\u5806\u6808\u200b\u4e0a\u4e0b\u6587\u200b\nprobing $ENDPOINT query \"SELECT func, file, lineno FROM python.backtrace ORDER BY depth LIMIT 10\"\n</code></pre>"},{"location":"zh/quickstart/#2_1","title":"\u573a\u666f\u200b 2\uff1a\u200b\u5185\u5b58\u200b\u6cc4\u6f0f\u200b\u6392\u67e5","text":"<p>\u200b\u95ee\u9898\u200b\uff1a\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u5185\u5b58\u200b\u4f7f\u7528\u200b\u6301\u7eed\u589e\u957f\u200b\u3002</p> <pre><code># \u200b\u5f3a\u5236\u200b\u6e05\u7406\u200b\u5e76\u200b\u83b7\u53d6\u200b\u5f53\u524d\u200b\u72b6\u6001\u200b\nprobing $ENDPOINT eval \"import gc, torch; gc.collect(); torch.cuda.empty_cache()\"\n\n# \u200b\u5206\u6790\u200b\u5206\u914d\u200b\u8d8b\u52bf\u200b\nprobing $ENDPOINT query \"SELECT step, AVG(allocated) as avg_memory FROM python.torch_trace GROUP BY step ORDER BY step\"\n</code></pre>"},{"location":"zh/quickstart/#3_1","title":"\u573a\u666f\u200b 3\uff1a\u200b\u6027\u80fd\u200b\u74f6\u9888\u200b\u5206\u6790","text":"<p>\u200b\u95ee\u9898\u200b\uff1a\u200b\u9700\u8981\u200b\u627e\u51fa\u200b\u54ea\u4e9b\u200b\u6a21\u578b\u200b\u7ec4\u4ef6\u200b\u6700\u6162\u200b\u3002</p> <pre><code># \u200b\u67e5\u627e\u200b\u6700\u200b\u8017\u65f6\u200b\u7684\u200b\u64cd\u4f5c\u200b\nprobing $ENDPOINT query \"\nSELECT module, stage, AVG(duration) as avg_duration\nFROM python.torch_trace\nGROUP BY module, stage\nORDER BY avg_duration DESC\nLIMIT 10\"\n</code></pre>"},{"location":"zh/quickstart/#_7","title":"\u4e0b\u200b\u4e00\u6b65","text":"<ul> <li>SQL \u200b\u5206\u6790\u200b - \u200b\u9ad8\u7ea7\u200b\u67e5\u8be2\u200b\u6280\u5de7\u200b</li> <li>\u200b\u5185\u5b58\u200b\u5206\u6790\u200b - \u200b\u6df1\u5165\u200b\u5185\u5b58\u200b\u8c03\u8bd5\u200b</li> <li>\u200b\u8c03\u8bd5\u200b\u6307\u5357\u200b - \u200b\u4e13\u5bb6\u7ea7\u200b\u8c03\u8bd5\u6a21\u5f0f\u200b</li> </ul>"},{"location":"zh/versions/","title":"\u7248\u672c\u200b\u517c\u5bb9\u6027","text":"<p>\u200b\u672c\u9875\u200b\u8bb0\u5f55\u200b Probing \u200b\u7684\u200b\u7248\u672c\u200b\u517c\u5bb9\u6027\u200b\u548c\u200b\u66f4\u65b0\u200b\u65e5\u5fd7\u200b\u3002</p>"},{"location":"zh/versions/#_2","title":"\u5f53\u524d\u200b\u7248\u672c","text":"<p>Probing v0.6.x (\u200b\u6700\u65b0\u200b)</p>"},{"location":"zh/versions/#_3","title":"\u7cfb\u7edf\u200b\u8981\u6c42","text":""},{"location":"zh/versions/#python","title":"Python \u200b\u7248\u672c","text":"Probing \u200b\u7248\u672c\u200b Python \u200b\u652f\u6301\u200b 0.6.x Python 3.9 - 3.12 0.5.x Python 3.8 - 3.11"},{"location":"zh/versions/#pytorch","title":"PyTorch \u200b\u7248\u672c","text":"Probing \u200b\u7248\u672c\u200b PyTorch \u200b\u652f\u6301\u200b 0.6.x PyTorch 2.0+ 0.5.x PyTorch 1.13+"},{"location":"zh/versions/#_4","title":"\u64cd\u4f5c\u7cfb\u7edf","text":"<ul> <li>Linux: \u200b\u5b8c\u5168\u200b\u652f\u6301\u200b\uff08\u200b\u751f\u4ea7\u200b\u73af\u5883\u200b\u63a8\u8350\u200b\uff09</li> <li>macOS: \u200b\u5b8c\u5168\u200b\u652f\u6301\u200b\uff08Intel \u200b\u548c\u200b Apple Silicon\uff09</li> <li>Windows: \u200b\u5b9e\u9a8c\u6027\u200b\u652f\u6301\u200b\uff08\u200b\u63a8\u8350\u200b\u4f7f\u7528\u200b WSL2\uff09</li> </ul>"},{"location":"zh/versions/#_5","title":"\u66f4\u65b0\u200b\u65e5\u5fd7","text":""},{"location":"zh/versions/#v060","title":"v0.6.0","text":"<p>\u200b\u65b0\u200b\u529f\u80fd\u200b</p> <ul> <li>\u200b\u57fa\u4e8e\u200b DataFusion \u200b\u7684\u200b SQL \u200b\u67e5\u8be2\u200b\u5f15\u64ce\u200b</li> <li>\u200b\u6587\u6863\u200b\u652f\u6301\u200b Mermaid \u200b\u56fe\u8868\u200b</li> <li>\u200b\u6539\u8fdb\u200b\u7684\u200b\u5206\u5e03\u5f0f\u200b\u8c03\u8bd5\u200b\u652f\u6301\u200b</li> <li>\u200b\u65b0\u589e\u200b <code>torch_trace</code> \u200b\u8868\u200b\u7528\u4e8e\u200b PyTorch \u200b\u6027\u80fd\u200b\u5206\u6790\u200b</li> </ul> <p>\u200b\u7834\u574f\u6027\u200b\u53d8\u66f4\u200b</p> <ul> <li>\u200b\u5e9f\u5f03\u200b <code>probing.trace()</code> API\uff0c\u200b\u6539\u7528\u200b <code>probing.enable_torch_profiling()</code></li> <li>\u200b\u914d\u7f6e\u200b\u683c\u5f0f\u200b\u4ece\u200b JSON \u200b\u6539\u4e3a\u200b TOML</li> </ul> <p>Bug \u200b\u4fee\u590d\u200b</p> <ul> <li>\u200b\u4fee\u590d\u200b\u957f\u65f6\u95f4\u200b\u8fd0\u884c\u200b\u4f1a\u8bdd\u200b\u4e2d\u200b\u7684\u200b\u5185\u5b58\u200b\u6cc4\u6f0f\u200b</li> <li>\u200b\u6539\u8fdb\u200b\u65e0\u6548\u200b SQL \u200b\u67e5\u8be2\u200b\u7684\u200b\u9519\u8bef\u200b\u6d88\u606f\u200b</li> </ul>"},{"location":"zh/versions/#v050","title":"v0.5.0","text":"<p>\u200b\u65b0\u200b\u529f\u80fd\u200b</p> <ul> <li>\u200b\u521d\u59cb\u200b PyTorch \u200b\u6027\u80fd\u200b\u5206\u6790\u200b\u652f\u6301\u200b</li> <li>\u200b\u5185\u5b58\u200b\u5206\u6790\u200b\u80fd\u529b\u200b</li> <li>\u200b\u57fa\u7840\u200b SQL \u200b\u67e5\u8be2\u200b\u652f\u6301\u200b</li> </ul> <p>Bug \u200b\u4fee\u590d\u200b</p> <ul> <li>\u200b\u5404\u79cd\u200b\u7a33\u5b9a\u6027\u200b\u6539\u8fdb\u200b</li> </ul>"},{"location":"zh/versions/#_6","title":"\u5347\u7ea7\u200b\u6307\u5357","text":""},{"location":"zh/versions/#v05x-v06x","title":"\u4ece\u200b v0.5.x \u200b\u5347\u7ea7\u200b\u5230\u200b v0.6.x","text":"<ol> <li>\u200b\u5982\u200b\u9700\u8981\u200b\uff0c\u200b\u5347\u7ea7\u200b Python \u200b\u5230\u200b 3.9+</li> <li>\u200b\u5982\u200b\u9700\u8981\u200b\uff0c\u200b\u5347\u7ea7\u200b PyTorch \u200b\u5230\u200b 2.0+</li> <li>\u200b\u66f4\u65b0\u200b Probing\uff1a</li> </ol> <pre><code>pip install --upgrade probing\n</code></pre> <ol> <li>\u200b\u66f4\u65b0\u200b\u914d\u7f6e\u6587\u4ef6\u200b\uff08\u200b\u5982\u679c\u200b\u4f7f\u7528\u200b\u81ea\u5b9a\u4e49\u200b\u914d\u7f6e\u200b\uff09\uff1a</li> </ol> <pre><code># \u200b\u65e7\u200b\u683c\u5f0f\u200b (v0.5.x)\nprobing.trace(enabled=True)\n\n# \u200b\u65b0\u200b\u683c\u5f0f\u200b (v0.6.x)\nprobing.enable_torch_profiling()\n</code></pre>"},{"location":"zh/versions/#_7","title":"\u5e9f\u5f03\u200b\u7b56\u7565","text":"<ul> <li>\u200b\u4e3b\u200b\u7248\u672c\u200b\u53d8\u66f4\u200b\u53ef\u80fd\u200b\u5305\u542b\u200b\u7834\u574f\u6027\u200b\u53d8\u66f4\u200b</li> <li>\u200b\u6b21\u200b\u7248\u672c\u200b\u53d8\u66f4\u200b\u4fdd\u6301\u200b\u5411\u200b\u540e\u200b\u517c\u5bb9\u6027\u200b</li> <li>\u200b\u5e9f\u5f03\u200b\u529f\u80fd\u200b\u5728\u200b\u79fb\u9664\u200b\u524d\u200b\u81f3\u5c11\u200b\u663e\u793a\u200b\u4e00\u4e2a\u200b\u6b21\u200b\u7248\u672c\u200b\u7684\u200b\u8b66\u544a\u200b</li> </ul>"},{"location":"zh/versions/#_8","title":"\u529f\u80fd\u200b\u652f\u6301\u200b\u77e9\u9635","text":"\u529f\u80fd\u200b v0.5.x v0.6.x \u200b\u57fa\u7840\u200b\u6027\u80fd\u200b\u5206\u6790\u200b \u2705 \u2705 SQL \u200b\u67e5\u8be2\u200b \u200b\u90e8\u5206\u200b \u2705 PyTorch \u200b\u8ddf\u8e2a\u200b \u200b\u57fa\u7840\u200b \u200b\u5b8c\u6574\u200b \u200b\u5185\u5b58\u200b\u5206\u6790\u200b \u200b\u57fa\u7840\u200b \u200b\u5b8c\u6574\u200b \u200b\u5206\u5e03\u5f0f\u200b\u652f\u6301\u200b \u274c \u2705 \u200b\u81ea\u5b9a\u4e49\u200b\u8868\u200b \u274c \u2705 Web UI \u274c Beta"},{"location":"zh/versions/#_9","title":"\u62a5\u544a\u200b\u95ee\u9898","text":"<p>\u200b\u5982\u9700\u200b\u62a5\u544a\u200b Bug \u200b\u6216\u200b\u529f\u80fd\u200b\u8bf7\u6c42\u200b\uff0c\u200b\u8bf7\u200b\u4f7f\u7528\u200b GitHub Issue Tracker\u3002</p> <p>\u200b\u62a5\u544a\u200b\u95ee\u9898\u200b\u65f6\u200b\uff0c\u200b\u8bf7\u200b\u5305\u542b\u200b\uff1a</p> <ul> <li>Probing \u200b\u7248\u672c\u200b (<code>pip show probing</code>)</li> <li>Python \u200b\u7248\u672c\u200b (<code>python --version</code>)</li> <li>PyTorch \u200b\u7248\u672c\u200b\uff08\u200b\u5982\u200b\u9002\u7528\u200b\uff09</li> <li>\u200b\u64cd\u4f5c\u7cfb\u7edf\u200b</li> <li>\u200b\u6700\u5c0f\u200b\u590d\u73b0\u200b\u793a\u4f8b\u200b</li> </ul>"},{"location":"zh/design/","title":"\u8bbe\u8ba1\u200b\u6982\u89c8","text":""},{"location":"zh/design/#probing","title":"\u4e3a\u4ec0\u4e48\u200b\u9009\u62e9\u200b Probing\uff1f","text":""},{"location":"zh/design/#pythonic","title":"Pythonic \u200b\u7684\u200b\u4f18\u52bf","text":"<p>Python \u200b\u5728\u200b AI \u200b\u9886\u57df\u200b\u7684\u200b\u4e3b\u5bfc\u5730\u4f4d\u200b\u6e90\u4e8e\u200b\u4e00\u4e2a\u200b\u6838\u5fc3\u200b\u539f\u5219\u200b\uff1a\u200b\u4e00\u5207\u200b\u90fd\u200b\u50cf\u200b Python\u3002\u200b\u65e0\u8bba\u200b\u60a8\u200b\u4f7f\u7528\u200b pandas\u3001PyTorch \u200b\u8fd8\u662f\u200b NumPy\uff0c\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u7528\u200b Pythonic \u200b\u7684\u200b\u65b9\u5f0f\u200b\u4e0e\u200b\u5b83\u4eec\u200b\u4ea4\u4e92\u200b\u2014\u2014\u200b\u76f8\u540c\u200b\u7684\u200b <code>print()</code>\u3001\u200b\u8fed\u4ee3\u200b\u548c\u200b\u5c5e\u6027\u200b\u8bbf\u95ee\u200b\u6a21\u5f0f\u200b\u968f\u5904\u200b\u53ef\u7528\u200b\u3002</p>"},{"location":"zh/design/#_2","title":"\u5206\u5e03\u5f0f\u7cfb\u7edf\u200b\u5982\u4f55\u200b\u7834\u574f\u200b\u4e86\u200b\u8fd9\u200b\u4e00\u70b9","text":"<p>\u200b\u5f53\u200b AI \u200b\u6a21\u578b\u200b\u6269\u5c55\u200b\u5230\u200b\u5206\u5e03\u5f0f\u200b\u96c6\u7fa4\u200b\u65f6\u200b\uff0c\u200b\u4e00\u4e9b\u200b\u6839\u672c\u6027\u200b\u7684\u200b\u4e1c\u897f\u200b\u88ab\u200b\u6253\u7834\u200b\u4e86\u200b\uff1a\u200b\u5206\u5e03\u5f0f\u7cfb\u7edf\u200b\u4e0d\u662f\u200b Pythonic \u200b\u7684\u200b\u3002\u200b\u5355\u673a\u200b\u8c03\u8bd5\u200b\u611f\u89c9\u200b\u5f88\u200b\u81ea\u7136\u200b\u2014\u2014<code>print(model.parameters())</code>\u3001<code>loss.item()</code>\u3001<code>torch.cuda.memory_allocated()</code>\u2014\u2014\u200b\u4f46\u200b\u5206\u5e03\u5f0f\u200b\u8c03\u8bd5\u200b\u8feb\u4f7f\u200b\u60a8\u200b\u4f7f\u7528\u200b\u7cfb\u7edf\u7ba1\u7406\u200b\u5de5\u5177\u200b\uff1a<code>kubectl get nodes</code>\u3001SSH \u200b\u4f1a\u8bdd\u200b\u3001\u200b\u65e5\u5fd7\u200b\u6587\u4ef6\u200b\u89e3\u6790\u200b\u3001\u200b\u76d1\u63a7\u200b\u4eea\u8868\u677f\u200b\u3002</p>"},{"location":"zh/design/#probing_1","title":"Probing \u200b\u7684\u200b\u4f7f\u547d","text":"<p>Probing \u200b\u7684\u200b\u6838\u5fc3\u200b\u4f7f\u547d\u200b\u5f88\u200b\u7b80\u5355\u200b\uff1a\u200b\u8ba9\u200b\u5206\u5e03\u5f0f\u7cfb\u7edf\u200b\u91cd\u65b0\u200b\u53d8\u5f97\u200b Pythonic\u3002\u200b\u60a8\u200b\u7684\u200b\u96c6\u7fa4\u200b\u3001\u200b\u8282\u70b9\u200b\u548c\u200b\u5206\u5e03\u5f0f\u200b\u8fdb\u7a0b\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u719f\u6089\u200b\u7684\u200b\u63a5\u53e3\u200b\u8bbf\u95ee\u200b\u3002\u200b\u4e0d\u518d\u200b\u9700\u8981\u200b\u5728\u200b\u5de5\u5177\u200b\u4e4b\u95f4\u200b\u5207\u6362\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u7559\u5728\u200b Python \u200b\u4e2d\u200b\uff0c\u200b\u7528\u200b Pythonic \u200b\u7684\u200b\u65b9\u5f0f\u200b\u4e0e\u200b\u60a8\u200b\u7684\u200b\u5206\u5e03\u5f0f\u7cfb\u7edf\u200b\u5bf9\u8bdd\u200b\u3002</p>"},{"location":"zh/design/#_3","title":"\u8bbe\u8ba1\u200b\u539f\u5219","text":""},{"location":"zh/design/#_4","title":"\ud83d\udd0d \u200b\u96f6\u200b\u4fb5\u5165","text":"<ul> <li>\u200b\u65e0\u9700\u200b\u4fee\u6539\u200b\u4ee3\u7801\u200b</li> <li>\u200b\u65e0\u9700\u200b\u66f4\u6539\u200b\u73af\u5883\u200b\u8bbe\u7f6e\u200b</li> <li>\u200b\u65e0\u9700\u200b\u4e2d\u65ad\u200b\u5de5\u4f5c\u200b\u6d41\u200b</li> <li>\u200b\u52a8\u6001\u200b\u63a2\u9488\u200b\u6ce8\u5165\u200b\u5230\u200b\u8fd0\u884c\u200b\u4e2d\u200b\u7684\u200b\u8fdb\u7a0b\u200b</li> </ul>"},{"location":"zh/design/#_5","title":"\ud83c\udfaf \u200b\u96f6\u200b\u5b66\u4e60\u66f2\u7ebf","text":"<ul> <li>\u200b\u6807\u51c6\u200b SQL \u200b\u63a5\u53e3\u200b\u7528\u4e8e\u200b\u6570\u636e\u5206\u6790\u200b</li> <li>\u200b\u719f\u6089\u200b\u7684\u200b\u6570\u636e\u5e93\u200b\u67e5\u8be2\u200b\u6a21\u5f0f\u200b</li> <li>\u200b\u76f4\u89c2\u200b\u7684\u200b\u547d\u4ee4\u884c\u200b\u5de5\u5177\u200b</li> <li>\u200b\u57fa\u4e8e\u200b Web \u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u4eea\u8868\u677f\u200b</li> </ul>"},{"location":"zh/design/#_6","title":"\ud83d\udce6 \u200b\u96f6\u200b\u90e8\u7f72\u200b\u8d1f\u62c5","text":"<ul> <li>\u200b\u5355\u4e00\u200b\u4e8c\u8fdb\u5236\u200b\u90e8\u7f72\u200b\uff08\u200b\u57fa\u4e8e\u200b Rust\uff09</li> <li>\u200b\u9759\u6001\u200b\u7f16\u8bd1\u200b\uff0c\u200b\u6700\u5c0f\u200b\u4f9d\u8d56\u200b</li> <li>Linux \u200b\u4f18\u5148\u200b\u8bbe\u8ba1\u200b\uff0c\u200b\u5176\u4ed6\u200b\u5e73\u53f0\u200b\u652f\u6301\u200b\u67e5\u8be2\u200b/\u200b\u6267\u884c\u200b</li> <li>\u200b\u5f39\u6027\u200b\u6269\u5c55\u200b\u80fd\u529b\u200b</li> </ul>"},{"location":"zh/design/#_7","title":"\u8bbe\u8ba1\u200b\u6587\u6863","text":"\u6587\u6863\u200b \u200b\u63cf\u8ff0\u200b \u200b\u7cfb\u7edf\u200b\u67b6\u6784\u200b \u200b\u7cfb\u7edf\u7ed3\u6784\u200b\u548c\u200b\u7ec4\u4ef6\u200b \u200b\u6027\u80fd\u200b\u5206\u6790\u200b \u200b\u6027\u80fd\u200b\u6570\u636e\u200b\u6536\u96c6\u200b \u200b\u8c03\u8bd5\u200b \u200b\u8c03\u8bd5\u200b\u80fd\u529b\u200b \u200b\u5206\u5e03\u5f0f\u200b \u200b\u591a\u200b\u8282\u70b9\u200b\u652f\u6301\u200b \u200b\u6269\u5c55\u200b\u673a\u5236\u200b \u200b\u81ea\u5b9a\u4e49\u200b\u8868\u200b\u548c\u200b\u6307\u6807"},{"location":"zh/design/architecture/","title":"\u7cfb\u7edf\u200b\u67b6\u6784","text":"<p>Probing \u200b\u91c7\u7528\u200b\u7b80\u6d01\u200b\u7684\u200b\u4e24\u5c42\u200b\u67b6\u6784\u8bbe\u8ba1\u200b\uff0c\u200b\u4ee5\u200b\u6700\u5c0f\u5316\u200b\u590d\u6742\u6027\u200b\u548c\u200b\u90e8\u7f72\u200b\u96be\u5ea6\u200b\u3002</p>"},{"location":"zh/design/architecture/#_2","title":"\u9ad8\u5c42\u200b\u67b6\u6784","text":"<pre><code>graph TB\n    subgraph \"\u200b\u7528\u6237\u754c\u9762\u200b\"\n        CLI[CLI \u200b\u5ba2\u6237\u7aef\u200b]\n        HTTP[HTTP API]\n        WEB[Web UI]\n    end\n\n    subgraph \"\u200b\u76ee\u6807\u200b\u8fdb\u7a0b\u200b\"\n        PROBE[\u200b\u63a2\u9488\u200b]\n        subgraph \"\u200b\u63a2\u9488\u200b\u7ec4\u4ef6\u200b\"\n            ENGINE[\u200b\u5f15\u64ce\u200b]\n            SERVER[\u200b\u670d\u52a1\u5668\u200b]\n            EXT[\u200b\u6269\u5c55\u200b]\n        end\n    end\n\n    CLI --&gt; |Unix Socket / TCP| PROBE\n    HTTP --&gt; |HTTP/REST| PROBE\n    WEB --&gt; |WebSocket| PROBE\n\n    PROBE --&gt; ENGINE\n    PROBE --&gt; SERVER\n    PROBE --&gt; EXT</code></pre>"},{"location":"zh/design/architecture/#_3","title":"\u7ec4\u4ef6","text":""},{"location":"zh/design/architecture/#1-probe","title":"1. \u200b\u63a2\u9488\u200b (Probe)","text":"<p>\u200b\u6ce8\u5165\u200b\u5230\u200b\u76ee\u6807\u200b\u8fdb\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u83b7\u5f97\u200b\u5bf9\u200b\u6240\u6709\u200b\u8d44\u6e90\u200b\u7684\u200b\u5b8c\u5168\u200b\u8bbf\u95ee\u200b\u6743\u9650\u200b\uff1a</p> <ul> <li>Python \u200b\u89e3\u91ca\u5668\u200b\u8bbf\u95ee\u200b</li> <li>\u200b\u6587\u4ef6\u7cfb\u7edf\u200b\u8bbf\u95ee\u200b</li> <li>\u200b\u5185\u5b58\u200b\u68c0\u67e5\u200b</li> <li>\u200b\u7f51\u7edc\u200b\u80fd\u529b\u200b</li> </ul> <p>\u200b\u63a2\u9488\u200b\u8fd0\u884c\u200b\u4e00\u4e2a\u200b\u5d4c\u5165\u5f0f\u200b HTTP \u200b\u670d\u52a1\u5668\u200b\uff0c\u200b\u76d1\u542c\u200b\u5728\u200b\uff1a</p> <ul> <li>Unix \u200b\u57df\u5957\u200b\u63a5\u5b57\u200b - \u200b\u7528\u4e8e\u200b\u672c\u5730\u8fde\u63a5\u200b\uff08\u200b\u9ed8\u8ba4\u200b\uff09</li> <li>TCP \u200b\u7aef\u53e3\u200b - \u200b\u7528\u4e8e\u200b\u8fdc\u7a0b\u200b\u8fde\u63a5\u200b</li> </ul>"},{"location":"zh/design/architecture/#2-cli","title":"2. CLI","text":"<p>\u200b\u7528\u4e8e\u200b\u63a7\u5236\u200b\u63a2\u9488\u200b\u7684\u200b\u547d\u4ee4\u884c\u200b\u754c\u9762\u200b\uff1a</p> <ul> <li>\u200b\u8fdb\u7a0b\u200b\u53d1\u73b0\u200b\u548c\u200b\u5217\u8868\u200b</li> <li>\u200b\u63a2\u9488\u200b\u6ce8\u5165\u200b\u548c\u200b\u7ba1\u7406\u200b</li> <li>\u200b\u67e5\u8be2\u200b\u6267\u884c\u200b</li> <li>\u200b\u4ee3\u7801\u6267\u884c\u200b</li> </ul> <p>\u200b\u901a\u8fc7\u200b HTTP \u200b\u534f\u8bae\u200b\u5728\u200b Unix \u200b\u57df\u5957\u200b\u63a5\u5b57\u200b\uff08\u200b\u672c\u5730\u200b\uff09\u200b\u6216\u200b TCP\uff08\u200b\u8fdc\u7a0b\u200b\uff09\u200b\u4e0a\u200b\u901a\u4fe1\u200b\u3002</p>"},{"location":"zh/design/architecture/#3-http-api","title":"3. HTTP API","text":"<p>\u200b\u7528\u4e8e\u200b\u7f16\u7a0b\u200b\u8bbf\u95ee\u200b\u7684\u200b RESTful API\uff1a</p> <ul> <li>\u200b\u6240\u6709\u200b CLI \u200b\u547d\u4ee4\u200b\u90fd\u200b\u53ef\u200b\u4f5c\u4e3a\u200b\u7aef\u70b9\u200b\u4f7f\u7528\u200b</li> <li>WebSocket \u200b\u652f\u6301\u200b\u5b9e\u65f6\u200b\u6570\u636e\u200b</li> <li>\u200b\u4e0e\u200b\u76d1\u63a7\u200b\u5de5\u5177\u200b\u96c6\u6210\u200b</li> </ul>"},{"location":"zh/design/architecture/#_4","title":"\u63a2\u9488\u200b\u5185\u90e8\u200b\u67b6\u6784","text":"<pre><code>graph LR\n    subgraph \"\u200b\u63a2\u9488\u200b\"\n        SERVER[HTTP \u200b\u670d\u52a1\u5668\u200b]\n        ENGINE[\u200b\u67e5\u8be2\u200b\u5f15\u64ce\u200b]\n        CONFIG[\u200b\u914d\u7f6e\u7ba1\u7406\u200b]\n\n        subgraph \"\u200b\u6269\u5c55\u200b\"\n            PYTHON[Python \u200b\u6269\u5c55\u200b]\n            TORCH[PyTorch \u200b\u6269\u5c55\u200b]\n            CUSTOM[\u200b\u81ea\u5b9a\u4e49\u200b\u8868\u200b]\n        end\n    end\n\n    SERVER --&gt; ENGINE\n    SERVER --&gt; CONFIG\n    ENGINE --&gt; PYTHON\n    ENGINE --&gt; TORCH\n    ENGINE --&gt; CUSTOM</code></pre>"},{"location":"zh/design/architecture/#_5","title":"\u5f15\u64ce","text":"<p>\u200b\u6838\u5fc3\u200b\u6570\u636e\u200b\u5b58\u50a8\u200b\u548c\u200b\u5904\u7406\u200b\uff1a</p> <ul> <li>DataFusion - SQL \u200b\u67e5\u8be2\u200b\u5f15\u64ce\u200b</li> <li>Arrow - \u200b\u5217\u5f0f\u200b\u6570\u636e\u683c\u5f0f\u200b</li> <li>\u200b\u65f6\u95f4\u200b\u5e8f\u5217\u200b\u6570\u636e\u200b\u5b58\u50a8\u200b</li> <li>\u200b\u914d\u7f6e\u7ba1\u7406\u200b</li> </ul>"},{"location":"zh/design/architecture/#_6","title":"\u670d\u52a1\u5668","text":"<p>HTTP \u200b\u670d\u52a1\u5668\u200b\u5904\u7406\u200b\uff1a</p> <ul> <li>\u200b\u8bf7\u6c42\u200b\u8def\u7531\u200b</li> <li>\u200b\u8ba4\u8bc1\u200b\uff08\u200b\u53ef\u200b\u9009\u200b\uff09</li> <li>WebSocket \u200b\u8fde\u63a5\u200b</li> <li>\u200b\u54cd\u5e94\u200b\u683c\u5f0f\u5316\u200b</li> </ul>"},{"location":"zh/design/architecture/#_7","title":"\u6269\u5c55","text":"<p>\u200b\u53ef\u200b\u63d2\u62d4\u200b\u7684\u200b\u6570\u636e\u200b\u63d0\u4f9b\u8005\u200b\uff1a</p> <ul> <li>Python \u200b\u6269\u5c55\u200b - \u200b\u5806\u6808\u200b\u8ddf\u8e2a\u200b\u3001\u200b\u53d8\u91cf\u200b</li> <li>PyTorch \u200b\u6269\u5c55\u200b - Torch \u200b\u8ddf\u8e2a\u200b\u3001\u200b\u5185\u5b58\u200b</li> <li>\u200b\u81ea\u5b9a\u4e49\u200b\u8868\u200b - \u200b\u7528\u6237\u200b\u5b9a\u4e49\u200b\u7684\u200b\u6570\u636e\u6e90\u200b</li> </ul>"},{"location":"zh/design/architecture/#_8","title":"\u6570\u636e\u6d41","text":"<pre><code>sequenceDiagram\n    participant CLI\n    participant \u200b\u63a2\u9488\u200b\n    participant \u200b\u5f15\u64ce\u200b\n    participant \u200b\u6269\u5c55\u200b\n\n    CLI-&gt;&gt;\u200b\u63a2\u9488\u200b: query \"SELECT * FROM python.torch_trace\"\n    \u200b\u63a2\u9488\u200b-&gt;&gt;\u200b\u5f15\u64ce\u200b: \u200b\u89e3\u6790\u200b &amp; \u200b\u8ba1\u5212\u200b\u67e5\u8be2\u200b\n    \u200b\u5f15\u64ce\u200b-&gt;&gt;\u200b\u6269\u5c55\u200b: \u200b\u8bf7\u6c42\u200b\u6570\u636e\u200b\n    \u200b\u6269\u5c55\u200b--&gt;&gt;\u200b\u5f15\u64ce\u200b: Arrow RecordBatch\n    \u200b\u5f15\u64ce\u200b--&gt;&gt;\u200b\u63a2\u9488\u200b: \u200b\u67e5\u8be2\u200b\u7ed3\u679c\u200b\n    \u200b\u63a2\u9488\u200b--&gt;&gt;CLI: JSON \u200b\u54cd\u5e94\u200b</code></pre>"},{"location":"zh/design/architecture/#_9","title":"\u901a\u4fe1\u534f\u8bae","text":""},{"location":"zh/design/architecture/#_10","title":"\u672c\u5730\u8fde\u63a5","text":"<pre><code>probing -t &lt;pid&gt; query \"...\"\n         |\n         v\n    Unix Socket: /tmp/probing-&lt;pid&gt;.sock\n         |\n         v\n    HTTP \u200b\u8bf7\u6c42\u200b: POST /query\n</code></pre>"},{"location":"zh/design/architecture/#_11","title":"\u8fdc\u7a0b\u200b\u8fde\u63a5","text":"<pre><code>probing -t host:port query \"...\"\n         |\n         v\n    TCP \u200b\u8fde\u63a5\u200b: host:port\n         |\n         v\n    HTTP \u200b\u8bf7\u6c42\u200b: POST /query\n</code></pre>"},{"location":"zh/design/architecture/#_12","title":"\u5b89\u5168\u200b\u8003\u8651","text":"<ul> <li>\u200b\u672c\u5730\u200b\u6a21\u5f0f\u200b: Unix \u200b\u5957\u200b\u63a5\u5b57\u200b\u6743\u9650\u200b\uff08\u200b\u4ec5\u200b\u8fdb\u7a0b\u200b\u6240\u6709\u8005\u200b\uff09</li> <li>\u200b\u8fdc\u7a0b\u200b\u6a21\u5f0f\u200b: \u200b\u53ef\u200b\u9009\u200b\u8ba4\u8bc1\u200b</li> <li>\u200b\u7f51\u7edc\u200b: \u200b\u652f\u6301\u200b TLS \u200b\u52a0\u5bc6\u200b</li> </ul>"},{"location":"zh/design/architecture/#_13","title":"\u6027\u80fd\u200b\u7279\u5f81","text":"\u65b9\u9762\u200b \u200b\u76ee\u6807\u200b \u200b\u5f00\u9500\u200b \u200b\u5178\u578b\u200b\u5de5\u4f5c\u200b\u8d1f\u8f7d\u200b &lt; 5% \u200b\u5185\u5b58\u200b \u200b\u989d\u5916\u200b &lt; 50MB \u200b\u5ef6\u8fdf\u200b \u200b\u67e5\u8be2\u200b &lt; 10ms \u200b\u541e\u5410\u91cf\u200b 1000+ \u200b\u67e5\u8be2\u200b/\u200b\u79d2"},{"location":"zh/design/debugging/","title":"\u8c03\u8bd5\u200b\u67b6\u6784","text":"<p>Probing \u200b\u901a\u8fc7\u200b\u4ee3\u7801\u200b\u6ce8\u5165\u200b\u548c\u200b\u5806\u6808\u200b\u5206\u6790\u200b\u63d0\u4f9b\u200b\u5f3a\u5927\u200b\u7684\u200b\u8c03\u8bd5\u200b\u80fd\u529b\u200b\u3002</p>"},{"location":"zh/design/debugging/#_2","title":"\u6982\u89c8","text":"<p>\u200b\u8c03\u8bd5\u200b\u5b50\u7cfb\u7edf\u200b\u652f\u6301\u200b\uff1a</p> <ul> <li>\u200b\u5728\u200b\u76ee\u6807\u200b\u8fdb\u7a0b\u200b\u4e2d\u200b\u5b9e\u65f6\u200b\u6267\u884c\u200b\u4ee3\u7801\u200b</li> <li>\u200b\u5e26\u200b\u53d8\u91cf\u200b\u68c0\u67e5\u200b\u7684\u200b\u5806\u6808\u200b\u8ddf\u8e2a\u200b\u6355\u83b7\u200b</li> <li>\u200b\u4ea4\u4e92\u5f0f\u200b REPL \u200b\u4f1a\u8bdd\u200b</li> <li>\u200b\u8fdc\u7a0b\u200b\u8c03\u8bd5\u200b\u652f\u6301\u200b</li> </ul>"},{"location":"zh/design/debugging/#_3","title":"\u4ee3\u7801\u6267\u884c","text":""},{"location":"zh/design/debugging/#eval","title":"Eval \u200b\u547d\u4ee4","text":"<p>\u200b\u5728\u200b\u76ee\u6807\u200b\u8fdb\u7a0b\u200b\u4e0a\u4e0b\u6587\u200b\u4e2d\u200b\u6267\u884c\u200b\u4efb\u610f\u200b Python \u200b\u4ee3\u7801\u200b\uff1a</p> <pre><code>probing $ENDPOINT eval \"print(model.state_dict().keys())\"\n</code></pre>"},{"location":"zh/design/debugging/#_4","title":"\u6267\u884c\u200b\u6d41\u7a0b","text":"<pre><code>sequenceDiagram\n    participant CLI\n    participant \u200b\u670d\u52a1\u5668\u200b\n    participant Python\n\n    CLI-&gt;&gt;\u200b\u670d\u52a1\u5668\u200b: POST /eval {\"code\": \"...\"}\n    \u200b\u670d\u52a1\u5668\u200b-&gt;&gt;Python: PyRun_String(code)\n    Python-&gt;&gt;Python: \u200b\u5728\u200b __main__ \u200b\u4e2d\u200b\u6267\u884c\u200b\n    Python--&gt;&gt;\u200b\u670d\u52a1\u5668\u200b: \u200b\u7ed3\u679c\u200b/\u200b\u5f02\u5e38\u200b\n    \u200b\u670d\u52a1\u5668\u200b--&gt;&gt;CLI: JSON \u200b\u54cd\u5e94\u200b</code></pre>"},{"location":"zh/design/debugging/#_5","title":"\u6267\u884c\u200b\u4e0a\u4e0b\u6587","text":"<p>\u200b\u4ee3\u7801\u200b\u5728\u200b\u4e3b\u200b\u6a21\u5757\u200b\u7684\u200b\u5168\u5c40\u200b\u547d\u540d\u200b\u7a7a\u95f4\u200b\u4e2d\u200b\u6267\u884c\u200b\uff1a</p> <ul> <li>\u200b\u8bbf\u95ee\u200b\u6240\u6709\u200b\u5df2\u200b\u5bfc\u5165\u200b\u7684\u200b\u6a21\u5757\u200b</li> <li>\u200b\u8bbf\u95ee\u200b\u5168\u5c40\u53d8\u91cf\u200b</li> <li>\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u4fee\u6539\u200b\u72b6\u6001\u200b</li> </ul>"},{"location":"zh/design/debugging/#_6","title":"\u5b89\u5168\u200b\u8003\u8651","text":"<ul> <li>\u200b\u4ee3\u7801\u200b\u4ee5\u200b\u5b8c\u5168\u200b\u6743\u9650\u200b\u8fd0\u884c\u200b</li> <li>\u200b\u65e0\u200b\u6c99\u7bb1\u200b\uff08\u200b\u8bbe\u8ba1\u200b\u5982\u6b64\u200b\uff09</li> <li>\u200b\u4f7f\u7528\u200b\u9002\u5f53\u200b\u7684\u200b\u8bbf\u95ee\u63a7\u5236\u200b</li> </ul>"},{"location":"zh/design/debugging/#_7","title":"\u5806\u6808\u200b\u5206\u6790","text":""},{"location":"zh/design/debugging/#_8","title":"\u5806\u6808\u200b\u8ddf\u8e2a\u200b\u6355\u83b7","text":"<p>\u200b\u6355\u83b7\u200b\u5f53\u524d\u200b\u6267\u884c\u200b\u5806\u6808\u200b\uff1a</p> <pre><code>probing $ENDPOINT backtrace\n</code></pre>"},{"location":"zh/design/debugging/#_9","title":"\u5e27\u200b\u4fe1\u606f","text":"<p>\u200b\u6bcf\u4e2a\u200b\u5806\u6808\u200b\u5e27\u200b\u5305\u62ec\u200b\uff1a</p> \u200b\u5b57\u200b\u6bb5\u200b \u200b\u63cf\u8ff0\u200b func \u200b\u51fd\u6570\u200b\u540d\u200b file \u200b\u6e90\u6587\u4ef6\u200b\u8def\u5f84\u200b lineno \u200b\u884c\u53f7\u200b depth \u200b\u5806\u6808\u200b\u6df1\u5ea6\u200b\uff080 = \u200b\u6700\u200b\u5185\u5c42\u200b\uff09 frame_type Python \u200b\u6216\u200b Native locals \u200b\u5c40\u90e8\u53d8\u91cf\u200b\uff08\u200b\u53ef\u200b\u9009\u200b\uff09"},{"location":"zh/design/debugging/#_10","title":"\u67e5\u8be2\u200b\u5806\u6808","text":"<pre><code>SELECT func, file, lineno, depth\nFROM python.backtrace\nORDER BY depth;\n</code></pre>"},{"location":"zh/design/debugging/#repl","title":"\u4ea4\u4e92\u5f0f\u200b REPL","text":""},{"location":"zh/design/debugging/#repl_1","title":"\u542f\u52a8\u200b REPL","text":"<pre><code>probing -t &lt;pid&gt; repl\n</code></pre>"},{"location":"zh/design/debugging/#repl_2","title":"REPL \u200b\u529f\u80fd","text":"<ul> <li>Tab \u200b\u8865\u5168\u200b</li> <li>\u200b\u591a\u884c\u200b\u8f93\u5165\u200b</li> <li>\u200b\u5386\u53f2\u8bb0\u5f55\u200b\u652f\u6301\u200b</li> <li>\u200b\u5f02\u5e38\u200b\u663e\u793a\u200b</li> </ul>"},{"location":"zh/design/debugging/#_11","title":"\u793a\u4f8b\u200b\u4f1a\u8bdd","text":"<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; model = get_model()\n&gt;&gt;&gt; model.training\nTrue\n&gt;&gt;&gt; torch.cuda.memory_allocated() / 1024**3\n2.5\n</code></pre>"},{"location":"zh/design/debugging/#_12","title":"\u8fdc\u7a0b\u200b\u8c03\u8bd5","text":""},{"location":"zh/design/debugging/#_13","title":"\u8bbe\u7f6e","text":"<pre><code># \u200b\u5728\u200b\u76ee\u6807\u200b\u673a\u5668\u200b\u4e0a\u200b\nPROBING_PORT=8080 python train.py\n\n# \u200b\u4ece\u200b\u8fdc\u7a0b\u200b\u673a\u5668\u200b\nprobing -t host:8080 eval \"...\"\n</code></pre>"},{"location":"zh/design/debugging/#_14","title":"\u5b89\u5168","text":"<ul> <li>\u200b\u901a\u8fc7\u200b\u4ee4\u724c\u200b\u8ba4\u8bc1\u200b</li> <li>\u200b\u652f\u6301\u200b TLS \u200b\u52a0\u5bc6\u200b</li> <li>\u200b\u57fa\u4e8e\u200b IP \u200b\u7684\u200b\u8bbf\u95ee\u63a7\u5236\u200b</li> </ul>"},{"location":"zh/design/debugging/#_15","title":"\u8c03\u8bd5\u6a21\u5f0f","text":""},{"location":"zh/design/debugging/#_16","title":"\u67e5\u627e\u200b\u5361\u4f4f\u200b\u4f4d\u7f6e","text":"<pre><code># \u200b\u6355\u83b7\u200b\u5806\u6808\u200b\nprobing $ENDPOINT backtrace\n\n# \u200b\u68c0\u67e5\u200b\u6267\u884c\u200b\u963b\u585e\u200b\u5728\u200b\u54ea\u91cc\u200b\nprobing $ENDPOINT query \"\nSELECT func, file, lineno\nFROM python.backtrace\nWHERE depth &lt; 5\"\n</code></pre>"},{"location":"zh/design/debugging/#_17","title":"\u68c0\u67e5\u200b\u72b6\u6001","text":"<pre><code># \u200b\u68c0\u67e5\u200b\u6a21\u578b\u200b\u72b6\u6001\u200b\nprobing $ENDPOINT eval \"\nfor name, param in model.named_parameters():\n    print(f'{name}: {param.shape}')\"\n</code></pre>"},{"location":"zh/design/debugging/#_18","title":"\u4fee\u6539\u200b\u884c\u4e3a","text":"<pre><code># \u200b\u4fee\u6539\u200b\u5b66\u4e60\u200b\u7387\u200b\nprobing $ENDPOINT eval \"\noptimizer.param_groups[0]['lr'] = 0.0001\"\n\n# \u200b\u5f3a\u5236\u200b\u4fdd\u5b58\u200b\u68c0\u67e5\u70b9\u200b\nprobing $ENDPOINT eval \"\ntrainer.save_checkpoint('debug_checkpoint.pt')\"\n</code></pre>"},{"location":"zh/design/debugging/#_19","title":"\u7ebf\u7a0b\u200b\u8c03\u8bd5","text":""},{"location":"zh/design/debugging/#_20","title":"\u5217\u51fa\u200b\u7ebf\u7a0b","text":"<pre><code>probing $ENDPOINT eval \"\nimport threading\nfor t in threading.enumerate():\n    print(f'{t.name}: alive={t.is_alive()}')\"\n</code></pre>"},{"location":"zh/design/debugging/#_21","title":"\u4e3b\u7ebf\u200b\u7a0b\u200b\u7126\u70b9","text":"<p>\u200b\u9ed8\u8ba4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0cbacktrace \u200b\u6355\u83b7\u200b\u4e3b\u7ebf\u200b\u7a0b\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u5176\u4ed6\u200b\u7ebf\u7a0b\u200b\uff1a</p> <pre><code>probing $ENDPOINT eval \"\nimport sys, traceback\nfor thread_id, frame in sys._current_frames().items():\n    print(f'\u200b\u7ebf\u7a0b\u200b {thread_id}:')\n    traceback.print_stack(frame)\"\n</code></pre>"},{"location":"zh/design/distributed/","title":"\u5206\u5e03\u5f0f\u200b\u67b6\u6784","text":"<p>Probing \u200b\u652f\u6301\u200b\u76d1\u63a7\u200b\u548c\u200b\u8c03\u8bd5\u200b\u8de8\u591a\u200b\u8282\u70b9\u200b\u7684\u200b\u5206\u5e03\u5f0f\u200b AI \u200b\u5de5\u4f5c\u200b\u8d1f\u8f7d\u200b\u3002</p>"},{"location":"zh/design/distributed/#_2","title":"\u6982\u89c8","text":"<p>\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u5e26\u6765\u200b\u4e86\u200b\u6311\u6218\u200b\uff1a</p> <ul> <li>\u200b\u8de8\u200b\u8282\u70b9\u200b\u7684\u200b\u591a\u200b\u8fdb\u7a0b\u200b</li> <li>Rank \u200b\u4e4b\u95f4\u200b\u7684\u200b\u901a\u4fe1\u200b</li> <li>\u200b\u540c\u6b65\u200b\u8c03\u8bd5\u200b\u9700\u6c42\u200b</li> <li>\u200b\u8de8\u200b\u8282\u70b9\u200b\u6570\u636e\u200b\u5173\u8054\u200b</li> </ul> <p>Probing \u200b\u901a\u8fc7\u200b\u5206\u5e03\u5f0f\u200b\u67b6\u6784\u200b\u89e3\u51b3\u200b\u8fd9\u4e9b\u200b\u95ee\u9898\u200b\u3002</p>"},{"location":"zh/design/distributed/#_3","title":"\u67b6\u6784","text":"<pre><code>graph TB\n    subgraph \"\u200b\u8282\u70b9\u200b 1\"\n        P1[\u200b\u8fdb\u7a0b\u200b Rank 0]\n        PROBE1[\u200b\u63a2\u9488\u200b]\n    end\n\n    subgraph \"\u200b\u8282\u70b9\u200b 2\"\n        P2[\u200b\u8fdb\u7a0b\u200b Rank 1]\n        PROBE2[\u200b\u63a2\u9488\u200b]\n    end\n\n    subgraph \"\u200b\u63a7\u5236\u200b\u5e73\u9762\u200b\"\n        CLI[CLI \u200b\u5ba2\u6237\u7aef\u200b]\n        AGG[\u200b\u805a\u5408\u200b\u5668\u200b]\n    end\n\n    PROBE1 --&gt; AGG\n    PROBE2 --&gt; AGG\n    CLI --&gt; AGG\n    CLI --&gt; PROBE1\n    CLI --&gt; PROBE2</code></pre>"},{"location":"zh/design/distributed/#_4","title":"\u8fdb\u7a0b\u200b\u53d1\u73b0","text":""},{"location":"zh/design/distributed/#_5","title":"\u672c\u5730\u200b\u53d1\u73b0","text":"<pre><code># \u200b\u5217\u51fa\u200b\u672c\u5730\u200b\u673a\u5668\u200b\u4e0a\u200b\u6240\u6709\u200b\u542f\u7528\u200b probing \u200b\u7684\u200b\u8fdb\u7a0b\u200b\nprobing list\n</code></pre>"},{"location":"zh/design/distributed/#_6","title":"\u8fdc\u7a0b\u200b\u53d1\u73b0","text":"<pre><code># \u200b\u8fde\u63a5\u200b\u5230\u200b\u8fdc\u7a0b\u200b\u8282\u70b9\u200b\nprobing -t node1:8080 list\nprobing -t node2:8080 list\n</code></pre>"},{"location":"zh/design/distributed/#_7","title":"\u96c6\u7fa4\u200b\u89c6\u56fe","text":"<pre><code># \u200b\u8de8\u200b\u96c6\u7fa4\u200b\u7684\u200b\u805a\u5408\u200b\u89c6\u56fe\u200b\nprobing cluster list\n</code></pre>"},{"location":"zh/design/distributed/#_8","title":"\u8de8\u200b\u8282\u70b9\u200b\u67e5\u8be2","text":""},{"location":"zh/design/distributed/#_9","title":"\u67e5\u8be2\u200b\u5355\u4e2a\u200b\u8282\u70b9","text":"<pre><code>probing -t node1:8080 query \"\nSELECT * FROM python.torch_trace\nWHERE step = (SELECT MAX(step) FROM python.torch_trace)\"\n</code></pre>"},{"location":"zh/design/distributed/#_10","title":"\u67e5\u8be2\u200b\u6240\u6709\u200b\u8282\u70b9","text":"<pre><code># \u200b\u8054\u5408\u200b\u67e5\u8be2\u200b\uff08\u200b\u672a\u6765\u200b\u529f\u80fd\u200b\uff09\nprobing cluster query \"\nSELECT node, rank, step, loss\nFROM python.training_metrics\nORDER BY step\"\n</code></pre>"},{"location":"zh/design/distributed/#_11","title":"\u540c\u6b65\u200b\u8c03\u8bd5","text":""},{"location":"zh/design/distributed/#_12","title":"\u6355\u83b7\u200b\u6240\u6709\u200b\u5806\u6808","text":"<pre><code># \u200b\u4ece\u200b\u6240\u6709\u200b rank \u200b\u6355\u83b7\u200b\u5806\u6808\u200b\u8ddf\u8e2a\u200b\nfor node in node1 node2 node3; do\n    echo \"=== $node ===\"\n    probing -t $node:8080 backtrace\ndone\n</code></pre>"},{"location":"zh/design/distributed/#_13","title":"\u68c0\u67e5\u200b\u5206\u5e03\u5f0f\u200b\u72b6\u6001","text":"<pre><code>probing -t $ENDPOINT eval \"\nimport torch.distributed as dist\n\nif dist.is_initialized():\n    print(f'Rank: {dist.get_rank()}')\n    print(f'World Size: {dist.get_world_size()}')\n    print(f'Backend: {dist.get_backend()}')\"\n</code></pre>"},{"location":"zh/design/distributed/#_14","title":"\u901a\u4fe1\u200b\u5206\u6790","text":""},{"location":"zh/design/distributed/#nccl","title":"NCCL \u200b\u64cd\u4f5c","text":"<pre><code>-- \u200b\u5206\u6790\u200b\u901a\u4fe1\u200b\u6a21\u5f0f\u200b\nSELECT\n    operation,\n    src_rank,\n    dst_rank,\n    bytes_transferred,\n    duration_ms\nFROM python.nccl_trace\nWHERE step = (SELECT MAX(step) FROM python.nccl_trace)\nORDER BY duration_ms DESC;\n</code></pre>"},{"location":"zh/design/distributed/#rdma","title":"RDMA \u200b\u6d41\u200b\u5206\u6790","text":"<pre><code># RDMA \u200b\u7279\u5b9a\u200b\u5206\u6790\u200b\nprobing -t $ENDPOINT rdma\n</code></pre>"},{"location":"zh/design/distributed/#_15","title":"\u5206\u5e03\u5f0f\u200b\u95ee\u9898\u200b\u6392\u67e5","text":""},{"location":"zh/design/distributed/#rank","title":"Rank \u200b\u540c\u6b65","text":"<pre><code># \u200b\u68c0\u67e5\u200b\u6240\u6709\u200b rank \u200b\u662f\u5426\u200b\u5728\u200b\u76f8\u540c\u200b\u6b65\u9aa4\u200b\nfor node in node1 node2 node3; do\n    probing -t $node:8080 eval \"print(f'Step: {trainer.current_step}')\"\ndone\n</code></pre>"},{"location":"zh/design/distributed/#_16","title":"\u6b7b\u9501\u200b\u68c0\u6d4b","text":"<pre><code># \u200b\u68c0\u67e5\u200b\u6302\u200b\u8d77\u200b\u7684\u200b\u96c6\u5408\u200b\u64cd\u4f5c\u200b\nprobing -t $ENDPOINT query \"\nSELECT func, file, lineno\nFROM python.backtrace\nWHERE func LIKE '%collective%' OR func LIKE '%allreduce%'\"\n</code></pre>"},{"location":"zh/design/distributed/#_17","title":"\u5185\u5b58\u200b\u4e0d\u200b\u5747\u8861","text":"<pre><code>-- \u200b\u6bd4\u8f83\u200b\u5404\u200b rank \u200b\u7684\u200b\u5185\u5b58\u200b\nSELECT\n    rank,\n    AVG(allocated) as avg_memory,\n    MAX(allocated) as peak_memory\nFROM python.torch_trace\nGROUP BY rank;\n</code></pre>"},{"location":"zh/design/distributed/#_18","title":"\u914d\u7f6e","text":""},{"location":"zh/design/distributed/#_19","title":"\u542f\u7528\u200b\u8fdc\u7a0b\u200b\u8bbf\u95ee","text":"<pre><code># \u200b\u4ee5\u200b TCP \u200b\u670d\u52a1\u5668\u200b\u542f\u52a8\u200b\nPROBING_PORT=8080 python train.py\n\n# \u200b\u6216\u200b\u52a8\u6001\u200b\u914d\u7f6e\u200b\nprobing $ENDPOINT config probing.server.port=8080\n</code></pre>"},{"location":"zh/design/distributed/#_20","title":"\u5b89\u5168","text":"<pre><code># \u200b\u542f\u7528\u200b\u8ba4\u8bc1\u200b\nPROBING_AUTH_TOKEN=secret python train.py\n\n# \u200b\u5e26\u200b\u4ee4\u724c\u200b\u8fde\u63a5\u200b\nprobing -t host:8080 --token secret query \"...\"\n</code></pre>"},{"location":"zh/design/distributed/#_21","title":"\u6700\u4f73\u200b\u5b9e\u8df5","text":""},{"location":"zh/design/distributed/#1","title":"1. \u200b\u4e00\u81f4\u200b\u7684\u200b\u914d\u7f6e","text":"<p>\u200b\u5728\u200b\u6240\u6709\u200b\u8282\u70b9\u200b\u4e0a\u200b\u4f7f\u7528\u200b\u76f8\u540c\u200b\u914d\u7f6e\u200b\uff1a</p> <pre><code>export PROBING_PORT=8080\nexport PROBING_TORCH_PROFILING=on\n</code></pre>"},{"location":"zh/design/distributed/#2","title":"2. \u200b\u96c6\u4e2d\u200b\u6536\u96c6","text":"<p>\u200b\u5bf9\u4e8e\u200b\u5927\u578b\u200b\u96c6\u7fa4\u200b\uff0c\u200b\u8003\u8651\u200b\u805a\u5408\u200b\uff1a</p> <pre><code># \u200b\u5bfc\u51fa\u200b\u6570\u636e\u200b\u5230\u200b\u4e2d\u5fc3\u200b\u4f4d\u7f6e\u200b\nprobing -t $node query \"SELECT * FROM python.torch_trace\" &gt;&gt; /shared/traces.json\n</code></pre>"},{"location":"zh/design/distributed/#3","title":"3. \u200b\u65f6\u95f4\u200b\u6233\u200b\u540c\u6b65","text":"<p>\u200b\u786e\u4fdd\u200b\u914d\u7f6e\u200b NTP \u200b\u4ee5\u200b\u83b7\u5f97\u200b\u51c6\u786e\u200b\u7684\u200b\u8de8\u200b\u8282\u70b9\u200b\u65f6\u95f4\u200b\u3002</p>"},{"location":"zh/design/distributed/#4","title":"4. \u200b\u7f51\u7edc\u200b\u8003\u8651","text":"<ul> <li>\u200b\u5c3d\u53ef\u80fd\u200b\u4f7f\u7528\u200b\u4e13\u7528\u200b\u7f51\u7edc\u200b\u8fdb\u884c\u200b probing \u200b\u6d41\u91cf\u200b</li> <li>\u200b\u8003\u8651\u200b probing \u200b\u7aef\u53e3\u200b\u7684\u200b\u9632\u706b\u5899\u200b\u89c4\u5219\u200b</li> <li>\u200b\u76d1\u63a7\u200b probing \u200b\u5bf9\u200b\u8bad\u7ec3\u200b\u7f51\u7edc\u200b\u7684\u200b\u5f00\u9500\u200b</li> </ul>"},{"location":"zh/design/extensibility/","title":"\u6269\u5c55\u200b\u673a\u5236","text":"<p>Probing \u200b\u63d0\u4f9b\u200b\u673a\u5236\u200b\u6765\u200b\u6269\u5c55\u200b\u5176\u200b\u529f\u80fd\u200b\uff0c\u200b\u652f\u6301\u200b\u81ea\u5b9a\u4e49\u200b\u6570\u636e\u6e90\u200b\u548c\u200b\u6307\u6807\u200b\u3002</p>"},{"location":"zh/design/extensibility/#_2","title":"\u6982\u89c8","text":"<p>\u200b\u6269\u5c55\u200b\u7cfb\u7edf\u200b\u5141\u8bb8\u200b\uff1a</p> <ul> <li>\u200b\u81ea\u5b9a\u4e49\u200b\u6570\u636e\u8868\u200b</li> <li>\u200b\u7528\u6237\u200b\u5b9a\u4e49\u200b\u7684\u200b\u6307\u6807\u200b</li> <li>\u200b\u4e0e\u200b\u5916\u90e8\u200b\u5de5\u5177\u200b\u96c6\u6210\u200b</li> <li>\u200b\u63d2\u4ef6\u200b\u67b6\u6784\u200b</li> </ul>"},{"location":"zh/design/extensibility/#_3","title":"\u81ea\u5b9a\u4e49\u200b\u8868","text":""},{"location":"zh/design/extensibility/#python-api","title":"Python API","text":"<p>\u200b\u4f7f\u7528\u200b <code>@table</code> \u200b\u88c5\u9970\u200b\u5668\u200b\u521b\u5efa\u200b\u81ea\u5b9a\u4e49\u200b\u8868\u200b\uff1a</p> <pre><code>from probing import table\n\n@table(\"my_metrics\")\ndef get_metrics():\n    \"\"\"\u200b\u8fd4\u56de\u200b\u5b57\u5178\u200b\u5217\u8868\u200b\u6216\u200b pandas DataFrame\u3002\"\"\"\n    return [\n        {\"name\": \"loss\", \"value\": current_loss},\n        {\"name\": \"accuracy\", \"value\": current_acc},\n    ]\n</code></pre>"},{"location":"zh/design/extensibility/#_4","title":"\u67e5\u8be2\u200b\u81ea\u5b9a\u4e49\u200b\u8868","text":"<pre><code>SELECT * FROM python.my_metrics;\n</code></pre>"},{"location":"zh/design/extensibility/#schema","title":"\u8868\u200b Schema","text":"<p>\u200b\u8868\u200b\u6839\u636e\u200b\u8fd4\u56de\u200b\u6570\u636e\u200b\u52a8\u6001\u200b\u7c7b\u578b\u5316\u200b\uff1a</p> <pre><code>@table(\"training_state\")\ndef get_training_state():\n    return {\n        \"step\": trainer.current_step,      # int\n        \"loss\": trainer.last_loss,         # float\n        \"lr\": optimizer.param_groups[0][\"lr\"],  # float\n        \"epoch\": trainer.current_epoch,    # int\n    }\n</code></pre>"},{"location":"zh/design/extensibility/#_5","title":"\u5916\u90e8\u200b\u8868\u200b\u96c6\u6210","text":""},{"location":"zh/design/extensibility/#arrow","title":"Arrow \u200b\u8868","text":"<pre><code>import pyarrow as pa\nfrom probing import register_table\n\ntable = pa.table({\n    \"id\": [1, 2, 3],\n    \"value\": [10.0, 20.0, 30.0]\n})\n\nregister_table(\"arrow_data\", table)\n</code></pre>"},{"location":"zh/design/extensibility/#_6","title":"\u81ea\u5b9a\u4e49\u200b\u6307\u6807","text":""},{"location":"zh/design/extensibility/#_7","title":"\u5b9a\u4e49\u200b\u6307\u6807","text":"<pre><code>from probing import metric\n\n@metric(\"gpu_utilization\")\ndef gpu_util():\n    \"\"\"\u200b\u8fd4\u56de\u200b\u5f53\u524d\u200b GPU \u200b\u5229\u7528\u7387\u200b\u3002\"\"\"\n    import pynvml\n    pynvml.nvmlInit()\n    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n    util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n    return util.gpu\n</code></pre>"},{"location":"zh/design/extensibility/#_8","title":"\u67e5\u8be2\u200b\u6307\u6807","text":"<pre><code>SELECT * FROM python.metrics WHERE name = 'gpu_utilization';\n</code></pre>"},{"location":"zh/design/extensibility/#_9","title":"\u94a9\u5b50\u200b\u7cfb\u7edf","text":""},{"location":"zh/design/extensibility/#_10","title":"\u6a21\u5757\u200b\u94a9\u5b50","text":"<pre><code>from probing import register_hook\n\n@register_hook(\"torch.nn.Linear\", \"forward\")\ndef linear_hook(module, input, output):\n    \"\"\"\u200b\u6bcf\u6b21\u200b Linear \u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\u65f6\u200b\u8c03\u7528\u200b\u3002\"\"\"\n    record_custom_data({\n        \"module\": str(module),\n        \"input_shape\": list(input[0].shape),\n        \"output_shape\": list(output.shape),\n    })\n</code></pre>"},{"location":"zh/design/extensibility/#_11","title":"\u51fd\u6570\u200b\u94a9\u5b50","text":"<pre><code>from probing import hook_function\n\n@hook_function(\"torch.optim.Adam.step\")\ndef optimizer_hook(optimizer):\n    \"\"\"\u200b\u6bcf\u6b21\u200b\u4f18\u5316\u200b\u5668\u200b\u6b65\u9aa4\u200b\u65f6\u200b\u8c03\u7528\u200b\u3002\"\"\"\n    record_custom_data({\n        \"lr\": optimizer.param_groups[0][\"lr\"],\n        \"step_count\": optimizer.state_dict()[\"state\"][0][\"step\"],\n    })\n</code></pre>"},{"location":"zh/design/extensibility/#_12","title":"\u63d2\u4ef6\u200b\u67b6\u6784","text":""},{"location":"zh/design/extensibility/#_13","title":"\u521b\u5efa\u200b\u63d2\u4ef6","text":"<pre><code># my_plugin.py\nfrom probing import Plugin\n\nclass MyPlugin(Plugin):\n    name = \"my_plugin\"\n\n    def on_load(self):\n        \"\"\"\u200b\u63d2\u4ef6\u200b\u52a0\u8f7d\u200b\u65f6\u200b\u8c03\u7528\u200b\u3002\"\"\"\n        self.register_table(\"plugin_data\", self.get_data)\n\n    def on_unload(self):\n        \"\"\"\u200b\u63d2\u4ef6\u200b\u5378\u8f7d\u200b\u65f6\u200b\u8c03\u7528\u200b\u3002\"\"\"\n        pass\n\n    def get_data(self):\n        return [{\"key\": \"value\"}]\n</code></pre>"},{"location":"zh/design/extensibility/#_14","title":"\u52a0\u8f7d\u200b\u63d2\u4ef6","text":"<pre><code># \u200b\u73af\u5883\u53d8\u91cf\u200b\nPROBING_PLUGINS=my_plugin python train.py\n\n# \u200b\u6216\u200b\u7f16\u7a0b\u200b\u65b9\u5f0f\u200b\nimport probing\nprobing.load_plugin(\"my_plugin\")\n</code></pre>"},{"location":"zh/design/extensibility/#_15","title":"\u914d\u7f6e\u200b\u6269\u5c55","text":""},{"location":"zh/design/extensibility/#_16","title":"\u81ea\u5b9a\u4e49\u200b\u914d\u7f6e\u200b\u9009\u9879","text":"<pre><code>from probing import config\n\n# \u200b\u6ce8\u518c\u200b\u81ea\u5b9a\u4e49\u200b\u914d\u7f6e\u200b\nconfig.register(\"my_plugin.sample_rate\", default=0.1, type=float)\n\n# \u200b\u5728\u200b\u63d2\u4ef6\u200b\u4e2d\u200b\u4f7f\u7528\u200b\nrate = config.get(\"my_plugin.sample_rate\")\n</code></pre>"},{"location":"zh/design/extensibility/#_17","title":"\u67e5\u8be2\u200b\u914d\u7f6e","text":"<pre><code>SELECT * FROM information_schema.df_settings\nWHERE name LIKE 'my_plugin.%';\n</code></pre>"},{"location":"zh/design/extensibility/#_18","title":"\u96c6\u6210\u200b\u793a\u4f8b","text":""},{"location":"zh/design/extensibility/#tensorboard","title":"TensorBoard","text":"<pre><code>from probing import table\nfrom torch.utils.tensorboard import SummaryWriter\n\nwriter = SummaryWriter()\n\n@table(\"tensorboard_scalars\")\ndef get_tb_scalars():\n    # \u200b\u8bbf\u95ee\u200b TensorBoard \u200b\u6570\u636e\u200b\n    return logged_scalars\n</code></pre>"},{"location":"zh/design/extensibility/#prometheus","title":"Prometheus","text":"<pre><code>from probing import metric\nfrom prometheus_client import Gauge\n\ngpu_memory = Gauge(\"gpu_memory_bytes\", \"GPU \u200b\u5185\u5b58\u200b\u4f7f\u7528\u200b\")\n\n@metric(\"prometheus_gpu_memory\")\ndef update_prometheus():\n    mem = torch.cuda.memory_allocated()\n    gpu_memory.set(mem)\n    return mem\n</code></pre>"},{"location":"zh/design/extensibility/#_19","title":"\u6700\u4f73\u200b\u5b9e\u8df5","text":""},{"location":"zh/design/extensibility/#1","title":"1. \u200b\u8f7b\u91cf\u7ea7\u200b\u6570\u636e\u200b\u6536\u96c6","text":"<pre><code># \u200b\u597d\u200b\uff1a\u200b\u53ea\u200b\u8fd4\u56de\u200b\u5fc5\u8981\u200b\u6570\u636e\u200b\n@table(\"efficient\")\ndef get_efficient():\n    return {\"step\": step, \"loss\": loss}\n\n# \u200b\u907f\u514d\u200b\uff1a\u200b\u6602\u8d35\u200b\u7684\u200b\u64cd\u4f5c\u200b\n@table(\"expensive\")\ndef get_expensive():\n    return serialize_entire_model()  # \u200b\u592a\u91cd\u200b\n</code></pre>"},{"location":"zh/design/extensibility/#2","title":"2. \u200b\u9519\u8bef\u5904\u7406","text":"<pre><code>@table(\"safe_data\")\ndef get_safe_data():\n    try:\n        return {\"value\": compute_value()}\n    except Exception as e:\n        return {\"error\": str(e)}\n</code></pre>"},{"location":"zh/design/extensibility/#3","title":"3. \u200b\u7f13\u5b58","text":"<pre><code>from functools import lru_cache\n\n@table(\"cached_data\")\n@lru_cache(maxsize=1)\ndef get_cached_data():\n    return expensive_computation()\n</code></pre>"},{"location":"zh/design/profiling/","title":"\u6027\u80fd\u200b\u5206\u6790\u200b\u5b9e\u73b0","text":"<p>Probing \u200b\u4e3a\u200b AI \u200b\u5de5\u4f5c\u200b\u8d1f\u8f7d\u200b\u63d0\u4f9b\u200b\u5168\u9762\u200b\u7684\u200b\u6027\u80fd\u200b\u5206\u6790\u200b\u80fd\u529b\u200b\u3002</p>"},{"location":"zh/design/profiling/#_2","title":"\u6982\u89c8","text":"<p>\u200b\u6027\u80fd\u200b\u5206\u6790\u200b\u7cfb\u7edf\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u65b9\u5f0f\u200b\u4ee5\u200b\u6700\u5c0f\u200b\u5f00\u9500\u200b\u6536\u96c6\u200b\u6027\u80fd\u200b\u6570\u636e\u200b\uff1a</p> <ul> <li>\u200b\u57fa\u4e8e\u200b\u4e8b\u4ef6\u200b\u7684\u200b\u6536\u96c6\u200b</li> <li>\u200b\u9ad8\u6548\u200b\u7684\u200b\u91c7\u6837\u200b\u7b56\u7565\u200b</li> <li>\u200b\u5217\u5f0f\u200b\u6570\u636e\u200b\u5b58\u50a8\u200b</li> <li>SQL \u200b\u67e5\u8be2\u200b\u63a5\u53e3\u200b</li> </ul>"},{"location":"zh/design/profiling/#_3","title":"\u6570\u636e\u200b\u6536\u96c6\u200b\u67b6\u6784","text":"<pre><code>graph TB\n    subgraph \"\u200b\u6570\u636e\u6e90\u200b\"\n        TORCH[PyTorch \u200b\u94a9\u5b50\u200b]\n        PYTHON[Python \u200b\u5e27\u200b]\n        SYSTEM[\u200b\u7cfb\u7edf\u200b\u6307\u6807\u200b]\n    end\n\n    subgraph \"\u200b\u6536\u96c6\u200b\u5c42\u200b\"\n        BUFFER[\u200b\u73af\u5f62\u200b\u7f13\u51b2\u533a\u200b]\n        SAMPLER[\u200b\u91c7\u6837\u5668\u200b]\n    end\n\n    subgraph \"\u200b\u5b58\u50a8\u200b\u5c42\u200b\"\n        ARROW[Arrow \u200b\u8868\u200b]\n        QUERY[\u200b\u67e5\u8be2\u200b\u5f15\u64ce\u200b]\n    end\n\n    TORCH --&gt; SAMPLER\n    PYTHON --&gt; SAMPLER\n    SYSTEM --&gt; SAMPLER\n\n    SAMPLER --&gt; BUFFER\n    BUFFER --&gt; ARROW\n    ARROW --&gt; QUERY</code></pre>"},{"location":"zh/design/profiling/#pytorch","title":"PyTorch \u200b\u5206\u6790","text":""},{"location":"zh/design/profiling/#_4","title":"\u94a9\u5b50\u200b\u96c6\u6210","text":"<p>Probing \u200b\u4e0e\u200b PyTorch \u200b\u7684\u200b\u6a21\u5757\u200b\u94a9\u5b50\u200b\u96c6\u6210\u200b\uff1a</p> <pre><code># \u200b\u524d\u5411\u200b\u94a9\u5b50\u200b\ndef forward_hook(module, input, output):\n    record_trace(module, \"forward\", memory_stats())\n\n# \u200b\u53cd\u5411\u200b\u94a9\u5b50\u200b\ndef backward_hook(module, grad_input, grad_output):\n    record_trace(module, \"backward\", memory_stats())\n</code></pre>"},{"location":"zh/design/profiling/#_5","title":"\u6536\u96c6\u200b\u7684\u200b\u6570\u636e","text":"\u5b57\u200b\u6bb5\u200b \u200b\u7c7b\u578b\u200b \u200b\u63cf\u8ff0\u200b step int \u200b\u8bad\u7ec3\u200b\u6b65\u6570\u200b seq int \u200b\u6b65\u5185\u200b\u5e8f\u53f7\u200b module string \u200b\u6a21\u5757\u200b\u540d\u200b stage string forward/backward/step allocated float GPU \u200b\u5df2\u200b\u5206\u914d\u5185\u5b58\u200b (MB) max_allocated float GPU \u200b\u5cf0\u503c\u200b\u5185\u5b58\u200b (MB) cached float GPU \u200b\u7f13\u5b58\u200b\u5185\u5b58\u200b (MB) duration float \u200b\u6267\u884c\u200b\u65f6\u95f4\u200b\uff08\u200b\u79d2\u200b\uff09"},{"location":"zh/design/profiling/#pytorch_1","title":"\u542f\u7528\u200b PyTorch \u200b\u5206\u6790","text":"<pre><code># \u200b\u73af\u5883\u53d8\u91cf\u200b\nPROBING_TORCH_PROFILING=on python train.py\n\n# \u200b\u6216\u200b\u7f16\u7a0b\u200b\u65b9\u5f0f\u200b\nimport probing\nprobing.enable_torch_profiling()\n</code></pre>"},{"location":"zh/design/profiling/#python","title":"Python \u200b\u5806\u6808\u200b\u5206\u6790","text":""},{"location":"zh/design/profiling/#_6","title":"\u5806\u6808\u200b\u8ddf\u8e2a\u200b\u6536\u96c6","text":"<p>\u200b\u6355\u83b7\u200b Python \u200b\u8c03\u7528\u200b\u6808\u200b\u5305\u62ec\u200b\uff1a</p> <ul> <li>\u200b\u51fd\u6570\u200b\u540d\u200b</li> <li>\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b</li> <li>\u200b\u884c\u53f7\u200b</li> <li>\u200b\u5c40\u90e8\u53d8\u91cf\u200b\uff08\u200b\u53ef\u200b\u9009\u200b\uff09</li> </ul> <pre><code># \u200b\u5806\u6808\u200b\u5e27\u200b\u6570\u636e\u200b\n{\n    \"func\": \"forward\",\n    \"file\": \"/app/model.py\",\n    \"lineno\": 123,\n    \"depth\": 5,\n    \"frame_type\": \"Python\"\n}\n</code></pre>"},{"location":"zh/design/profiling/#_7","title":"\u91c7\u6837\u200b\u7b56\u7565","text":"<ul> <li>\u200b\u5468\u671f\u200b\u91c7\u6837\u200b: \u200b\u53ef\u200b\u914d\u7f6e\u200b\u95f4\u9694\u200b\uff08\u200b\u9ed8\u8ba4\u200b\uff1a100ms\uff09</li> <li>\u200b\u4e8b\u4ef6\u200b\u89e6\u53d1\u200b: \u200b\u7279\u5b9a\u200b\u64cd\u4f5c\u200b\u65f6\u200b</li> <li>\u200b\u6309\u200b\u9700\u200b: \u200b\u901a\u8fc7\u200b backtrace \u200b\u547d\u4ee4\u200b</li> </ul>"},{"location":"zh/design/profiling/#_8","title":"\u7cfb\u7edf\u200b\u6307\u6807","text":""},{"location":"zh/design/profiling/#_9","title":"\u6536\u96c6\u200b\u7684\u200b\u6307\u6807","text":"<ul> <li>CPU \u200b\u5229\u7528\u7387\u200b</li> <li>\u200b\u5185\u5b58\u200b\u4f7f\u7528\u200b\uff08RSS\u3001VMS\uff09</li> <li>GPU \u200b\u5229\u7528\u7387\u200b</li> <li>GPU \u200b\u5185\u5b58\u200b</li> <li>I/O \u200b\u7edf\u8ba1\u200b</li> <li>\u200b\u7f51\u7edc\u200b\u7edf\u8ba1\u200b</li> </ul>"},{"location":"zh/design/profiling/#_10","title":"\u6536\u96c6\u200b\u95f4\u9694","text":"<pre><code># \u200b\u914d\u7f6e\u200b\u91c7\u6837\u200b\u95f4\u9694\u200b\nprobing $ENDPOINT config probing.sample_rate=0.1  # 100ms\n</code></pre>"},{"location":"zh/design/profiling/#_11","title":"\u6570\u636e\u200b\u5b58\u50a8","text":""},{"location":"zh/design/profiling/#_12","title":"\u73af\u5f62\u200b\u7f13\u51b2\u533a","text":"<p>\u200b\u9ad8\u6548\u200b\u7684\u200b\u56fa\u5b9a\u200b\u5927\u5c0f\u200b\u7f13\u51b2\u533a\u200b\u7528\u4e8e\u200b\u5b58\u50a8\u200b\u6700\u65b0\u200b\u6570\u636e\u200b\uff1a</p> <ul> <li>\u200b\u53ef\u200b\u914d\u7f6e\u200b\u5927\u5c0f\u200b\uff08\u200b\u9ed8\u8ba4\u200b\uff1a10000 \u200b\u6761\u200b\u8bb0\u5f55\u200b\uff09</li> <li>\u200b\u81ea\u52a8\u200b\u6dd8\u6c70\u200b\u65e7\u200b\u6570\u636e\u200b</li> <li>\u200b\u65e0\u9501\u200b\u5e76\u53d1\u200b\u8bbf\u95ee\u200b</li> </ul>"},{"location":"zh/design/profiling/#arrow","title":"Arrow \u200b\u8868","text":"<p>\u200b\u7528\u4e8e\u200b\u9ad8\u6548\u200b\u67e5\u8be2\u200b\u7684\u200b\u5217\u5f0f\u200b\u683c\u5f0f\u200b\uff1a</p> <ul> <li>\u200b\u5411\u200b\u91cf\u5316\u200b\u64cd\u4f5c\u200b</li> <li>\u200b\u5185\u5b58\u200b\u6620\u5c04\u200b I/O</li> <li>\u200b\u538b\u7f29\u200b\u652f\u6301\u200b</li> </ul>"},{"location":"zh/design/profiling/#_13","title":"\u6027\u80fd\u200b\u5f00\u9500","text":""},{"location":"zh/design/profiling/#_14","title":"\u6d4b\u91cf\u200b\u7ed3\u679c","text":"\u573a\u666f\u200b \u200b\u5f00\u9500\u200b \u200b\u7a7a\u95f2\u200b\uff08\u200b\u65e0\u200b\u5206\u6790\u200b\uff09 &lt; 0.1% \u200b\u57fa\u7840\u200b\u5206\u6790\u200b &lt; 1% \u200b\u5b8c\u6574\u200b PyTorch \u200b\u5206\u6790\u200b &lt; 5% \u200b\u5e26\u200b\u53d8\u91cf\u200b\u6355\u83b7\u200b &lt; 10%"},{"location":"zh/design/profiling/#_15","title":"\u4f18\u5316\u200b\u6280\u672f","text":"<ol> <li>\u200b\u5ef6\u8fdf\u200b\u8ba1\u7b97\u200b - \u200b\u4ec5\u200b\u5728\u200b\u67e5\u8be2\u200b\u65f6\u200b\u8ba1\u7b97\u200b\u6307\u6807\u200b</li> <li>\u200b\u6279\u91cf\u200b\u5199\u5165\u200b - \u200b\u7f13\u51b2\u200b\u591a\u6761\u200b\u8bb0\u5f55\u200b\u540e\u200b\u518d\u200b\u5b58\u50a8\u200b</li> <li>\u200b\u91c7\u6837\u200b - \u200b\u53ef\u200b\u914d\u7f6e\u200b\u91c7\u6837\u7387\u200b</li> <li>\u200b\u9009\u62e9\u6027\u200b\u94a9\u5b50\u200b - \u200b\u4ec5\u200b\u542f\u7528\u200b\u9700\u8981\u200b\u7684\u200b\u6570\u636e\u6e90\u200b</li> </ol>"},{"location":"zh/examples/","title":"\u793a\u4f8b","text":"<p>\u200b\u5c55\u793a\u200b Probing \u200b\u80fd\u529b\u200b\u7684\u200b\u771f\u5b9e\u200b\u793a\u4f8b\u200b\u3002</p>"},{"location":"zh/examples/#_2","title":"\u6982\u89c8","text":"<p>\u200b\u8fd9\u4e9b\u200b\u793a\u4f8b\u200b\u5c55\u793a\u200b\u4e86\u200b AI/ML \u200b\u5de5\u4f5c\u200b\u6d41\u4e2d\u200b\u5e38\u89c1\u200b\u7684\u200b\u8c03\u8bd5\u200b\u548c\u200b\u5206\u6790\u200b\u573a\u666f\u200b\u3002</p> \u200b\u793a\u4f8b\u200b \u200b\u63cf\u8ff0\u200b \u200b\u8bad\u7ec3\u200b\u8c03\u8bd5\u200b \u200b\u8c03\u8bd5\u200b\u8bad\u7ec3\u200b\u95ee\u9898\u200b \u200b\u5185\u5b58\u200b\u6cc4\u6f0f\u200b \u200b\u67e5\u627e\u200b\u548c\u200b\u4fee\u590d\u200b\u5185\u5b58\u200b\u6cc4\u6f0f\u200b \u200b\u6027\u80fd\u200b\u5206\u6790\u200b \u200b\u8bc6\u522b\u200b\u74f6\u9888"},{"location":"zh/examples/#_3","title":"\u5feb\u901f\u200b\u793a\u4f8b","text":""},{"location":"zh/examples/#_4","title":"\u68c0\u67e5\u200b\u8bad\u7ec3\u200b\u8fdb\u5ea6","text":"<pre><code>probing $ENDPOINT eval \"\nprint(f'\u200b\u6b65\u9aa4\u200b: {trainer.current_step}')\nprint(f'\u200b\u635f\u5931\u200b: {trainer.last_loss:.4f}')\nprint(f'\u200b\u5b66\u4e60\u200b\u7387\u200b: {optimizer.param_groups[0][\\\"lr\\\"]:.6f}')\"\n</code></pre>"},{"location":"zh/examples/#gpu","title":"\u76d1\u63a7\u200b GPU \u200b\u5185\u5b58","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\nallocated = torch.cuda.memory_allocated() / 1024**3\nreserved = torch.cuda.memory_reserved() / 1024**3\nprint(f'\u200b\u5df2\u200b\u5206\u914d\u200b: {allocated:.2f} GB')\nprint(f'\u200b\u5df2\u200b\u4fdd\u7559\u200b: {reserved:.2f} GB')\"\n</code></pre>"},{"location":"zh/examples/#_5","title":"\u67e5\u627e\u200b\u6162\u200b\u64cd\u4f5c","text":"<pre><code>probing $ENDPOINT query \"\nSELECT module, AVG(duration) as avg_time\nFROM python.torch_trace\nWHERE step &gt; (SELECT MAX(step) - 5 FROM python.torch_trace)\nGROUP BY module\nORDER BY avg_time DESC\nLIMIT 5\"\n</code></pre>"},{"location":"zh/examples/#_6","title":"\u68c0\u67e5\u200b\u7ebf\u7a0b\u200b\u72b6\u6001","text":"<pre><code>probing $ENDPOINT eval \"\nimport threading\nfor t in threading.enumerate():\n    print(f'{t.name}: alive={t.is_alive()}, daemon={t.daemon}')\"\n</code></pre>"},{"location":"zh/examples/#_7","title":"\u8fd0\u884c\u200b\u793a\u4f8b","text":"<p>\u200b\u6240\u6709\u200b\u793a\u4f8b\u200b\u5047\u8bbe\u200b\u60a8\u200b\u5df2\u7ecf\u200b\uff1a</p> <ol> <li>\u200b\u4e00\u4e2a\u200b\u542f\u7528\u200b\u4e86\u200b Probing \u200b\u7684\u200b\u8fd0\u884c\u200b\u4e2d\u200b Python \u200b\u8fdb\u7a0b\u200b</li> <li>\u200b\u8bbe\u7f6e\u200b\u4e86\u200b <code>$ENDPOINT</code> \u200b\u73af\u5883\u53d8\u91cf\u200b</li> </ol> <pre><code># \u200b\u8bbe\u7f6e\u200b\u7aef\u70b9\u200b\nexport ENDPOINT=12345  # \u200b\u8fdb\u7a0b\u200b ID\n# \u200b\u6216\u200b\nexport ENDPOINT=host:8080  # \u200b\u8fdc\u7a0b\u200b\u5730\u5740\u200b\n\n# \u200b\u8fd0\u884c\u200b\u793a\u4f8b\u200b\u547d\u4ee4\u200b\nprobing $ENDPOINT eval \"...\"\n</code></pre>"},{"location":"zh/examples/#_8","title":"\u8d21\u732e\u200b\u793a\u4f8b","text":"<p>\u200b\u6709\u200b\u5b9e\u7528\u200b\u7684\u200b\u8c03\u8bd5\u6a21\u5f0f\u200b\uff1f\u200b\u6b22\u8fce\u200b\u8d21\u732e\u200b\uff01</p> <ol> <li>Fork \u200b\u4ed3\u5e93\u200b</li> <li>\u200b\u5c06\u200b\u60a8\u200b\u7684\u200b\u793a\u4f8b\u200b\u6dfb\u52a0\u200b\u5230\u200b <code>docs/src/examples/</code></li> <li>\u200b\u63d0\u4ea4\u200b Pull Request</li> </ol>"},{"location":"zh/examples/memory-leak/","title":"\u5185\u5b58\u200b\u6cc4\u6f0f\u200b\u793a\u4f8b","text":"<p>\u200b\u68c0\u6d4b\u200b\u548c\u200b\u4fee\u590d\u200b AI \u200b\u5e94\u7528\u200b\u4e2d\u200b\u7684\u200b\u5185\u5b58\u200b\u6cc4\u6f0f\u200b\u3002</p>"},{"location":"zh/examples/memory-leak/#_2","title":"\u8bc6\u522b\u200b\u5185\u5b58\u200b\u6cc4\u6f0f","text":""},{"location":"zh/examples/memory-leak/#_3","title":"\u5185\u5b58\u200b\u589e\u957f\u200b\u6a21\u5f0f","text":"<pre><code># \u200b\u8ffd\u8e2a\u200b\u5404\u200b\u6b65\u9aa4\u200b\u7684\u200b\u5185\u5b58\u200b\nprobing $ENDPOINT query \"\nSELECT\n    step,\n    MAX(allocated) as peak_memory_mb\nFROM python.torch_trace\nGROUP BY step\nORDER BY step\"\n</code></pre>"},{"location":"zh/examples/memory-leak/#_4","title":"\u5355\u8c03\u200b\u589e\u957f\u200b\u68c0\u6d4b","text":"<pre><code>WITH memory_trend AS (\n  SELECT\n    step,\n    MAX(allocated) as peak,\n    LAG(MAX(allocated)) OVER (ORDER BY step) as prev_peak\n  FROM python.torch_trace\n  GROUP BY step\n)\nSELECT\n    step,\n    peak,\n    peak - prev_peak as growth\nFROM memory_trend\nWHERE peak &gt; prev_peak\nORDER BY step;\n</code></pre>"},{"location":"zh/examples/memory-leak/#_5","title":"\u5e38\u89c1\u200b\u6cc4\u6f0f\u200b\u6a21\u5f0f","text":""},{"location":"zh/examples/memory-leak/#1","title":"\u6a21\u5f0f\u200b 1\uff1a\u200b\u5217\u8868\u200b\u4e2d\u200b\u7d2f\u79ef\u200b\u5f20\u91cf","text":"<p>\u200b\u95ee\u9898\u200b\uff1a</p> <pre><code># \u200b\u9519\u8bef\u200b\uff1a\u200b\u5f20\u91cf\u200b\u5728\u200b\u5217\u8868\u200b\u4e2d\u200b\u7d2f\u79ef\u200b\nall_losses = []\nfor batch in dataloader:\n    loss = model(batch)\n    all_losses.append(loss)  # \u200b\u4fdd\u6301\u200b\u8ba1\u7b97\u200b\u56fe\u200b\uff01\n</code></pre> <p>\u200b\u68c0\u6d4b\u200b\uff1a</p> <pre><code>probing $ENDPOINT eval \"\nimport gc\nimport torch\ntensors = [obj for obj in gc.get_objects() if isinstance(obj, torch.Tensor)]\nprint(f'\u200b\u5f20\u91cf\u200b\u603b\u6570\u200b: {len(tensors)}')\"\n</code></pre> <p>\u200b\u4fee\u590d\u200b\uff1a</p> <pre><code>probing $ENDPOINT eval \"\n# \u200b\u4f7f\u7528\u200b .item() \u200b\u63d0\u53d6\u200b\u6807\u91cf\u200b\nall_losses.clear()\nprint('\u200b\u5df2\u200b\u6e05\u7a7a\u200b\u635f\u5931\u200b\u5217\u8868\u200b')\"\n</code></pre>"},{"location":"zh/examples/memory-leak/#2","title":"\u6a21\u5f0f\u200b 2\uff1a\u200b\u9057\u5fd8\u200b\u7684\u200b\u68af\u5ea6\u200b\u56fe","text":"<p>\u200b\u95ee\u9898\u200b\uff1a</p> <pre><code># \u200b\u9519\u8bef\u200b\uff1a\u200b\u4e2d\u95f4\u200b\u5f20\u91cf\u200b\u4fdd\u6301\u200b grad_fn\nintermediate = model.encoder(x)\n# ... \u200b\u5f88\u591a\u200b\u64cd\u4f5c\u200b ...\n# intermediate \u200b\u4ecd\u7136\u200b\u4fdd\u6301\u200b\u8ba1\u7b97\u200b\u56fe\u200b\n</code></pre> <p>\u200b\u68c0\u6d4b\u200b\uff1a</p> <pre><code>probing $ENDPOINT eval \"\nimport torch\nfor name, param in model.named_parameters():\n    if param.grad is not None:\n        print(f'{name}: grad_fn={param.grad.grad_fn}')\"\n</code></pre> <p>\u200b\u4fee\u590d\u200b\uff1a</p> <pre><code>probing $ENDPOINT eval \"\nmodel.zero_grad(set_to_none=True)\nimport torch\ntorch.cuda.empty_cache()\nprint('\u200b\u5df2\u200b\u6e05\u9664\u200b\u68af\u5ea6\u200b')\"\n</code></pre>"},{"location":"zh/examples/memory-leak/#3","title":"\u6a21\u5f0f\u200b 3\uff1a\u200b\u5faa\u73af\u200b\u5f15\u7528","text":"<p>\u200b\u68c0\u6d4b\u200b\uff1a</p> <pre><code>probing $ENDPOINT eval \"\nimport gc\ngc.set_debug(gc.DEBUG_SAVEALL)\ngc.collect()\nprint(f'\u200b\u65e0\u6cd5\u200b\u56de\u6536\u200b: {len(gc.garbage)}')\"\n</code></pre>"},{"location":"zh/examples/memory-leak/#gpu","title":"GPU \u200b\u5185\u5b58\u200b\u6cc4\u6f0f","text":""},{"location":"zh/examples/memory-leak/#cuda","title":"\u68c0\u67e5\u200b CUDA \u200b\u5185\u5b58\u200b\u72b6\u6001","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\nprint(f'\u200b\u5df2\u200b\u5206\u914d\u200b: {torch.cuda.memory_allocated() / 1024**3:.2f} GB')\nprint(f'\u200b\u5df2\u200b\u4fdd\u7559\u200b: {torch.cuda.memory_reserved() / 1024**3:.2f} GB')\nprint(f'\u200b\u5cf0\u503c\u200b: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB')\"\n</code></pre>"},{"location":"zh/examples/memory-leak/#_6","title":"\u5185\u5b58\u200b\u5feb\u7167","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\nif torch.cuda.is_available():\n    snapshot = torch.cuda.memory_snapshot()\n    print(f'\u200b\u5757\u200b\u6570\u91cf\u200b: {len(snapshot)}')\"\n</code></pre>"},{"location":"zh/examples/memory-leak/#cuda_1","title":"\u5f3a\u5236\u200b CUDA \u200b\u6e05\u7406","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\nimport gc\n\n# \u200b\u6e05\u9664\u200b\u6240\u6709\u200b\u5f15\u7528\u200b\ngc.collect()\n\n# \u200b\u6e05\u7a7a\u200b CUDA \u200b\u7f13\u5b58\u200b\ntorch.cuda.empty_cache()\n\n# \u200b\u91cd\u7f6e\u200b\u5cf0\u503c\u200b\u7edf\u8ba1\u200b\ntorch.cuda.reset_peak_memory_stats()\n\nprint('CUDA \u200b\u5185\u5b58\u200b\u5df2\u200b\u6e05\u7406\u200b')\"\n</code></pre>"},{"location":"zh/examples/memory-leak/#cpu","title":"CPU \u200b\u5185\u5b58\u200b\u6cc4\u6f0f","text":""},{"location":"zh/examples/memory-leak/#_7","title":"\u8ffd\u8e2a\u200b\u8fdb\u7a0b\u200b\u5185\u5b58","text":"<pre><code>probing $ENDPOINT eval \"\nimport psutil\nimport os\n\nproc = psutil.Process(os.getpid())\nmem = proc.memory_info()\nprint(f'RSS: {mem.rss / 1024**3:.2f} GB')\nprint(f'VMS: {mem.vms / 1024**3:.2f} GB')\"\n</code></pre>"},{"location":"zh/examples/memory-leak/#_8","title":"\u67e5\u627e\u200b\u5927\u200b\u5bf9\u8c61","text":"<pre><code>probing $ENDPOINT eval \"\nimport sys\nimport gc\n\n# \u200b\u67e5\u627e\u200b\u6700\u5927\u200b\u5bf9\u8c61\u200b\nobjects = gc.get_objects()\nsizes = [(sys.getsizeof(obj), type(obj).__name__) for obj in objects[:1000]]\nsizes.sort(reverse=True)\nfor size, name in sizes[:10]:\n    print(f'{name}: {size / 1024:.1f} KB')\"\n</code></pre>"},{"location":"zh/examples/memory-leak/#_9","title":"\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\u6cc4\u6f0f","text":""},{"location":"zh/examples/memory-leak/#worker","title":"\u68c0\u67e5\u200b Worker \u200b\u72b6\u6001","text":"<pre><code>probing $ENDPOINT eval \"\nprint(f'Num workers: {train_loader.num_workers}')\nprint(f'Batch size: {train_loader.batch_size}')\"\n</code></pre>"},{"location":"zh/examples/memory-leak/#_10","title":"\u68c0\u67e5\u200b\u9884\u53d6\u200b\u6570\u636e","text":"<pre><code>probing $ENDPOINT eval \"\n# \u200b\u68c0\u67e5\u6570\u636e\u200b\u662f\u5426\u200b\u88ab\u200b\u4fdd\u6301\u200b\nif hasattr(train_loader, '_iterator'):\n    print('\u200b\u8fed\u4ee3\u200b\u5668\u200b\u5b58\u5728\u200b')\nelse:\n    print('\u200b\u6ca1\u6709\u200b\u6d3b\u52a8\u200b\u7684\u200b\u8fed\u4ee3\u200b\u5668\u200b')\"\n</code></pre>"},{"location":"zh/examples/memory-leak/#_11","title":"\u6301\u7eed\u200b\u76d1\u63a7","text":""},{"location":"zh/examples/memory-leak/#_12","title":"\u5b9a\u671f\u200b\u5185\u5b58\u200b\u68c0\u67e5","text":"<pre><code># \u200b\u6bcf\u5206\u949f\u200b\u8fd0\u884c\u200b\nwhile true; do\n    probing $ENDPOINT eval \"\nimport torch\nimport psutil\nimport os\nproc = psutil.Process(os.getpid())\nprint(f'CPU: {proc.memory_info().rss / 1024**3:.2f} GB, GPU: {torch.cuda.memory_allocated() / 1024**3:.2f} GB')\"\n    sleep 60\ndone\n</code></pre>"},{"location":"zh/examples/memory-leak/#sql","title":"\u57fa\u4e8e\u200b SQL \u200b\u7684\u200b\u76d1\u63a7","text":"<pre><code>-- \u200b\u6700\u8fd1\u200b 100 \u200b\u6b65\u200b\u7684\u200b\u5185\u5b58\u200b\u8d8b\u52bf\u200b\nSELECT\n    step,\n    AVG(allocated) as avg_memory,\n    MAX(allocated) as peak_memory\nFROM python.torch_trace\nWHERE step &gt; (SELECT MAX(step) - 100 FROM python.torch_trace)\nGROUP BY step\nORDER BY step;\n</code></pre>"},{"location":"zh/examples/memory-leak/#_13","title":"\u9884\u9632\u200b\u6700\u4f73\u200b\u5b9e\u8df5","text":""},{"location":"zh/examples/memory-leak/#1_1","title":"1. \u200b\u4f7f\u7528\u200b\u4e0a\u4e0b\u6587\u200b\u7ba1\u7406\u5668","text":"<pre><code># \u200b\u597d\u200b\u7684\u200b\u505a\u6cd5\u200b\nwith torch.no_grad():\n    output = model(input)\n</code></pre>"},{"location":"zh/examples/memory-leak/#2_1","title":"2. \u200b\u5206\u79bb\u200b\u5f20\u91cf","text":"<pre><code># \u200b\u5b58\u50a8\u200b\u65f6\u200b\nstored_output = output.detach().cpu()\n</code></pre>"},{"location":"zh/examples/memory-leak/#3_1","title":"3. \u200b\u5b9a\u671f\u200b\u6e05\u7406\u200b\u7f13\u5b58","text":"<pre><code>if step % 100 == 0:\n    gc.collect()\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"zh/examples/memory-leak/#4-set_to_nonetrue","title":"4. \u200b\u4f7f\u7528\u200b <code>set_to_none=True</code>","text":"<pre><code>optimizer.zero_grad(set_to_none=True)  # \u200b\u66f4\u200b\u9ad8\u6548\u200b\n</code></pre>"},{"location":"zh/examples/performance-analysis/","title":"\u6027\u80fd\u200b\u5206\u6790\u200b\u793a\u4f8b","text":"<p>\u200b\u8bc6\u522b\u200b\u548c\u200b\u4fee\u590d\u200b AI \u200b\u5de5\u4f5c\u200b\u8d1f\u8f7d\u200b\u4e2d\u200b\u7684\u200b\u6027\u80fd\u200b\u74f6\u9888\u200b\u3002</p>"},{"location":"zh/examples/performance-analysis/#_2","title":"\u67e5\u627e\u200b\u74f6\u9888","text":""},{"location":"zh/examples/performance-analysis/#_3","title":"\u6574\u4f53\u200b\u6027\u80fd\u200b\u6982\u51b5","text":"<pre><code>probing $ENDPOINT query \"\nSELECT\n    module,\n    stage,\n    COUNT(*) as executions,\n    AVG(duration) as avg_time_sec,\n    SUM(duration) as total_time_sec,\n    SUM(duration) * 100.0 / SUM(SUM(duration)) OVER () as pct_time\nFROM python.torch_trace\nWHERE step &gt; (SELECT MAX(step) - 10 FROM python.torch_trace)\nGROUP BY module, stage\nORDER BY total_time_sec DESC\nLIMIT 15\"\n</code></pre>"},{"location":"zh/examples/performance-analysis/#_4","title":"\u6bcf\u6b65\u200b\u5206\u89e3","text":"<pre><code>probing $ENDPOINT query \"\nSELECT\n    step,\n    SUM(CASE WHEN stage = 'forward' THEN duration ELSE 0 END) as forward_time,\n    SUM(CASE WHEN stage = 'backward' THEN duration ELSE 0 END) as backward_time,\n    SUM(CASE WHEN stage = 'step' THEN duration ELSE 0 END) as optimizer_time\nFROM python.torch_trace\nWHERE step &gt; (SELECT MAX(step) - 5 FROM python.torch_trace)\nGROUP BY step\nORDER BY step\"\n</code></pre>"},{"location":"zh/examples/performance-analysis/#gpu","title":"GPU \u200b\u5229\u7528\u7387","text":""},{"location":"zh/examples/performance-analysis/#_5","title":"\u68c0\u67e5\u200b\u5f53\u524d\u200b\u5229\u7528\u7387","text":"<pre><code>probing $ENDPOINT eval \"\nimport subprocess\nresult = subprocess.run(\n    ['nvidia-smi', '--query-gpu=utilization.gpu,utilization.memory,temperature.gpu',\n     '--format=csv,noheader,nounits'],\n    capture_output=True, text=True\n)\nfor i, line in enumerate(result.stdout.strip().split('\\\\n')):\n    gpu_util, mem_util, temp = line.split(', ')\n    print(f'GPU {i}: \u200b\u5229\u7528\u7387\u200b={gpu_util}%, \u200b\u5185\u5b58\u200b={mem_util}%, \u200b\u6e29\u5ea6\u200b={temp}\u00b0C')\"\n</code></pre>"},{"location":"zh/examples/performance-analysis/#cuda","title":"CUDA \u200b\u540c\u6b65\u200b\u5f00\u9500","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\nimport time\n\n# \u200b\u6d4b\u91cf\u200b\u540c\u6b65\u200b\u5f00\u9500\u200b\nstart = time.perf_counter()\ntorch.cuda.synchronize()\nsync_time = time.perf_counter() - start\nprint(f'CUDA \u200b\u540c\u6b65\u200b\u65f6\u95f4\u200b: {sync_time*1000:.2f} ms')\"\n</code></pre>"},{"location":"zh/examples/performance-analysis/#_6","title":"\u5185\u5b58\u200b\u5e26\u5bbd","text":""},{"location":"zh/examples/performance-analysis/#_7","title":"\u5185\u5b58\u200b\u53d7\u9650\u200b\u64cd\u4f5c","text":"<pre><code>probing $ENDPOINT query \"\nSELECT\n    module,\n    AVG(allocated) as avg_memory_mb,\n    AVG(duration) as avg_time_sec,\n    AVG(allocated) / AVG(duration) as memory_bandwidth_mb_per_sec\nFROM python.torch_trace\nWHERE duration &gt; 0.001\nGROUP BY module\nORDER BY memory_bandwidth_mb_per_sec DESC\nLIMIT 10\"\n</code></pre>"},{"location":"zh/examples/performance-analysis/#_8","title":"\u6570\u636e\u200b\u52a0\u8f7d\u200b\u6027\u80fd","text":""},{"location":"zh/examples/performance-analysis/#_9","title":"\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\u8ba1\u65f6","text":"<pre><code>probing $ENDPOINT eval \"\nimport time\n\n# \u200b\u8ba1\u65f6\u200b\u4e00\u4e2a\u200b\u6279\u6b21\u200b\u52a0\u8f7d\u200b\nstart = time.perf_counter()\nbatch = next(iter(train_loader))\nload_time = time.perf_counter() - start\nprint(f'\u200b\u6279\u6b21\u200b\u52a0\u8f7d\u200b\u65f6\u95f4\u200b: {load_time*1000:.2f} ms')\"\n</code></pre>"},{"location":"zh/examples/performance-analysis/#worker","title":"Worker \u200b\u5206\u6790","text":"<pre><code>probing $ENDPOINT eval \"\nprint(f'Num workers: {train_loader.num_workers}')\nprint(f'Pin memory: {train_loader.pin_memory}')\nprint(f'Prefetch factor: {getattr(train_loader, \\\"prefetch_factor\\\", 2)}')\"\n</code></pre>"},{"location":"zh/examples/performance-analysis/#_10","title":"\u901a\u4fe1\u200b\u5f00\u9500\u200b\uff08\u200b\u5206\u5e03\u5f0f\u200b\uff09","text":""},{"location":"zh/examples/performance-analysis/#nccl","title":"NCCL \u200b\u64cd\u4f5c\u200b\u65f6\u95f4","text":"<pre><code>probing $ENDPOINT query \"\nSELECT\n    operation_type,\n    COUNT(*) as count,\n    AVG(duration_ms) as avg_time_ms,\n    MAX(duration_ms) as max_time_ms\nFROM python.nccl_trace\nGROUP BY operation_type\nORDER BY avg_time_ms DESC\"\n</code></pre>"},{"location":"zh/examples/performance-analysis/#all-reduce","title":"All-Reduce \u200b\u6269\u5c55","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch.distributed as dist\nimport time\n\nif dist.is_initialized():\n    tensor = torch.randn(1000000, device='cuda')\n\n    start = time.perf_counter()\n    dist.all_reduce(tensor)\n    torch.cuda.synchronize()\n    allreduce_time = time.perf_counter() - start\n\n    print(f'4MB All-reduce \u200b\u65f6\u95f4\u200b: {allreduce_time*1000:.2f} ms')\"\n</code></pre>"},{"location":"zh/examples/performance-analysis/#attention","title":"Attention \u200b\u74f6\u9888","text":""},{"location":"zh/examples/performance-analysis/#self-attention","title":"Self-Attention \u200b\u5206\u6790","text":"<pre><code>probing $ENDPOINT query \"\nSELECT\n    module,\n    AVG(duration) as avg_time,\n    AVG(allocated) as avg_memory\nFROM python.torch_trace\nWHERE module LIKE '%attention%' OR module LIKE '%attn%'\nGROUP BY module\nORDER BY avg_time DESC\"\n</code></pre>"},{"location":"zh/examples/performance-analysis/#_11","title":"\u6bcf\u200b\u5e8f\u5217\u200b\u957f\u5ea6\u200b\u7684\u200b\u5185\u5b58","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\n\n# \u200b\u68c0\u67e5\u200b attention \u200b\u5185\u5b58\u200b\u6269\u5c55\u200b\nseq_len = model.config.max_position_embeddings\nhidden = model.config.hidden_size\nnum_heads = model.config.num_attention_heads\n\n# Attention \u200b\u5206\u6570\u200b\u5185\u5b58\u200b: O(seq_len^2)\nattention_memory = seq_len * seq_len * num_heads * 4 / 1024**3  # GB\nprint(f'\u200b\u4f30\u8ba1\u200b attention \u200b\u5185\u5b58\u200b: {attention_memory:.2f} GB')\"\n</code></pre>"},{"location":"zh/examples/performance-analysis/#_12","title":"\u4f18\u5316\u200b\u5efa\u8bae","text":""},{"location":"zh/examples/performance-analysis/#_13","title":"\u57fa\u4e8e\u200b\u5206\u6790\u200b\u7684\u200b\u4f18\u5316","text":"<pre><code># 1. \u200b\u8bc6\u522b\u200b\u6700\u6162\u200b\u6a21\u5757\u200b\nprobing $ENDPOINT query \"\nSELECT module, AVG(duration) as avg_time\nFROM python.torch_trace\nGROUP BY module\nORDER BY avg_time DESC\nLIMIT 5\"\n\n# 2. \u200b\u68c0\u67e5\u200b\u662f\u200b\u8ba1\u7b97\u200b\u53d7\u9650\u200b\u8fd8\u662f\u200b\u5185\u5b58\u200b\u53d7\u9650\u200b\nprobing $ENDPOINT eval \"\nimport torch\n# \u200b\u9ad8\u200b\u8ba1\u7b97\u200b\u5229\u7528\u7387\u200b + \u200b\u4f4e\u200b\u5185\u5b58\u200b\u5e26\u5bbd\u200b = \u200b\u8ba1\u7b97\u200b\u53d7\u9650\u200b\n# \u200b\u4f4e\u200b\u8ba1\u7b97\u200b\u5229\u7528\u7387\u200b + \u200b\u9ad8\u200b\u5185\u5b58\u200b\u5229\u7528\u7387\u200b = \u200b\u5185\u5b58\u200b\u53d7\u9650\u200b\"\n</code></pre>"},{"location":"zh/examples/performance-analysis/#_14","title":"\u5e38\u89c1\u200b\u4f18\u5316","text":""},{"location":"zh/examples/performance-analysis/#torch-compile","title":"\u542f\u7528\u200b Torch Compile","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\nif hasattr(torch, 'compile'):\n    model = torch.compile(model)\n    print('\u200b\u6a21\u578b\u200b\u5df2\u200b\u7528\u200b torch.compile \u200b\u7f16\u8bd1\u200b')\"\n</code></pre>"},{"location":"zh/examples/performance-analysis/#_15","title":"\u542f\u7528\u200b\u6df7\u5408\u200b\u7cbe\u5ea6","text":"<pre><code>probing $ENDPOINT eval \"\nfrom torch.cuda.amp import autocast\nprint(f'AMP \u200b\u5df2\u200b\u542f\u7528\u200b: {torch.cuda.amp.autocast_mode.is_autocast_enabled()}')\"\n</code></pre>"},{"location":"zh/examples/performance-analysis/#_16","title":"\u68c0\u67e5\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u70b9","text":"<pre><code>probing $ENDPOINT eval \"\n# \u200b\u68c0\u67e5\u200b\u662f\u5426\u200b\u542f\u7528\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u70b9\u200b\nfor name, module in model.named_modules():\n    if hasattr(module, 'gradient_checkpointing'):\n        print(f'{name}: checkpoint={module.gradient_checkpointing}')\"\n</code></pre>"},{"location":"zh/examples/performance-analysis/#_17","title":"\u57fa\u51c6\u200b\u6d4b\u8bd5","text":""},{"location":"zh/examples/performance-analysis/#_18","title":"\u541e\u5410\u91cf\u200b\u6d4b\u91cf","text":"<pre><code>probing $ENDPOINT eval \"\nimport time\n\n# \u200b\u6d4b\u91cf\u200b 10 \u200b\u6b65\u200b\u7684\u200b\u541e\u5410\u91cf\u200b\nsteps = 10\nstart = time.perf_counter()\nfor _ in range(steps):\n    trainer.train_step()\nelapsed = time.perf_counter() - start\n\nsamples_per_sec = (steps * batch_size) / elapsed\nprint(f'\u200b\u541e\u5410\u91cf\u200b: {samples_per_sec:.1f} samples/sec')\"\n</code></pre>"},{"location":"zh/examples/performance-analysis/#_19","title":"\u4f18\u5316\u200b\u524d\u540e\u200b\u5bf9\u6bd4","text":"<pre><code># \u200b\u4f18\u5316\u200b\u524d\u200b\nprobing $ENDPOINT query \"\nSELECT AVG(duration) as before_avg\nFROM python.torch_trace\nWHERE step BETWEEN 100 AND 110\"\n\n# \u200b\u4f18\u5316\u200b\u540e\u200b\nprobing $ENDPOINT query \"\nSELECT AVG(duration) as after_avg\nFROM python.torch_trace\nWHERE step BETWEEN 200 AND 210\"\n</code></pre>"},{"location":"zh/examples/training-debugging/","title":"\u8bad\u7ec3\u200b\u8c03\u8bd5\u200b\u793a\u4f8b","text":"<p>\u200b\u5e38\u89c1\u200b\u7684\u200b\u8bad\u7ec3\u200b\u8c03\u8bd5\u200b\u573a\u666f\u200b\u548c\u200b\u89e3\u51b3\u65b9\u6848\u200b\u3002</p>"},{"location":"zh/examples/training-debugging/#_2","title":"\u8bad\u7ec3\u200b\u5361\u4f4f","text":""},{"location":"zh/examples/training-debugging/#_3","title":"\u75c7\u72b6","text":"<ul> <li>\u200b\u635f\u5931\u200b\u505c\u6b62\u200b\u66f4\u65b0\u200b</li> <li>\u200b\u6ca1\u6709\u200b\u9519\u8bef\u200b\u6d88\u606f\u200b</li> <li>\u200b\u8fdb\u7a0b\u200b\u4ecd\u200b\u5728\u200b\u8fd0\u884c\u200b</li> </ul>"},{"location":"zh/examples/training-debugging/#_4","title":"\u8bca\u65ad","text":"<pre><code># 1. \u200b\u6355\u83b7\u200b\u5806\u6808\u200b\u8ddf\u8e2a\u200b\nprobing $ENDPOINT backtrace\n\n# 2. \u200b\u68c0\u67e5\u200b\u6267\u884c\u200b\u5361\u200b\u5728\u200b\u54ea\u91cc\u200b\nprobing $ENDPOINT query \"\nSELECT func, file, lineno, depth\nFROM python.backtrace\nORDER BY depth\nLIMIT 10\"\n\n# 3. \u200b\u68c0\u67e5\u200b\u7ebf\u7a0b\u200b\u72b6\u6001\u200b\nprobing $ENDPOINT eval \"\nimport threading\nfor t in threading.enumerate():\n    print(f'{t.name}: alive={t.is_alive()}')\"\n</code></pre>"},{"location":"zh/examples/training-debugging/#_5","title":"\u5e38\u89c1\u200b\u539f\u56e0","text":""},{"location":"zh/examples/training-debugging/#_6","title":"\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\u963b\u585e","text":"<pre><code># \u200b\u68c0\u67e5\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\nprobing $ENDPOINT eval \"\nimport torch.utils.data\nprint(f'DataLoader workers: {train_loader.num_workers}')\nprint(f'Prefetch factor: {train_loader.prefetch_factor}')\"\n</code></pre>"},{"location":"zh/examples/training-debugging/#_7","title":"\u5206\u5e03\u5f0f\u200b\u6b7b\u9501","text":"<pre><code># \u200b\u68c0\u67e5\u200b\u5206\u5e03\u5f0f\u200b\u72b6\u6001\u200b\nprobing $ENDPOINT eval \"\nimport torch.distributed as dist\nif dist.is_initialized():\n    print(f'Rank: {dist.get_rank()}')\n    print(f'World Size: {dist.get_world_size()}')\"\n</code></pre>"},{"location":"zh/examples/training-debugging/#_8","title":"\u635f\u5931\u200b\u7206\u70b8","text":""},{"location":"zh/examples/training-debugging/#_9","title":"\u75c7\u72b6","text":"<ul> <li>\u200b\u635f\u5931\u200b\u53d8\u6210\u200b NaN \u200b\u6216\u200b Inf</li> <li>\u200b\u8bad\u7ec3\u200b\u53d1\u6563\u200b</li> </ul>"},{"location":"zh/examples/training-debugging/#_10","title":"\u8bca\u65ad","text":"<pre><code># \u200b\u68c0\u67e5\u200b\u68af\u5ea6\u200b\u4e2d\u200b\u7684\u200b NaN\nprobing $ENDPOINT eval \"\nimport torch\nfor name, param in model.named_parameters():\n    if param.grad is not None:\n        has_nan = torch.isnan(param.grad).any().item()\n        has_inf = torch.isinf(param.grad).any().item()\n        if has_nan or has_inf:\n            print(f'{name}: NaN={has_nan}, Inf={has_inf}')\"\n</code></pre>"},{"location":"zh/examples/training-debugging/#_11","title":"\u4fee\u590d\u200b\uff1a\u200b\u68af\u5ea6\u200b\u88c1\u526a","text":"<pre><code># \u200b\u68c0\u67e5\u200b\u5f53\u524d\u200b\u88c1\u526a\u200b\u8bbe\u7f6e\u200b\nprobing $ENDPOINT eval \"\nprint(f'Grad clip value: {getattr(trainer, \\\"grad_clip\\\", None)}')\"\n\n# \u200b\u5e94\u7528\u200b\u7d27\u6025\u200b\u88c1\u526a\u200b\nprobing $ENDPOINT eval \"\ntorch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\"\n</code></pre>"},{"location":"zh/examples/training-debugging/#_12","title":"\u8bad\u7ec3\u200b\u6162","text":""},{"location":"zh/examples/training-debugging/#_13","title":"\u8bca\u65ad","text":"<pre><code># \u200b\u627e\u5230\u200b\u6700\u6162\u200b\u7684\u200b\u6a21\u5757\u200b\nprobing $ENDPOINT query \"\nSELECT\n    module,\n    stage,\n    COUNT(*) as count,\n    AVG(duration) as avg_time,\n    SUM(duration) as total_time\nFROM python.torch_trace\nWHERE step &gt; (SELECT MAX(step) - 5 FROM python.torch_trace)\nGROUP BY module, stage\nORDER BY total_time DESC\nLIMIT 10\"\n</code></pre>"},{"location":"zh/examples/training-debugging/#gpu","title":"\u68c0\u67e5\u200b GPU \u200b\u5229\u7528\u7387","text":"<pre><code>probing $ENDPOINT eval \"\nimport subprocess\nresult = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader'],\n                        capture_output=True, text=True)\nprint(f'GPU \u200b\u5229\u7528\u7387\u200b: {result.stdout.strip()}')\"\n</code></pre>"},{"location":"zh/examples/training-debugging/#_14","title":"\u8bad\u7ec3\u200b\u671f\u95f4\u200b\u7684\u200b\u5185\u5b58\u200b\u95ee\u9898","text":""},{"location":"zh/examples/training-debugging/#_15","title":"\u8ffd\u8e2a\u200b\u5185\u5b58\u200b\u589e\u957f","text":"<pre><code>probing $ENDPOINT query \"\nSELECT\n    step,\n    MAX(allocated) as peak_memory,\n    MAX(allocated) - MIN(allocated) as memory_range\nFROM python.torch_trace\nWHERE step &gt; (SELECT MAX(step) - 20 FROM python.torch_trace)\nGROUP BY step\nORDER BY step\"\n</code></pre>"},{"location":"zh/examples/training-debugging/#_16","title":"\u5f3a\u5236\u200b\u6e05\u7406","text":"<pre><code>probing $ENDPOINT eval \"\nimport gc\nimport torch\n\n# \u200b\u6e05\u9664\u200b\u68af\u5ea6\u200b\nmodel.zero_grad(set_to_none=True)\n\n# \u200b\u5783\u573e\u200b\u56de\u6536\u200b\ngc.collect()\n\n# \u200b\u6e05\u7406\u200b CUDA \u200b\u7f13\u5b58\u200b\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint('\u200b\u6e05\u7406\u200b\u5b8c\u6210\u200b')\"\n</code></pre>"},{"location":"zh/examples/training-debugging/#_17","title":"\u68c0\u67e5\u70b9\u200b\u6062\u590d","text":""},{"location":"zh/examples/training-debugging/#_18","title":"\u4fdd\u5b58\u200b\u7d27\u6025\u200b\u68c0\u67e5\u70b9","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\ncheckpoint = {\n    'step': trainer.current_step,\n    'model': model.state_dict(),\n    'optimizer': optimizer.state_dict(),\n}\ntorch.save(checkpoint, 'emergency_checkpoint.pt')\nprint('\u200b\u7d27\u6025\u200b\u68c0\u67e5\u70b9\u200b\u5df2\u200b\u4fdd\u5b58\u200b')\"\n</code></pre>"},{"location":"zh/examples/training-debugging/#_19","title":"\u68c0\u67e5\u200b\u68c0\u67e5\u70b9","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\nckpt = torch.load('checkpoint.pt', map_location='cpu')\nprint(f'Keys: {ckpt.keys()}')\nprint(f'Step: {ckpt.get(\\\"step\\\", \\\"N/A\\\")}')\"\n</code></pre>"},{"location":"zh/examples/training-debugging/#_20","title":"\u5b66\u4e60\u200b\u7387\u200b\u95ee\u9898","text":""},{"location":"zh/examples/training-debugging/#_21","title":"\u68c0\u67e5\u200b\u5f53\u524d\u200b\u5b66\u4e60\u200b\u7387","text":"<pre><code>probing $ENDPOINT eval \"\nfor i, pg in enumerate(optimizer.param_groups):\n    print(f'Group {i}: lr={pg[\\\"lr\\\"]}, weight_decay={pg.get(\\\"weight_decay\\\", 0)}')\"\n</code></pre>"},{"location":"zh/examples/training-debugging/#_22","title":"\u8c03\u6574\u200b\u5b66\u4e60\u200b\u7387","text":"<pre><code># \u200b\u964d\u4f4e\u200b\u5b66\u4e60\u200b\u7387\u200b\nprobing $ENDPOINT eval \"\nfor pg in optimizer.param_groups:\n    pg['lr'] *= 0.1\nprint(f'\u200b\u65b0\u200b\u5b66\u4e60\u200b\u7387\u200b: {optimizer.param_groups[0][\\\"lr\\\"]}')\"\n</code></pre>"},{"location":"zh/examples/training-debugging/#_23","title":"\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u95ee\u9898","text":""},{"location":"zh/examples/training-debugging/#rank","title":"\u68c0\u67e5\u200b\u6240\u6709\u200b Rank","text":"<pre><code># \u200b\u5728\u200b\u6bcf\u4e2a\u200b\u8282\u70b9\u200b\u4e0a\u200b\u8fd0\u884c\u200b\nprobing -t node1:8080 eval \"print(f'Rank 0 step: {trainer.current_step}')\"\nprobing -t node2:8080 eval \"print(f'Rank 1 step: {trainer.current_step}')\"\n</code></pre>"},{"location":"zh/examples/training-debugging/#_24","title":"\u9a8c\u8bc1\u200b\u540c\u6b65","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch.distributed as dist\nif dist.is_initialized():\n    tensor = torch.tensor([trainer.current_step], device='cuda')\n    dist.all_reduce(tensor)\n    print(f'\u200b\u6240\u6709\u200b rank \u200b\u6b65\u6570\u200b\u4e4b\u200b\u548c\u200b: {tensor.item()}')\"\n</code></pre>"},{"location":"zh/guide/","title":"\u7528\u6237\u200b\u6307\u5357","text":"<p>\u200b\u6b22\u8fce\u200b\u9605\u8bfb\u200b Probing \u200b\u7528\u6237\u200b\u6307\u5357\u200b\u3002\u200b\u672c\u200b\u7ae0\u8282\u200b\u4ecb\u7ecd\u200b\u6838\u5fc3\u200b\u529f\u80fd\u200b\u548c\u200b\u4f7f\u7528\u200b\u6a21\u5f0f\u200b\u3002</p>"},{"location":"zh/guide/#_2","title":"\u6982\u89c8","text":"<p>Probing \u200b\u63d0\u4f9b\u200b\u4e09\u4e2a\u200b\u6838\u5fc3\u200b\u80fd\u529b\u200b\u6765\u200b\u5206\u6790\u200b\u548c\u200b\u8c03\u8bd5\u200b\u60a8\u200b\u7684\u200b AI \u200b\u5e94\u7528\u200b\uff1a</p> \u200b\u80fd\u529b\u200b \u200b\u547d\u4ee4\u200b \u200b\u63cf\u8ff0\u200b eval <code>probing $ENDPOINT eval \"...\"</code> \u200b\u5728\u200b\u76ee\u6807\u200b\u8fdb\u7a0b\u200b\u4e2d\u200b\u6267\u884c\u200b Python \u200b\u4ee3\u7801\u200b query <code>probing $ENDPOINT query \"...\"</code> \u200b\u4f7f\u7528\u200b SQL \u200b\u67e5\u8be2\u200b\u6027\u80fd\u200b\u6570\u636e\u200b backtrace <code>probing $ENDPOINT backtrace</code> \u200b\u6355\u83b7\u200b\u5e26\u200b\u53d8\u91cf\u200b\u7684\u200b\u6267\u884c\u200b\u5806\u6808"},{"location":"zh/guide/#_3","title":"\u5165\u95e8\u200b\u6307\u5357","text":"<p>\u200b\u5982\u679c\u200b\u60a8\u200b\u662f\u200b Probing \u200b\u65b0\u200b\u7528\u6237\u200b\uff0c\u200b\u5efa\u8bae\u200b\u6309\u200b\u4ee5\u4e0b\u200b\u987a\u5e8f\u200b\u9605\u8bfb\u200b\u8fd9\u4e9b\u200b\u6307\u5357\u200b\uff1a</p> <ol> <li>SQL \u200b\u5206\u6790\u200b - \u200b\u5b66\u4e60\u200b\u5f3a\u5927\u200b\u7684\u200b SQL \u200b\u67e5\u8be2\u200b\u63a5\u53e3\u200b</li> <li>\u200b\u5185\u5b58\u200b\u5206\u6790\u200b - \u200b\u8c03\u8bd5\u200b\u5185\u5b58\u200b\u6cc4\u6f0f\u200b\u548c\u200b\u4f7f\u7528\u200b\u6a21\u5f0f\u200b</li> <li>\u200b\u8c03\u8bd5\u200b\u6307\u5357\u200b - \u200b\u638c\u63e1\u200b\u5806\u6808\u200b\u5206\u6790\u200b\u548c\u200b\u5b9e\u65f6\u200b\u8c03\u8bd5\u200b</li> <li>\u200b\u5e38\u89c1\u95ee\u9898\u200b - \u200b\u5e38\u89c1\u95ee\u9898\u200b\u548c\u200b\u89e3\u51b3\u65b9\u6848\u200b</li> </ol>"},{"location":"zh/guide/#_4","title":"\u6838\u5fc3\u200b\u6982\u5ff5","text":""},{"location":"zh/guide/#_5","title":"\u76ee\u6807\u200b\u7aef\u70b9","text":"<p>\u200b\u6240\u6709\u200b Probing \u200b\u547d\u4ee4\u200b\u90fd\u200b\u9700\u8981\u200b\u4e00\u4e2a\u200b\u76ee\u6807\u200b\u7aef\u70b9\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u662f\u200b\uff1a</p> <ul> <li>\u200b\u8fdb\u7a0b\u200b ID\uff1a\u200b\u672c\u5730\u200b\u8fdb\u7a0b\u200b\uff08\u200b\u5982\u200b <code>12345</code>\uff09</li> <li>\u200b\u8fdc\u7a0b\u200b\u5730\u5740\u200b\uff1a\u200b\u7f51\u7edc\u200b\u7aef\u70b9\u200b\uff08\u200b\u5982\u200b <code>host:8080</code>\uff09</li> </ul> <pre><code># \u200b\u8bbe\u7f6e\u200b\u76ee\u6807\u200b\u7aef\u70b9\u200b\nexport ENDPOINT=12345  # \u200b\u6216\u200b host:8080\n</code></pre>"},{"location":"zh/guide/#_6","title":"\u6570\u636e\u8868","text":"<p>Probing \u200b\u901a\u8fc7\u200b SQL \u200b\u8868\u200b\u66b4\u9732\u200b\u6027\u80fd\u200b\u6570\u636e\u200b\uff1a</p> \u200b\u8868\u540d\u200b \u200b\u63cf\u8ff0\u200b <code>python.backtrace</code> \u200b\u5806\u6808\u200b\u8ddf\u8e2a\u200b\u4fe1\u606f\u200b <code>python.torch_trace</code> PyTorch \u200b\u6267\u884c\u200b\u8ddf\u8e2a\u200b <code>python.variables</code> \u200b\u53d8\u91cf\u200b\u8ffd\u8e2a\u200b <code>information_schema.df_settings</code> \u200b\u914d\u7f6e\u200b\u8bbe\u7f6e"},{"location":"zh/guide/#_7","title":"\u5de5\u4f5c\u200b\u6d41\u200b\u6a21\u5f0f","text":"<p>\u200b\u8c03\u8bd5\u200b\u5de5\u4f5c\u200b\u6d41\u200b\uff1a <pre><code># 1. \u200b\u6355\u83b7\u200b\u5f53\u524d\u200b\u72b6\u6001\u200b\nprobing $ENDPOINT backtrace\n\n# 2. \u200b\u68c0\u67e5\u200b\u7279\u5b9a\u200b\u503c\u200b\nprobing $ENDPOINT eval \"print(my_variable)\"\n\n# 3. \u200b\u67e5\u8be2\u200b\u5386\u53f2\u6570\u636e\u200b\nprobing $ENDPOINT query \"SELECT * FROM python.torch_trace\"\n</code></pre></p>"},{"location":"zh/guide/#_8","title":"\u8fdb\u9636\u200b\u4e3b\u9898","text":"<ul> <li>\u200b\u7cfb\u7edf\u200b\u67b6\u6784\u200b - \u200b\u7cfb\u7edf\u200b\u8bbe\u8ba1\u200b\u548c\u200b\u5185\u90e8\u200b\u5b9e\u73b0\u200b</li> <li>\u200b\u5206\u5e03\u5f0f\u200b - \u200b\u591a\u200b\u8282\u70b9\u200b\u76d1\u63a7\u200b</li> <li>\u200b\u6269\u5c55\u200b\u673a\u5236\u200b - \u200b\u81ea\u5b9a\u4e49\u200b\u8868\u200b\u548c\u200b\u6307\u6807\u200b</li> </ul>"},{"location":"zh/guide/debugging/","title":"\u8c03\u8bd5\u200b\u6307\u5357","text":"<p>\u200b\u638c\u63e1\u200b\u4f7f\u7528\u200b Probing \u200b\u5f3a\u5927\u200b\u7684\u200b\u5185\u7701\u200b\u80fd\u529b\u200b\u8c03\u8bd5\u200b AI \u200b\u5e94\u7528\u200b\u7684\u200b\u827a\u672f\u200b\u3002</p>"},{"location":"zh/guide/debugging/#_2","title":"\u6982\u89c8","text":"<p>Probing \u200b\u63d0\u4f9b\u200b\u4e09\u79cd\u200b\u4e92\u8865\u200b\u7684\u200b\u8c03\u8bd5\u200b\u65b9\u5f0f\u200b\uff1a</p> <ul> <li>backtrace\uff1a\u200b\u6355\u83b7\u200b\u5e26\u6709\u200b\u5806\u6808\u200b\u5e27\u200b\u7684\u200b\u6267\u884c\u200b\u4e0a\u4e0b\u6587\u200b</li> <li>eval\uff1a\u200b\u5728\u200b\u76ee\u6807\u200b\u8fdb\u7a0b\u200b\u4e2d\u200b\u6267\u884c\u200b\u4efb\u610f\u200b Python \u200b\u4ee3\u7801\u200b</li> <li>query\uff1a\u200b\u4f7f\u7528\u200b SQL \u200b\u5206\u6790\u200b\u6536\u96c6\u200b\u7684\u200b\u6570\u636e\u200b</li> </ul>"},{"location":"zh/guide/debugging/#_3","title":"\u5806\u6808\u200b\u5206\u6790","text":""},{"location":"zh/guide/debugging/#_4","title":"\u6355\u83b7\u200b\u5f53\u524d\u200b\u5806\u6808","text":"<pre><code># \u200b\u83b7\u53d6\u200b\u5f53\u524d\u200b\u6267\u884c\u200b\u5806\u6808\u200b\nprobing $ENDPOINT backtrace\n\n# \u200b\u67e5\u8be2\u200b\u5806\u6808\u200b\u5e27\u200b\nprobing $ENDPOINT query \"\nSELECT func, file, lineno, depth\nFROM python.backtrace\nORDER BY depth\nLIMIT 10\"\n</code></pre>"},{"location":"zh/guide/debugging/#_5","title":"\u7406\u89e3\u200b\u5806\u6808\u200b\u5e27","text":"<p>\u200b\u6bcf\u4e2a\u200b\u5806\u6808\u200b\u5e27\u200b\u5305\u62ec\u200b\uff1a</p> \u200b\u5b57\u200b\u6bb5\u200b \u200b\u63cf\u8ff0\u200b func \u200b\u51fd\u6570\u200b\u540d\u200b file \u200b\u6e90\u6587\u4ef6\u200b\u8def\u5f84\u200b lineno \u200b\u884c\u53f7\u200b depth \u200b\u5806\u6808\u200b\u6df1\u5ea6\u200b\uff080 = \u200b\u6700\u200b\u5185\u5c42\u200b\uff09 frame_type Python \u200b\u6216\u200b Native"},{"location":"zh/guide/debugging/#_6","title":"\u627e\u5230\u200b\u4ee3\u7801\u200b\u5361\u4f4f\u200b\u7684\u200b\u4f4d\u7f6e","text":"<pre><code># \u200b\u6355\u83b7\u200b\u5806\u6808\u200b\nprobing $ENDPOINT backtrace\n\n# \u200b\u68c0\u67e5\u200b\u5806\u6808\u200b\u9876\u90e8\u200b\nprobing $ENDPOINT query \"\nSELECT func, file, lineno\nFROM python.backtrace\nWHERE depth = 0\"\n</code></pre>"},{"location":"zh/guide/debugging/#_7","title":"\u5b9e\u65f6\u200b\u68c0\u67e5","text":""},{"location":"zh/guide/debugging/#_8","title":"\u68c0\u67e5\u200b\u53d8\u91cf","text":"<pre><code># \u200b\u68c0\u67e5\u200b\u5168\u5c40\u53d8\u91cf\u200b\nprobing $ENDPOINT eval \"print(globals().keys())\"\n\n# \u200b\u68c0\u67e5\u200b\u7279\u5b9a\u200b\u5bf9\u8c61\u200b\nprobing $ENDPOINT eval \"print(type(model), model)\"\n\n# \u200b\u68c0\u67e5\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\nprobing $ENDPOINT eval \"\nfor name, param in model.named_parameters():\n    print(f'{name}: {param.shape}, grad={param.grad is not None}')\n\"\n</code></pre>"},{"location":"zh/guide/debugging/#_9","title":"\u68c0\u67e5\u200b\u7ebf\u7a0b\u200b\u72b6\u6001","text":"<pre><code>probing $ENDPOINT eval \"\nimport threading\nfor t in threading.enumerate():\n    print(f'{t.name}: alive={t.is_alive()}, daemon={t.daemon}')\n\"\n</code></pre>"},{"location":"zh/guide/debugging/#_10","title":"\u76d1\u63a7\u200b\u8bad\u7ec3\u200b\u8fdb\u5ea6","text":"<pre><code>probing $ENDPOINT eval \"\n# \u200b\u68c0\u67e5\u200b\u5f53\u524d\u200b\u6b65\u9aa4\u200b\nprint(f'\u200b\u5f53\u524d\u200b\u6b65\u9aa4\u200b: {trainer.current_step}')\nprint(f'\u200b\u5f53\u524d\u200b\u635f\u5931\u200b: {trainer.last_loss}')\nprint(f'\u200b\u5b66\u4e60\u200b\u7387\u200b: {optimizer.param_groups[0][\\\"lr\\\"]}')\n\"\n</code></pre>"},{"location":"zh/guide/debugging/#_11","title":"\u8c03\u8bd5\u200b\u573a\u666f","text":""},{"location":"zh/guide/debugging/#1","title":"\u573a\u666f\u200b 1\uff1a\u200b\u8bad\u7ec3\u200b\u5361\u4f4f","text":"<p>\u200b\u75c7\u72b6\u200b\uff1a\u200b\u8bad\u7ec3\u200b\u8fdb\u5ea6\u200b\u505c\u6b62\u200b\uff0c\u200b\u6ca1\u6709\u200b\u9519\u8bef\u200b\u3002</p> <p>\u200b\u8bca\u65ad\u200b\uff1a</p> <pre><code># 1. \u200b\u6355\u83b7\u200b\u5806\u6808\u200b\nprobing $ENDPOINT backtrace\n\n# 2. \u200b\u68c0\u67e5\u200b\u662f\u200b\u4ec0\u4e48\u200b\u963b\u585e\u200b\u4e86\u200b\nprobing $ENDPOINT query \"\nSELECT func, file, lineno\nFROM python.backtrace\nWHERE depth &lt; 5\"\n\n# 3. \u200b\u68c0\u67e5\u200b\u7ebf\u7a0b\u200b\u72b6\u6001\u200b\nprobing $ENDPOINT eval \"\nimport threading\nfor t in threading.enumerate():\n    print(f'{t.name}: {t.is_alive()}')\"\n\n# 4. \u200b\u68c0\u67e5\u200b\u6b7b\u9501\u200b\nprobing $ENDPOINT eval \"\nimport torch.distributed as dist\nif dist.is_initialized():\n    print(f'Rank: {dist.get_rank()}, World: {dist.get_world_size()}')\"\n</code></pre>"},{"location":"zh/guide/debugging/#2-naninf","title":"\u573a\u666f\u200b 2\uff1a\u200b\u68af\u5ea6\u200b\u4e2d\u200b\u51fa\u73b0\u200b NaN/Inf","text":"<p>\u200b\u75c7\u72b6\u200b\uff1a\u200b\u635f\u5931\u200b\u53d8\u6210\u200b NaN \u200b\u6216\u200b Inf\u3002</p> <p>\u200b\u8bca\u65ad\u200b\uff1a</p> <pre><code># \u200b\u68c0\u67e5\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u4e2d\u200b\u7684\u200b NaN\nprobing $ENDPOINT eval \"\nimport torch\nfor name, param in model.named_parameters():\n    if param.grad is not None:\n        if torch.isnan(param.grad).any():\n            print(f'{name} \u200b\u68af\u5ea6\u200b\u4e2d\u6709\u200b NaN')\n        if torch.isinf(param.grad).any():\n            print(f'{name} \u200b\u68af\u5ea6\u200b\u4e2d\u6709\u200b Inf')\"\n\n# \u200b\u68c0\u67e5\u200b\u635f\u5931\u200b\u503c\u200b\nprobing $ENDPOINT eval \"\nimport torch\nprint(f'\u200b\u635f\u5931\u200b: {loss.item()}')\nprint(f'\u200b\u662f\u5426\u200b NaN: {torch.isnan(loss).item()}')\nprint(f'\u200b\u662f\u5426\u200b Inf: {torch.isinf(loss).item()}')\"\n</code></pre>"},{"location":"zh/guide/debugging/#3","title":"\u573a\u666f\u200b 3\uff1a\u200b\u8bad\u7ec3\u200b\u6162","text":"<p>\u200b\u75c7\u72b6\u200b\uff1a\u200b\u8bad\u7ec3\u200b\u6bd4\u200b\u9884\u671f\u200b\u6162\u200b\u3002</p> <p>\u200b\u8bca\u65ad\u200b\uff1a</p> <pre><code>-- \u200b\u627e\u5230\u200b\u6700\u6162\u200b\u7684\u200b\u64cd\u4f5c\u200b\nSELECT module, stage, avg(duration) as avg_time\nFROM python.torch_trace\nWHERE step &gt; (SELECT max(step) - 5 FROM python.torch_trace)\nGROUP BY module, stage\nORDER BY avg_time DESC\nLIMIT 10;\n</code></pre>"},{"location":"zh/guide/debugging/#_12","title":"\u6700\u4f73\u200b\u5b9e\u8df5","text":"<ol> <li>\u200b\u5148\u200b\u7528\u200b backtrace - \u200b\u5728\u200b\u6df1\u5165\u200b\u4e4b\u524d\u200b\u5148\u200b\u4e86\u89e3\u200b\u6267\u884c\u200b\u4f4d\u7f6e\u200b</li> <li>\u200b\u7528\u200b query \u200b\u5206\u6790\u200b\u8d8b\u52bf\u200b - SQL \u200b\u975e\u5e38\u9002\u5408\u200b\u5206\u6790\u200b\u65f6\u95f4\u200b\u6a21\u5f0f\u200b</li> <li>\u200b\u7528\u200b eval \u200b\u83b7\u53d6\u200b\u5b9e\u65f6\u200b\u72b6\u6001\u200b - \u200b\u7528\u200b Python \u200b\u4ee3\u7801\u200b\u83b7\u53d6\u200b\u5f53\u524d\u200b\u72b6\u6001\u200b</li> <li>\u200b\u7ec4\u5408\u200b\u4f7f\u7528\u200b - backtrace \u2192 eval \u2192 query \u200b\u5de5\u4f5c\u200b\u6d41\u200b</li> <li>\u200b\u8bb0\u5f55\u200b\u91cd\u8981\u200b\u72b6\u6001\u200b - \u200b\u4f7f\u7528\u200b eval \u200b\u6253\u5370\u200b\u8bca\u65ad\u200b\u4fe1\u606f\u200b</li> </ol>"},{"location":"zh/guide/debugging/#_13","title":"\u4e0b\u200b\u4e00\u6b65","text":"<ul> <li>\u200b\u5185\u5b58\u200b\u5206\u6790\u200b - \u200b\u8c03\u8bd5\u200b\u5185\u5b58\u200b\u95ee\u9898\u200b</li> <li>\u200b\u5e38\u89c1\u95ee\u9898\u200b - \u200b\u5e38\u89c1\u95ee\u9898\u200b\u548c\u200b\u89e3\u51b3\u65b9\u6848\u200b</li> <li>\u200b\u7cfb\u7edf\u200b\u67b6\u6784\u200b - \u200b\u4e86\u89e3\u200b\u5185\u90e8\u200b\u5b9e\u73b0\u200b</li> </ul>"},{"location":"zh/guide/memory-analysis/","title":"\u5185\u5b58\u200b\u5206\u6790","text":"<p>Probing \u200b\u63d0\u4f9b\u200b\u5168\u9762\u200b\u7684\u200b\u5de5\u5177\u200b\u7528\u4e8e\u200b\u5206\u6790\u200b AI \u200b\u5e94\u7528\u200b\u4e2d\u200b\u7684\u200b\u5185\u5b58\u200b\u4f7f\u7528\u200b\u3002</p>"},{"location":"zh/guide/memory-analysis/#_2","title":"\u6982\u89c8","text":"<p>\u200b\u5185\u5b58\u200b\u95ee\u9898\u200b\u5728\u200b AI \u200b\u5de5\u4f5c\u200b\u8d1f\u8f7d\u200b\u4e2d\u200b\u5f88\u200b\u5e38\u89c1\u200b\uff0c\u200b\u7279\u522b\u200b\u662f\u200b\u5728\u200b\u8bad\u7ec3\u200b\u671f\u95f4\u200b\u3002Probing \u200b\u5e2e\u52a9\u200b\u60a8\u200b\uff1a</p> <ul> <li>\u200b\u8ffd\u8e2a\u200b GPU \u200b\u548c\u200b CPU \u200b\u5185\u5b58\u200b\u5206\u914d\u200b</li> <li>\u200b\u68c0\u6d4b\u200b\u5185\u5b58\u200b\u6cc4\u6f0f\u200b</li> <li>\u200b\u5206\u6790\u200b\u5185\u5b58\u200b\u4f7f\u7528\u200b\u6a21\u5f0f\u200b</li> <li>\u200b\u4f18\u5316\u200b\u5185\u5b58\u200b\u6548\u7387\u200b</li> </ul>"},{"location":"zh/guide/memory-analysis/#_3","title":"\u5feb\u901f\u200b\u5185\u5b58\u200b\u68c0\u67e5","text":"<pre><code># \u200b\u83b7\u53d6\u200b\u5f53\u524d\u200b\u5185\u5b58\u200b\u72b6\u6001\u200b\nprobing $ENDPOINT eval \"\nimport torch\nimport psutil\n\nproc = psutil.Process()\nprint(f'CPU \u200b\u5185\u5b58\u200b: {proc.memory_info().rss / 1024**3:.2f} GB')\n\nif torch.cuda.is_available():\n    print(f'GPU \u200b\u5df2\u200b\u5206\u914d\u200b: {torch.cuda.memory_allocated() / 1024**3:.2f} GB')\n    print(f'GPU \u200b\u7f13\u5b58\u200b: {torch.cuda.memory_reserved() / 1024**3:.2f} GB')\n\"\n</code></pre>"},{"location":"zh/guide/memory-analysis/#_4","title":"\u5185\u5b58\u200b\u4f7f\u7528\u200b\u8d8b\u52bf","text":""},{"location":"zh/guide/memory-analysis/#_5","title":"\u8ffd\u8e2a\u200b\u8bad\u7ec3\u200b\u6b65\u9aa4\u200b\u7684\u200b\u5185\u5b58\u200b\u53d8\u5316","text":"<pre><code>SELECT\n  step,\n  avg(allocated) as avg_memory_mb,\n  max(allocated) as peak_memory_mb,\n  min(allocated) as min_memory_mb\nFROM python.torch_trace\nWHERE step IS NOT NULL\nGROUP BY step\nORDER BY step;\n</code></pre>"},{"location":"zh/guide/memory-analysis/#_6","title":"\u68c0\u6d4b\u200b\u5185\u5b58\u200b\u589e\u957f","text":"<pre><code>SELECT\n  step,\n  max(allocated) - min(allocated) as memory_growth_mb\nFROM python.torch_trace\nWHERE step &gt; (SELECT max(step) - 10 FROM python.torch_trace)\nGROUP BY step\nHAVING max(allocated) - min(allocated) &gt; 50\nORDER BY step;\n</code></pre>"},{"location":"zh/guide/memory-analysis/#_7","title":"\u6309\u200b\u6a21\u5757\u200b\u5206\u6790\u200b\u5185\u5b58","text":"<p>\u200b\u8bc6\u522b\u200b\u54ea\u4e9b\u200b\u6a21\u578b\u200b\u7ec4\u4ef6\u200b\u4f7f\u7528\u200b\u6700\u200b\u591a\u200b\u5185\u5b58\u200b\uff1a</p> <pre><code>SELECT\n  module,\n  stage,\n  avg(allocated) as avg_memory,\n  count(*) as execution_count\nFROM python.torch_trace\nWHERE module IS NOT NULL\nGROUP BY module, stage\nORDER BY avg_memory DESC\nLIMIT 10;\n</code></pre>"},{"location":"zh/guide/memory-analysis/#_8","title":"\u5185\u5b58\u200b\u6cc4\u6f0f\u200b\u68c0\u6d4b","text":""},{"location":"zh/guide/memory-analysis/#_9","title":"\u6a21\u5f0f\u200b\uff1a\u200b\u5355\u8c03\u200b\u589e\u957f","text":"<pre><code>WITH memory_trend AS (\n  SELECT\n    step,\n    max(allocated) as peak_memory,\n    LAG(max(allocated)) OVER (ORDER BY step) as prev_peak\n  FROM python.torch_trace\n  GROUP BY step\n)\nSELECT step, peak_memory, prev_peak,\n       peak_memory - prev_peak as growth\nFROM memory_trend\nWHERE peak_memory &gt; prev_peak\nORDER BY step;\n</code></pre>"},{"location":"zh/guide/memory-analysis/#_10","title":"\u5f3a\u5236\u200b\u5783\u573e\u200b\u56de\u6536","text":"<pre><code>probing $ENDPOINT eval \"\nimport gc\nimport torch\n\n# \u200b\u5f3a\u5236\u200b\u5783\u573e\u200b\u56de\u6536\u200b\ngc.collect()\n\n# \u200b\u6e05\u7406\u200b CUDA \u200b\u7f13\u5b58\u200b\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    print('CUDA \u200b\u7f13\u5b58\u200b\u5df2\u200b\u6e05\u7406\u200b')\n\nprint(f'\u200b\u5783\u573e\u200b\u56de\u6536\u200b\u5b8c\u6210\u200b: {gc.get_count()}')\n\"\n</code></pre>"},{"location":"zh/guide/memory-analysis/#gpu","title":"GPU \u200b\u5185\u5b58\u200b\u5206\u6790","text":""},{"location":"zh/guide/memory-analysis/#gpu_1","title":"\u5f53\u524d\u200b GPU \u200b\u72b6\u6001","text":"<pre><code>probing $ENDPOINT eval \"\nimport torch\n\nif torch.cuda.is_available():\n    for i in range(torch.cuda.device_count()):\n        props = torch.cuda.get_device_properties(i)\n        allocated = torch.cuda.memory_allocated(i) / 1024**3\n        reserved = torch.cuda.memory_reserved(i) / 1024**3\n        total = props.total_memory / 1024**3\n\n        print(f'GPU {i}: {props.name}')\n        print(f'  \u200b\u603b\u8ba1\u200b: {total:.2f} GB')\n        print(f'  \u200b\u5df2\u200b\u5206\u914d\u200b: {allocated:.2f} GB')\n        print(f'  \u200b\u5df2\u200b\u4fdd\u7559\u200b: {reserved:.2f} GB')\n        print(f'  \u200b\u7a7a\u95f2\u200b: {total - reserved:.2f} GB')\n\"\n</code></pre>"},{"location":"zh/guide/memory-analysis/#_11","title":"\u6700\u4f73\u200b\u5b9e\u8df5","text":""},{"location":"zh/guide/memory-analysis/#1","title":"1. \u200b\u5b9a\u671f\u200b\u5185\u5b58\u200b\u5feb\u7167","text":"<p>\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u5b9a\u671f\u200b\u62cd\u6444\u200b\u5feb\u7167\u200b\uff1a</p> <pre><code>probing $ENDPOINT eval \"\nimport torch\nstep = trainer.current_step\nif step % 100 == 0:\n    allocated = torch.cuda.memory_allocated() / 1024**3\n    print(f'\u200b\u6b65\u9aa4\u200b {step}: {allocated:.2f} GB')\n\"\n</code></pre>"},{"location":"zh/guide/memory-analysis/#2","title":"2. \u200b\u5206\u6790\u200b\u5185\u5b58\u200b\u5bc6\u96c6\u578b\u200b\u64cd\u4f5c","text":"<pre><code>-- \u200b\u67e5\u627e\u200b\u5185\u5b58\u200b\u6ce2\u52a8\u200b\u6700\u5927\u200b\u7684\u200b\u64cd\u4f5c\u200b\nSELECT\n  module,\n  stage,\n  stddev(allocated) as memory_variance,\n  max(allocated) - min(allocated) as memory_range\nFROM python.torch_trace\nGROUP BY module, stage\nHAVING stddev(allocated) &gt; 10\nORDER BY memory_variance DESC;\n</code></pre>"},{"location":"zh/guide/memory-analysis/#_12","title":"\u6545\u969c\u200b\u6392\u9664","text":""},{"location":"zh/guide/memory-analysis/#oom","title":"\u5185\u5b58\u4e0d\u8db3\u200b (OOM)","text":"<ol> <li>\u200b\u68c0\u67e5\u200b\u5f53\u524d\u200b\u5185\u5b58\u200b\u72b6\u6001\u200b</li> <li>\u200b\u8bc6\u522b\u200b\u5185\u5b58\u200b\u5bc6\u96c6\u578b\u200b\u6a21\u5757\u200b</li> <li>\u200b\u5f3a\u5236\u200b\u5783\u573e\u200b\u56de\u6536\u200b</li> <li>\u200b\u51cf\u5c0f\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u6216\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b</li> </ol>"},{"location":"zh/guide/memory-analysis/#_13","title":"\u5185\u5b58\u200b\u672a\u200b\u91ca\u653e","text":"<ol> <li>\u200b\u68c0\u67e5\u200b\u5faa\u73af\u200b\u5f15\u7528\u200b</li> <li>\u200b\u786e\u8ba4\u200b\u5f20\u91cf\u200b\u672a\u200b\u88ab\u200b\u4fdd\u5b58\u200b\u5728\u200b\u5217\u8868\u200b/\u200b\u5b57\u5178\u200b\u4e2d\u200b</li> <li>\u200b\u663e\u5f0f\u200b\u4f7f\u7528\u200b <code>del</code> \u200b\u5220\u9664\u200b\u5927\u200b\u5f20\u91cf\u200b</li> <li>\u200b\u8c03\u7528\u200b <code>torch.cuda.empty_cache()</code></li> </ol>"},{"location":"zh/guide/memory-analysis/#_14","title":"\u4e0b\u200b\u4e00\u6b65","text":"<ul> <li>SQL \u200b\u5206\u6790\u200b - \u200b\u66f4\u200b\u591a\u200b\u67e5\u8be2\u200b\u6a21\u5f0f\u200b</li> <li>\u200b\u8c03\u8bd5\u200b\u6307\u5357\u200b - \u200b\u5806\u6808\u200b\u5206\u6790\u200b\u6280\u672f\u200b</li> <li>\u200b\u5e38\u89c1\u95ee\u9898\u200b - \u200b\u5e38\u89c1\u95ee\u9898\u200b\u89e3\u51b3\u200b</li> </ul>"},{"location":"zh/guide/sql-analytics/","title":"SQL \u200b\u5206\u6790\u200b\u63a5\u53e3","text":"<p>Probing \u200b\u63d0\u4f9b\u200b\u5f3a\u5927\u200b\u7684\u200b SQL \u200b\u63a5\u53e3\u200b\u7528\u4e8e\u200b\u5206\u6790\u200b\u6027\u80fd\u200b\u548c\u200b\u76d1\u63a7\u200b\u6570\u636e\u200b\u3002</p>"},{"location":"zh/guide/sql-analytics/#_1","title":"\u6982\u89c8","text":"<p>SQL \u200b\u5206\u6790\u200b\u63a5\u53e3\u200b\u5c06\u200b\u590d\u6742\u200b\u7684\u200b\u6027\u80fd\u200b\u5206\u6790\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u76f4\u89c2\u200b\u7684\u200b\u6570\u636e\u5e93\u200b\u67e5\u8be2\u200b\u3002\u200b\u6240\u6709\u200b\u76d1\u63a7\u200b\u6570\u636e\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u6807\u51c6\u200b SQL \u200b\u64cd\u4f5c\u200b\u8bbf\u95ee\u200b\uff0c\u200b\u5305\u62ec\u200b <code>SELECT</code>\u3001<code>WHERE</code>\u3001<code>GROUP BY</code>\u3001<code>ORDER BY</code> \u200b\u548c\u200b\u9ad8\u7ea7\u200b\u5206\u6790\u200b\u51fd\u6570\u200b\u3002</p>"},{"location":"zh/guide/sql-analytics/#_2","title":"\u57fa\u672c\u200b\u67e5\u8be2\u200b\u7ed3\u6784","text":"<pre><code>probing $ENDPOINT query \"SELECT columns FROM table WHERE conditions\"\n</code></pre>"},{"location":"zh/guide/sql-analytics/#_3","title":"\u6838\u5fc3\u200b\u8868","text":""},{"location":"zh/guide/sql-analytics/#_4","title":"\u914d\u7f6e\u200b\u548c\u200b\u5143\u200b\u6570\u636e","text":"<p><code>information_schema.df_settings</code> - \u200b\u7cfb\u7edf\u914d\u7f6e\u200b\u548c\u200b\u8bbe\u7f6e\u200b</p> <pre><code>SELECT * FROM information_schema.df_settings\nWHERE name LIKE 'probing.%';\n</code></pre>"},{"location":"zh/guide/sql-analytics/#python","title":"Python \u200b\u547d\u540d\u200b\u7a7a\u95f4\u200b\u8868","text":"<p><code>python.backtrace</code> - \u200b\u5806\u6808\u200b\u8ddf\u8e2a\u200b\u4fe1\u606f\u200b</p> <pre><code>SELECT * FROM python.backtrace LIMIT 10;\n</code></pre> <p>\u200b\u5e38\u7528\u200b\u5217\u200b\uff1a</p> <ul> <li><code>ip</code> - \u200b\u6307\u4ee4\u200b\u6307\u9488\u200b\uff08\u200b\u7528\u4e8e\u200b\u539f\u751f\u200b\u5e27\u200b\uff09</li> <li><code>file</code> - \u200b\u6e90\u200b\u6587\u4ef6\u540d\u200b</li> <li><code>func</code> - \u200b\u51fd\u6570\u200b\u540d\u200b</li> <li><code>lineno</code> - \u200b\u884c\u53f7\u200b</li> <li><code>depth</code> - \u200b\u5806\u6808\u200b\u6df1\u5ea6\u200b</li> <li><code>frame_type</code> - \u200b\u5e27\u200b\u7c7b\u578b\u200b\uff08'Python' \u200b\u6216\u200b 'Native'\uff09</li> </ul>"},{"location":"zh/guide/sql-analytics/#pytorch","title":"PyTorch \u200b\u96c6\u6210","text":"<p>\u200b\u76d1\u63a7\u200b PyTorch \u200b\u5e94\u7528\u200b\u65f6\u200b\uff0c\u200b\u53ef\u7528\u200b\u989d\u5916\u200b\u7684\u200b\u8868\u200b\uff1a</p> <p><code>python.torch_trace</code> - PyTorch \u200b\u6267\u884c\u200b\u8ddf\u8e2a\u200b</p> <pre><code>SELECT step, module, stage, duration, allocated\nFROM python.torch_trace\nWHERE step &gt;= 5\nORDER BY step DESC, seq;\n</code></pre> <p>\u200b\u5e38\u7528\u200b\u5217\u200b\uff1a</p> <ul> <li><code>step</code> - \u200b\u8bad\u7ec3\u200b\u6b65\u6570\u200b</li> <li><code>seq</code> - \u200b\u6b65\u5185\u200b\u5e8f\u53f7\u200b</li> <li><code>module</code> - \u200b\u6a21\u5757\u200b\u540d\u200b</li> <li><code>stage</code> - \u200b\u6267\u884c\u200b\u9636\u6bb5\u200b\uff08forward\u3001backward\u3001step\uff09</li> <li><code>allocated</code> - GPU \u200b\u5df2\u200b\u5206\u914d\u5185\u5b58\u200b\uff08MB\uff09</li> <li><code>duration</code> - \u200b\u6267\u884c\u200b\u65f6\u200b\u957f\u200b\uff08\u200b\u79d2\u200b\uff09</li> </ul>"},{"location":"zh/guide/sql-analytics/#_5","title":"\u9ad8\u7ea7\u200b\u5206\u6790","text":""},{"location":"zh/guide/sql-analytics/#_6","title":"\u65f6\u95f4\u200b\u5e8f\u5217\u200b\u5206\u6790","text":"<p>\u200b\u5185\u5b58\u200b\u968f\u200b\u65f6\u95f4\u200b\u589e\u957f\u200b\uff1a</p> <pre><code>SELECT\n  step,\n  stage,\n  avg(allocated) as avg_memory_mb,\n  max(allocated) as peak_memory_mb\nFROM python.torch_trace\nWHERE step &gt; (SELECT max(step) - 10 FROM python.torch_trace)\nGROUP BY step, stage\nORDER BY step, stage;\n</code></pre> <p>\u200b\u6eda\u52a8\u200b\u5e73\u5747\u200b\uff1a</p> <pre><code>SELECT\n  step,\n  module,\n  duration,\n  AVG(duration) OVER (\n    PARTITION BY module\n    ORDER BY step, seq\n    ROWS BETWEEN 4 PRECEDING AND CURRENT ROW\n  ) as avg_duration_5_samples\nFROM python.torch_trace\nWHERE step &gt; (SELECT max(step) - 5 FROM python.torch_trace);\n</code></pre>"},{"location":"zh/guide/sql-analytics/#_7","title":"\u6027\u80fd\u200b\u5206\u6790","text":"<p>\u200b\u6700\u6162\u200b\u64cd\u4f5c\u200b\u6392\u540d\u200b\uff1a</p> <pre><code>SELECT\n  module,\n  stage,\n  count(*) as execution_count,\n  avg(duration) as avg_duration,\n  max(duration) as max_duration\nFROM python.torch_trace\nWHERE step &gt; (SELECT max(step) - 10 FROM python.torch_trace)\n  AND duration &gt; 0\nGROUP BY module, stage\nORDER BY avg_duration DESC\nLIMIT 10;\n</code></pre>"},{"location":"zh/guide/sql-analytics/#_8","title":"\u805a\u5408\u200b\u51fd\u6570","text":""},{"location":"zh/guide/sql-analytics/#_9","title":"\u7edf\u8ba1\u200b\u51fd\u6570","text":"<pre><code>SELECT\n  module,\n  stage,\n  count(*) as total_executions,\n  avg(duration) as mean_duration,\n  percentile_cont(0.5) WITHIN GROUP (ORDER BY duration) as median_duration,\n  percentile_cont(0.95) WITHIN GROUP (ORDER BY duration) as p95_duration,\n  min(duration) as min_duration,\n  max(duration) as max_duration\nFROM python.torch_trace\nWHERE duration &gt; 0\nGROUP BY module, stage;\n</code></pre>"},{"location":"zh/guide/sql-analytics/#_10","title":"\u7a97\u53e3\u200b\u51fd\u6570","text":"<pre><code>SELECT\n  step,\n  allocated,\n  LAG(allocated) OVER (ORDER BY step, seq) as prev_memory,\n  LEAD(allocated) OVER (ORDER BY step, seq) as next_memory,\n  ROW_NUMBER() OVER (ORDER BY allocated DESC) as memory_rank\nFROM python.torch_trace\nWHERE step &gt; (SELECT max(step) - 5 FROM python.torch_trace);\n</code></pre>"},{"location":"zh/guide/sql-analytics/#_11","title":"\u6570\u636e\u200b\u5bfc\u51fa","text":"<p>\u200b\u7ed3\u679c\u200b\u53ef\u4ee5\u200b\u5bfc\u51fa\u200b\u7528\u4e8e\u200b\u8fdb\u4e00\u6b65\u200b\u5206\u6790\u200b\uff1a</p> <pre><code># \u200b\u5bfc\u51fa\u200b\u4e3a\u200b JSON\nprobing $ENDPOINT query \"SELECT * FROM python.torch_trace\" &gt; torch_traces.json\n\n# \u200b\u65f6\u95f4\u200b\u5e8f\u5217\u200b\u6570\u636e\u200b\u7528\u4e8e\u200b\u7ed8\u56fe\u200b\nprobing $ENDPOINT query \"\n  SELECT step, stage, avg(duration), avg(allocated)\n  FROM python.torch_trace\n  GROUP BY step, stage\n\" &gt; training_metrics.json\n</code></pre>"},{"location":"zh/guide/sql-analytics/#_12","title":"\u6700\u4f73\u200b\u5b9e\u8df5","text":"<ol> <li>\u200b\u4f7f\u7528\u200b\u57fa\u4e8e\u200b\u6b65\u6570\u200b\u7684\u200b\u8fc7\u6ee4\u200b - \u200b\u59cb\u7ec8\u200b\u5305\u542b\u200b\u6b65\u6570\u200b\u7ea6\u675f\u200b\u4ee5\u200b\u83b7\u5f97\u200b\u66f4\u597d\u200b\u7684\u200b\u6027\u80fd\u200b</li> <li>\u200b\u9650\u5236\u200b\u7ed3\u679c\u200b\u96c6\u200b - \u200b\u5bf9\u5927\u200b\u6570\u636e\u200b\u96c6\u200b\u4f7f\u7528\u200b <code>LIMIT</code> \u200b\u5b50\u53e5\u200b</li> <li>\u200b\u9002\u5f53\u200b\u805a\u5408\u200b - \u200b\u4f7f\u7528\u200b <code>GROUP BY</code> \u200b\u83b7\u53d6\u200b\u6c47\u603b\u200b\u7edf\u8ba1\u200b</li> <li>\u200b\u6e10\u8fdb\u5f0f\u200b\u6d4b\u8bd5\u200b\u67e5\u8be2\u200b - \u200b\u4ece\u200b\u7b80\u5355\u200b\u5f00\u59cb\u200b\uff0c\u200b\u9010\u6b65\u200b\u589e\u52a0\u200b\u590d\u6742\u5ea6\u200b</li> </ol>"},{"location":"zh/guide/troubleshooting/","title":"\u5e38\u89c1\u95ee\u9898","text":"<p>\u200b\u4f7f\u7528\u200b Probing \u200b\u65f6\u200b\u7684\u200b\u5e38\u89c1\u95ee\u9898\u200b\u53ca\u200b\u89e3\u51b3\u65b9\u6848\u200b\u3002</p>"},{"location":"zh/guide/troubleshooting/#_2","title":"\u8fde\u63a5\u200b\u95ee\u9898","text":""},{"location":"zh/guide/troubleshooting/#_3","title":"\u65e0\u6cd5\u200b\u8fde\u63a5\u200b\u5230\u200b\u8fdb\u7a0b","text":"<p>\u200b\u75c7\u72b6\u200b\uff1a<code>probing $ENDPOINT inject</code> \u200b\u5931\u8d25\u200b\u6216\u200b\u8d85\u65f6\u200b\u3002</p> <p>\u200b\u89e3\u51b3\u65b9\u6848\u200b\uff1a</p> <ol> <li> <p>\u200b\u9a8c\u8bc1\u200b\u8fdb\u7a0b\u200b\u5b58\u5728\u200b\uff1a    <pre><code>ps aux | grep $ENDPOINT\n</code></pre></p> </li> <li> <p>\u200b\u68c0\u67e5\u200b Linux \u200b\u8981\u6c42\u200b\uff1a    \u200b\u6ce8\u5165\u200b\u529f\u80fd\u200b\u4ec5\u200b\u5728\u200b Linux \u200b\u4e0a\u200b\u53ef\u7528\u200b\u3002\u200b\u5728\u200b\u5176\u4ed6\u200b\u5e73\u53f0\u200b\u4e0a\u200b\uff0c\u200b\u8bf7\u200b\u5728\u200b\u542f\u52a8\u200b\u65f6\u200b\u542f\u7528\u200b\uff1a    <pre><code>PROBING=1 python your_script.py\n</code></pre></p> </li> <li> <p>\u200b\u68c0\u67e5\u200b\u6743\u9650\u200b\uff1a    <pre><code># \u200b\u53ef\u80fd\u200b\u9700\u8981\u200b sudo \u200b\u8fdb\u884c\u200b\u6ce8\u5165\u200b\nsudo probing $ENDPOINT inject\n</code></pre></p> </li> </ol>"},{"location":"zh/guide/troubleshooting/#_4","title":"\u8fde\u63a5\u200b\u88ab\u200b\u62d2\u7edd\u200b\uff08\u200b\u8fdc\u7a0b\u200b\uff09","text":"<p>\u200b\u75c7\u72b6\u200b\uff1a\u200b\u65e0\u6cd5\u200b\u8fde\u63a5\u200b\u5230\u200b\u8fdc\u7a0b\u200b\u8fdb\u7a0b\u200b\u3002</p> <p>\u200b\u89e3\u51b3\u65b9\u6848\u200b\uff1a</p> <ol> <li> <p>\u200b\u9a8c\u8bc1\u200b\u670d\u52a1\u5668\u200b\u6b63\u5728\u200b\u8fd0\u884c\u200b\uff1a    <pre><code># \u200b\u5728\u200b\u8fdc\u7a0b\u200b\u673a\u5668\u200b\u4e0a\u200b\nnetstat -tlnp | grep $PORT\n</code></pre></p> </li> <li> <p>\u200b\u68c0\u67e5\u200b\u9632\u706b\u5899\u200b\uff1a    <pre><code># \u200b\u5141\u8bb8\u200b\u7aef\u53e3\u200b\nsudo ufw allow $PORT\n</code></pre></p> </li> <li> <p>\u200b\u9a8c\u8bc1\u200b\u7aef\u70b9\u200b\u683c\u5f0f\u200b\uff1a    <pre><code>export ENDPOINT=hostname:port  # \u200b\u4e0d\u200b\u53ea\u662f\u200b hostname\n</code></pre></p> </li> </ol>"},{"location":"zh/guide/troubleshooting/#_5","title":"\u67e5\u8be2\u200b\u95ee\u9898","text":""},{"location":"zh/guide/troubleshooting/#_6","title":"\u8868\u4e0d\u200b\u5b58\u5728","text":"<p>\u200b\u75c7\u72b6\u200b\uff1a<code>Table 'python.torch_trace' not found</code></p> <p>\u200b\u89e3\u51b3\u65b9\u6848\u200b\uff1a</p> <ol> <li> <p>\u200b\u68c0\u67e5\u200b PyTorch \u200b\u5206\u6790\u200b\u662f\u5426\u200b\u542f\u7528\u200b\uff1a    <pre><code>probing $ENDPOINT eval \"\nimport probing\nprint(probing.get_config())\"\n</code></pre></p> </li> <li> <p>\u200b\u542f\u7528\u200b PyTorch \u200b\u8ffd\u8e2a\u200b\uff1a    <pre><code>PROBING_TORCH_PROFILING=on python your_script.py\n</code></pre></p> </li> <li> <p>\u200b\u7b49\u5f85\u200b\u6570\u636e\u200b\u6536\u96c6\u200b\uff1a    \u200b\u8868\u5728\u200b\u64cd\u4f5c\u200b\u53d1\u751f\u200b\u65f6\u200b\u586b\u5145\u200b\u3002\u200b\u5148\u200b\u8fd0\u884c\u200b\u4e00\u4e9b\u200b\u8bad\u7ec3\u200b\u6b65\u9aa4\u200b\u3002</p> </li> </ol>"},{"location":"zh/guide/troubleshooting/#_7","title":"\u7ed3\u679c\u200b\u4e3a\u7a7a","text":"<p>\u200b\u75c7\u72b6\u200b\uff1a\u200b\u67e5\u8be2\u200b\u6ca1\u6709\u200b\u8fd4\u56de\u200b\u884c\u200b\u3002</p> <p>\u200b\u89e3\u51b3\u65b9\u6848\u200b\uff1a</p> <ol> <li> <p>\u200b\u68c0\u67e5\u8868\u200b\u5185\u5bb9\u200b\uff1a    <pre><code>SELECT COUNT(*) FROM python.torch_trace;\n</code></pre></p> </li> <li> <p>\u200b\u9a8c\u8bc1\u200b\u8fc7\u6ee4\u200b\u6761\u4ef6\u200b\uff1a    <pre><code>-- \u200b\u79fb\u9664\u200b\u8fc7\u6ee4\u5668\u200b\u6765\u200b\u8c03\u8bd5\u200b\nSELECT * FROM python.torch_trace LIMIT 5;\n</code></pre></p> </li> <li> <p>\u200b\u68c0\u67e5\u200b\u6b65\u9aa4\u200b\u8303\u56f4\u200b\uff1a    <pre><code>SELECT MIN(step), MAX(step) FROM python.torch_trace;\n</code></pre></p> </li> </ol>"},{"location":"zh/guide/troubleshooting/#eval","title":"Eval \u200b\u95ee\u9898","text":""},{"location":"zh/guide/troubleshooting/#_8","title":"\u4ee3\u7801\u6267\u884c\u200b\u5931\u8d25","text":"<p>\u200b\u75c7\u72b6\u200b\uff1a<code>probing eval</code> \u200b\u8fd4\u56de\u200b\u9519\u8bef\u200b\u6216\u200b\u610f\u5916\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u89e3\u51b3\u65b9\u6848\u200b\uff1a</p> <ol> <li> <p>\u200b\u68c0\u67e5\u200b\u8bed\u6cd5\u200b\uff1a    <pre><code># \u200b\u4f7f\u7528\u200b\u6b63\u786e\u200b\u7684\u200b\u5f15\u53f7\u200b\nprobing $ENDPOINT eval \"print('hello')\"\n</code></pre></p> </li> <li> <p>\u200b\u5904\u7406\u200b\u5bfc\u5165\u200b\uff1a    <pre><code># \u200b\u5148\u5bfc\u200b\u5165\u200b\u6a21\u5757\u200b\nprobing $ENDPOINT eval \"import torch; print(torch.__version__)\"\n</code></pre></p> </li> <li> <p>\u200b\u68c0\u67e5\u200b\u53d8\u91cf\u200b\u4f5c\u7528\u57df\u200b\uff1a    <pre><code># \u200b\u4f7f\u7528\u200b globals() \u200b\u67e5\u770b\u200b\u53ef\u7528\u200b\u53d8\u91cf\u200b\nprobing $ENDPOINT eval \"print(list(globals().keys())[:10])\"\n</code></pre></p> </li> </ol>"},{"location":"zh/guide/troubleshooting/#_9","title":"\u6027\u80fd\u200b\u95ee\u9898","text":""},{"location":"zh/guide/troubleshooting/#_10","title":"\u5f00\u9500\u200b\u8fc7\u9ad8","text":"<p>\u200b\u75c7\u72b6\u200b\uff1a\u200b\u542f\u7528\u200b Probing \u200b\u540e\u200b\u5e94\u7528\u200b\u8fd0\u884c\u200b\u53d8\u6162\u200b\u3002</p> <p>\u200b\u89e3\u51b3\u65b9\u6848\u200b\uff1a</p> <ol> <li> <p>\u200b\u964d\u4f4e\u200b\u91c7\u6837\u7387\u200b\uff1a    <pre><code>probing $ENDPOINT config probing.sample_rate=0.01\n</code></pre></p> </li> <li> <p>\u200b\u7981\u7528\u200b\u672a\u200b\u4f7f\u7528\u200b\u7684\u200b\u529f\u80fd\u200b\uff1a    <pre><code>PROBING_TORCH_PROFILING=off python your_script.py\n</code></pre></p> </li> <li> <p>\u200b\u4f7f\u7528\u200b\u9488\u5bf9\u6027\u200b\u5206\u6790\u200b\uff1a    \u200b\u53ea\u200b\u4e3a\u200b\u7279\u5b9a\u200b\u6a21\u5757\u200b\u6216\u200b\u64cd\u4f5c\u200b\u542f\u7528\u200b\u5206\u6790\u200b\u3002</p> </li> </ol>"},{"location":"zh/guide/troubleshooting/#_11","title":"\u67e5\u8be2\u200b\u8d85\u65f6","text":"<p>\u200b\u75c7\u72b6\u200b\uff1aSQL \u200b\u67e5\u8be2\u200b\u8017\u65f6\u200b\u592a\u200b\u957f\u200b\u3002</p> <p>\u200b\u89e3\u51b3\u65b9\u6848\u200b\uff1a</p> <ol> <li> <p>\u200b\u6dfb\u52a0\u200b LIMIT \u200b\u5b50\u53e5\u200b\uff1a    <pre><code>SELECT * FROM python.torch_trace LIMIT 100;\n</code></pre></p> </li> <li> <p>\u200b\u4f7f\u7528\u200b\u6b65\u9aa4\u200b\u8fc7\u6ee4\u200b\uff1a    <pre><code>WHERE step &gt; (SELECT MAX(step) - 10 FROM python.torch_trace)\n</code></pre></p> </li> <li> <p>\u200b\u805a\u5408\u200b\u6570\u636e\u200b\uff1a    <pre><code>SELECT step, AVG(duration) FROM python.torch_trace GROUP BY step;\n</code></pre></p> </li> </ol>"},{"location":"zh/guide/troubleshooting/#_12","title":"\u83b7\u53d6\u200b\u5e2e\u52a9","text":"<p>\u200b\u5982\u679c\u200b\u4ecd\u200b\u6709\u200b\u95ee\u9898\u200b\uff1a</p> <ol> <li> <p>\u200b\u68c0\u67e5\u200b\u65e5\u5fd7\u200b\uff1a    <pre><code>probing $ENDPOINT eval \"\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\"\n</code></pre></p> </li> <li> <p>\u200b\u62a5\u544a\u200b\u95ee\u9898\u200b\uff1a    GitHub Issues</p> </li> <li> <p>\u200b\u5305\u542b\u200b\u8bca\u65ad\u200b\u4fe1\u606f\u200b\uff1a    <pre><code>probing --version\npython --version\nuname -a\n</code></pre></p> </li> </ol>"}]}